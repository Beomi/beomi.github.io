{"pages":[{"title":"","text":"naver-site-verification: naver0eed895aa522da30bfdcf1c4b681f4bd.html","link":"/naver0eed895aa522da30bfdcf1c4b681f4bd.html"},{"title":"","text":"빈 HTML # title content 1 Lorem Ipsum 로렘 입섬은 빈칸을 채우기 위한 문구입니다. 2 Hello World 헬로 월드는 언어를 배우기 시작할때 화면에 표준 출력을 할때 주로 사용하는 문구입니다. CSV 다운로드 받기 class ToCSV { constructor() { // CSV 버튼에 이벤트 등록 document.querySelector('#csvDownloadButton').addEventListener('click', e => { e.preventDefault() this.getCSV('mycsv.csv') }) } downloadCSV(csv, filename) { let csvFile; let downloadLink; // 한글 처리를 해주기 위해 BOM 추가하기 const BOM = \"\\uFEFF\"; csv = BOM + csv // CSV 파일을 위한 Blob 만들기 csvFile = new Blob([csv], {type: \"text/csv\"}) // Download link를 위한 a 엘리먼스 생성 downloadLink = document.createElement(\"a\") // 다운받을 csv 파일 이름 지정하기 downloadLink.download = filename; // 위에서 만든 blob과 링크를 연결 downloadLink.href = window.URL.createObjectURL(csvFile) // 링크가 눈에 보일 필요는 없으니 숨겨줍시다. downloadLink.style.display = \"none\" // HTML 가장 아래 부분에 링크를 붙여줍시다. document.body.appendChild(downloadLink) // 클릭 이벤트를 발생시켜 실제로 브라우저가 '다운로드'하도록 만들어줍시다. downloadLink.click() } getCSV(filename) { // csv를 담기 위한 빈 Array를 만듭시다. const csv = [] const rows = document.querySelectorAll(\"#mytable tr\") for (let i = 0; i < rows.length; i++) { const row = [], cols = rows[i].querySelectorAll(\"td, th\") for (let j = 0; j < cols.length; j++) row.push(cols[j].innerText) csv.push(row.join(\",\")) } // Download CSV this.downloadCSV(csv.join(\"\\n\"), filename) } } document.addEventListener('DOMContentLoaded', e => { new ToCSV() })","link":"/others/html_to_csv.html"},{"title":"","text":"window.PlotlyConfig = {MathJaxConfig: 'local'}; /** * plotly.js v1.48.3 * Copyright 2012-2019, Plotly, Inc. * All rights reserved. * Licensed under the MIT license */ !function(t){if(\"object\"==typeof exports&&\"undefined\"!=typeof module)module.exports=t();else if(\"function\"==typeof define&&define.amd)define([],t);else{(\"undefined\"!=typeof window?window:\"undefined\"!=typeof global?global:\"undefined\"!=typeof self?self:this).Plotly=t()}}(function(){return function(){return function t(e,r,n){function i(o,s){if(!r[o]){if(!e[o]){var l=\"function\"==typeof require&&require;if(!s&&l)return l(o,!0);if(a)return a(o,!0);var c=new Error(\"Cannot find module '\"+o+\"'\");throw c.code=\"MODULE_NOT_FOUND\",c}var u=r[o]={exports:{}};e[o][0].call(u.exports,function(t){return i(e[o][1][t]||t)},u,u.exports,t,e,r,n)}return r[o].exports}for(var a=\"function\"==typeof require&&require,o=0;o0&&(e.y0-=r,e.y1-=r),a=e.y0})}}(a),E(a),a}function E(t){t.nodes.forEach(function(t){t.sourceLinks.sort(l),t.targetLinks.sort(s)}),t.nodes.forEach(function(t){var e=t.y0,r=e;t.sourceLinks.forEach(function(t){t.y0=e+t.width/2,e+=t.width}),t.targetLinks.forEach(function(t){t.y1=r+t.width/2,r+=t.width})})}return S.update=function(t){return E(t),t},S.nodeId=function(t){return arguments.length?(_=\"function\"==typeof t?t:o(t),S):_},S.nodeAlign=function(t){return arguments.length?(w=\"function\"==typeof t?t:o(t),S):w},S.nodeWidth=function(t){return arguments.length?(x=+t,S):x},S.nodePadding=function(t){return arguments.length?(b=+t,S):b},S.nodes=function(t){return arguments.length?(k=\"function\"==typeof t?t:o(t),S):k},S.links=function(t){return arguments.length?(T=\"function\"==typeof t?t:o(t),S):T},S.size=function(e){return arguments.length?(t=n=0,i=+e[0],y=+e[1],S):[i-t,y-n]},S.extent=function(e){return arguments.length?(t=+e[0][0],i=+e[1][0],n=+e[0][1],y=+e[1][1],S):[[t,n],[i,y]]},S.iterations=function(t){return arguments.length?(A=+t,S):A},S},t.sankeyCenter=function(t){return t.targetLinks.length?t.depth:t.sourceLinks.length?e.min(t.sourceLinks,i)-1:0},t.sankeyLeft=function(t){return t.depth},t.sankeyRight=function(t,e){return e-1-t.height},t.sankeyJustify=a,t.sankeyLinkHorizontal=function(){return n.linkHorizontal().source(y).target(x)},Object.defineProperty(t,\"__esModule\",{value:!0})},\"object\"==typeof r&&\"undefined\"!=typeof e?i(r,t(\"d3-array\"),t(\"d3-collection\"),t(\"d3-shape\")):i(n.d3=n.d3||{},n.d3,n.d3,n.d3)},{\"d3-array\":145,\"d3-collection\":146,\"d3-shape\":155}],52:[function(t,e,r){\"use strict\";var n=\"undefined\"==typeof WeakMap?t(\"weak-map\"):WeakMap,i=t(\"gl-buffer\"),a=t(\"gl-vao\"),o=new n;e.exports=function(t){var e=o.get(t),r=e&&(e._triangleBuffer.handle||e._triangleBuffer.buffer);if(!r||!t.isBuffer(r)){var n=i(t,new Float32Array([-1,-1,-1,4,4,-1]));(e=a(t,[{buffer:n,type:t.FLOAT,size:2}]))._triangleBuffer=n,o.set(t,e)}e.bind(),t.drawArrays(t.TRIANGLES,0,3),e.unbind()}},{\"gl-buffer\":240,\"gl-vao\":322,\"weak-map\":539}],53:[function(t,e,r){e.exports=function(t){var e=0,r=0,n=0,i=0;return t.map(function(t){var a=(t=t.slice())[0],o=a.toUpperCase();if(a!=o)switch(t[0]=o,a){case\"a\":t[6]+=n,t[7]+=i;break;case\"v\":t[1]+=i;break;case\"h\":t[1]+=n;break;default:for(var s=1;s>24},r.countTrailingZeros=n,r.nextPow2=function(t){return t+=0===t,--t,t|=t>>>1,t|=t>>>2,t|=t>>>4,t|=t>>>8,(t|=t>>>16)+1},r.prevPow2=function(t){return t|=t>>>1,t|=t>>>2,t|=t>>>4,t|=t>>>8,(t|=t>>>16)-(t>>>1)},r.parity=function(t){return t^=t>>>16,t^=t>>>8,t^=t>>>4,27030>>>(t&=15)&1};var i=new Array(256);!function(t){for(var e=0;e>>=1;r;r>>>=1)n>>13,v=0|o[2],m=8191&v,y=v>>>13,x=0|o[3],b=8191&x,_=x>>>13,w=0|o[4],k=8191&w,T=w>>>13,A=0|o[5],M=8191&A,S=A>>>13,E=0|o[6],C=8191&E,L=E>>>13,z=0|o[7],O=8191&z,I=z>>>13,D=0|o[8],P=8191&D,R=D>>>13,F=0|o[9],B=8191&F,N=F>>>13,j=0|s[0],V=8191&j,U=j>>>13,H=0|s[1],q=8191&H,G=H>>>13,Y=0|s[2],W=8191&Y,X=Y>>>13,Z=0|s[3],$=8191&Z,J=Z>>>13,K=0|s[4],Q=8191&K,tt=K>>>13,et=0|s[5],rt=8191&et,nt=et>>>13,it=0|s[6],at=8191&it,ot=it>>>13,st=0|s[7],lt=8191&st,ct=st>>>13,ut=0|s[8],ft=8191&ut,ht=ut>>>13,pt=0|s[9],dt=8191&pt,gt=pt>>>13;r.negative=t.negative^e.negative,r.length=19;var vt=(c+(n=Math.imul(f,V))|0)+((8191&(i=(i=Math.imul(f,U))+Math.imul(h,V)|0))>13)|0)+(vt>>>26)|0,vt&=67108863,n=Math.imul(d,V),i=(i=Math.imul(d,U))+Math.imul(g,V)|0,a=Math.imul(g,U);var mt=(c+(n=n+Math.imul(f,q)|0)|0)+((8191&(i=(i=i+Math.imul(f,G)|0)+Math.imul(h,q)|0))>13)|0)+(mt>>>26)|0,mt&=67108863,n=Math.imul(m,V),i=(i=Math.imul(m,U))+Math.imul(y,V)|0,a=Math.imul(y,U),n=n+Math.imul(d,q)|0,i=(i=i+Math.imul(d,G)|0)+Math.imul(g,q)|0,a=a+Math.imul(g,G)|0;var yt=(c+(n=n+Math.imul(f,W)|0)|0)+((8191&(i=(i=i+Math.imul(f,X)|0)+Math.imul(h,W)|0))>13)|0)+(yt>>>26)|0,yt&=67108863,n=Math.imul(b,V),i=(i=Math.imul(b,U))+Math.imul(_,V)|0,a=Math.imul(_,U),n=n+Math.imul(m,q)|0,i=(i=i+Math.imul(m,G)|0)+Math.imul(y,q)|0,a=a+Math.imul(y,G)|0,n=n+Math.imul(d,W)|0,i=(i=i+Math.imul(d,X)|0)+Math.imul(g,W)|0,a=a+Math.imul(g,X)|0;var xt=(c+(n=n+Math.imul(f,$)|0)|0)+((8191&(i=(i=i+Math.imul(f,J)|0)+Math.imul(h,$)|0))>13)|0)+(xt>>>26)|0,xt&=67108863,n=Math.imul(k,V),i=(i=Math.imul(k,U))+Math.imul(T,V)|0,a=Math.imul(T,U),n=n+Math.imul(b,q)|0,i=(i=i+Math.imul(b,G)|0)+Math.imul(_,q)|0,a=a+Math.imul(_,G)|0,n=n+Math.imul(m,W)|0,i=(i=i+Math.imul(m,X)|0)+Math.imul(y,W)|0,a=a+Math.imul(y,X)|0,n=n+Math.imul(d,$)|0,i=(i=i+Math.imul(d,J)|0)+Math.imul(g,$)|0,a=a+Math.imul(g,J)|0;var bt=(c+(n=n+Math.imul(f,Q)|0)|0)+((8191&(i=(i=i+Math.imul(f,tt)|0)+Math.imul(h,Q)|0))>13)|0)+(bt>>>26)|0,bt&=67108863,n=Math.imul(M,V),i=(i=Math.imul(M,U))+Math.imul(S,V)|0,a=Math.imul(S,U),n=n+Math.imul(k,q)|0,i=(i=i+Math.imul(k,G)|0)+Math.imul(T,q)|0,a=a+Math.imul(T,G)|0,n=n+Math.imul(b,W)|0,i=(i=i+Math.imul(b,X)|0)+Math.imul(_,W)|0,a=a+Math.imul(_,X)|0,n=n+Math.imul(m,$)|0,i=(i=i+Math.imul(m,J)|0)+Math.imul(y,$)|0,a=a+Math.imul(y,J)|0,n=n+Math.imul(d,Q)|0,i=(i=i+Math.imul(d,tt)|0)+Math.imul(g,Q)|0,a=a+Math.imul(g,tt)|0;var _t=(c+(n=n+Math.imul(f,rt)|0)|0)+((8191&(i=(i=i+Math.imul(f,nt)|0)+Math.imul(h,rt)|0))>13)|0)+(_t>>>26)|0,_t&=67108863,n=Math.imul(C,V),i=(i=Math.imul(C,U))+Math.imul(L,V)|0,a=Math.imul(L,U),n=n+Math.imul(M,q)|0,i=(i=i+Math.imul(M,G)|0)+Math.imul(S,q)|0,a=a+Math.imul(S,G)|0,n=n+Math.imul(k,W)|0,i=(i=i+Math.imul(k,X)|0)+Math.imul(T,W)|0,a=a+Math.imul(T,X)|0,n=n+Math.imul(b,$)|0,i=(i=i+Math.imul(b,J)|0)+Math.imul(_,$)|0,a=a+Math.imul(_,J)|0,n=n+Math.imul(m,Q)|0,i=(i=i+Math.imul(m,tt)|0)+Math.imul(y,Q)|0,a=a+Math.imul(y,tt)|0,n=n+Math.imul(d,rt)|0,i=(i=i+Math.imul(d,nt)|0)+Math.imul(g,rt)|0,a=a+Math.imul(g,nt)|0;var wt=(c+(n=n+Math.imul(f,at)|0)|0)+((8191&(i=(i=i+Math.imul(f,ot)|0)+Math.imul(h,at)|0))>13)|0)+(wt>>>26)|0,wt&=67108863,n=Math.imul(O,V),i=(i=Math.imul(O,U))+Math.imul(I,V)|0,a=Math.imul(I,U),n=n+Math.imul(C,q)|0,i=(i=i+Math.imul(C,G)|0)+Math.imul(L,q)|0,a=a+Math.imul(L,G)|0,n=n+Math.imul(M,W)|0,i=(i=i+Math.imul(M,X)|0)+Math.imul(S,W)|0,a=a+Math.imul(S,X)|0,n=n+Math.imul(k,$)|0,i=(i=i+Math.imul(k,J)|0)+Math.imul(T,$)|0,a=a+Math.imul(T,J)|0,n=n+Math.imul(b,Q)|0,i=(i=i+Math.imul(b,tt)|0)+Math.imul(_,Q)|0,a=a+Math.imul(_,tt)|0,n=n+Math.imul(m,rt)|0,i=(i=i+Math.imul(m,nt)|0)+Math.imul(y,rt)|0,a=a+Math.imul(y,nt)|0,n=n+Math.imul(d,at)|0,i=(i=i+Math.imul(d,ot)|0)+Math.imul(g,at)|0,a=a+Math.imul(g,ot)|0;var kt=(c+(n=n+Math.imul(f,lt)|0)|0)+((8191&(i=(i=i+Math.imul(f,ct)|0)+Math.imul(h,lt)|0))>13)|0)+(kt>>>26)|0,kt&=67108863,n=Math.imul(P,V),i=(i=Math.imul(P,U))+Math.imul(R,V)|0,a=Math.imul(R,U),n=n+Math.imul(O,q)|0,i=(i=i+Math.imul(O,G)|0)+Math.imul(I,q)|0,a=a+Math.imul(I,G)|0,n=n+Math.imul(C,W)|0,i=(i=i+Math.imul(C,X)|0)+Math.imul(L,W)|0,a=a+Math.imul(L,X)|0,n=n+Math.imul(M,$)|0,i=(i=i+Math.imul(M,J)|0)+Math.imul(S,$)|0,a=a+Math.imul(S,J)|0,n=n+Math.imul(k,Q)|0,i=(i=i+Math.imul(k,tt)|0)+Math.imul(T,Q)|0,a=a+Math.imul(T,tt)|0,n=n+Math.imul(b,rt)|0,i=(i=i+Math.imul(b,nt)|0)+Math.imul(_,rt)|0,a=a+Math.imul(_,nt)|0,n=n+Math.imul(m,at)|0,i=(i=i+Math.imul(m,ot)|0)+Math.imul(y,at)|0,a=a+Math.imul(y,ot)|0,n=n+Math.imul(d,lt)|0,i=(i=i+Math.imul(d,ct)|0)+Math.imul(g,lt)|0,a=a+Math.imul(g,ct)|0;var Tt=(c+(n=n+Math.imul(f,ft)|0)|0)+((8191&(i=(i=i+Math.imul(f,ht)|0)+Math.imul(h,ft)|0))>13)|0)+(Tt>>>26)|0,Tt&=67108863,n=Math.imul(B,V),i=(i=Math.imul(B,U))+Math.imul(N,V)|0,a=Math.imul(N,U),n=n+Math.imul(P,q)|0,i=(i=i+Math.imul(P,G)|0)+Math.imul(R,q)|0,a=a+Math.imul(R,G)|0,n=n+Math.imul(O,W)|0,i=(i=i+Math.imul(O,X)|0)+Math.imul(I,W)|0,a=a+Math.imul(I,X)|0,n=n+Math.imul(C,$)|0,i=(i=i+Math.imul(C,J)|0)+Math.imul(L,$)|0,a=a+Math.imul(L,J)|0,n=n+Math.imul(M,Q)|0,i=(i=i+Math.imul(M,tt)|0)+Math.imul(S,Q)|0,a=a+Math.imul(S,tt)|0,n=n+Math.imul(k,rt)|0,i=(i=i+Math.imul(k,nt)|0)+Math.imul(T,rt)|0,a=a+Math.imul(T,nt)|0,n=n+Math.imul(b,at)|0,i=(i=i+Math.imul(b,ot)|0)+Math.imul(_,at)|0,a=a+Math.imul(_,ot)|0,n=n+Math.imul(m,lt)|0,i=(i=i+Math.imul(m,ct)|0)+Math.imul(y,lt)|0,a=a+Math.imul(y,ct)|0,n=n+Math.imul(d,ft)|0,i=(i=i+Math.imul(d,ht)|0)+Math.imul(g,ft)|0,a=a+Math.imul(g,ht)|0;var At=(c+(n=n+Math.imul(f,dt)|0)|0)+((8191&(i=(i=i+Math.imul(f,gt)|0)+Math.imul(h,dt)|0))>13)|0)+(At>>>26)|0,At&=67108863,n=Math.imul(B,q),i=(i=Math.imul(B,G))+Math.imul(N,q)|0,a=Math.imul(N,G),n=n+Math.imul(P,W)|0,i=(i=i+Math.imul(P,X)|0)+Math.imul(R,W)|0,a=a+Math.imul(R,X)|0,n=n+Math.imul(O,$)|0,i=(i=i+Math.imul(O,J)|0)+Math.imul(I,$)|0,a=a+Math.imul(I,J)|0,n=n+Math.imul(C,Q)|0,i=(i=i+Math.imul(C,tt)|0)+Math.imul(L,Q)|0,a=a+Math.imul(L,tt)|0,n=n+Math.imul(M,rt)|0,i=(i=i+Math.imul(M,nt)|0)+Math.imul(S,rt)|0,a=a+Math.imul(S,nt)|0,n=n+Math.imul(k,at)|0,i=(i=i+Math.imul(k,ot)|0)+Math.imul(T,at)|0,a=a+Math.imul(T,ot)|0,n=n+Math.imul(b,lt)|0,i=(i=i+Math.imul(b,ct)|0)+Math.imul(_,lt)|0,a=a+Math.imul(_,ct)|0,n=n+Math.imul(m,ft)|0,i=(i=i+Math.imul(m,ht)|0)+Math.imul(y,ft)|0,a=a+Math.imul(y,ht)|0;var Mt=(c+(n=n+Math.imul(d,dt)|0)|0)+((8191&(i=(i=i+Math.imul(d,gt)|0)+Math.imul(g,dt)|0))>13)|0)+(Mt>>>26)|0,Mt&=67108863,n=Math.imul(B,W),i=(i=Math.imul(B,X))+Math.imul(N,W)|0,a=Math.imul(N,X),n=n+Math.imul(P,$)|0,i=(i=i+Math.imul(P,J)|0)+Math.imul(R,$)|0,a=a+Math.imul(R,J)|0,n=n+Math.imul(O,Q)|0,i=(i=i+Math.imul(O,tt)|0)+Math.imul(I,Q)|0,a=a+Math.imul(I,tt)|0,n=n+Math.imul(C,rt)|0,i=(i=i+Math.imul(C,nt)|0)+Math.imul(L,rt)|0,a=a+Math.imul(L,nt)|0,n=n+Math.imul(M,at)|0,i=(i=i+Math.imul(M,ot)|0)+Math.imul(S,at)|0,a=a+Math.imul(S,ot)|0,n=n+Math.imul(k,lt)|0,i=(i=i+Math.imul(k,ct)|0)+Math.imul(T,lt)|0,a=a+Math.imul(T,ct)|0,n=n+Math.imul(b,ft)|0,i=(i=i+Math.imul(b,ht)|0)+Math.imul(_,ft)|0,a=a+Math.imul(_,ht)|0;var St=(c+(n=n+Math.imul(m,dt)|0)|0)+((8191&(i=(i=i+Math.imul(m,gt)|0)+Math.imul(y,dt)|0))>13)|0)+(St>>>26)|0,St&=67108863,n=Math.imul(B,$),i=(i=Math.imul(B,J))+Math.imul(N,$)|0,a=Math.imul(N,J),n=n+Math.imul(P,Q)|0,i=(i=i+Math.imul(P,tt)|0)+Math.imul(R,Q)|0,a=a+Math.imul(R,tt)|0,n=n+Math.imul(O,rt)|0,i=(i=i+Math.imul(O,nt)|0)+Math.imul(I,rt)|0,a=a+Math.imul(I,nt)|0,n=n+Math.imul(C,at)|0,i=(i=i+Math.imul(C,ot)|0)+Math.imul(L,at)|0,a=a+Math.imul(L,ot)|0,n=n+Math.imul(M,lt)|0,i=(i=i+Math.imul(M,ct)|0)+Math.imul(S,lt)|0,a=a+Math.imul(S,ct)|0,n=n+Math.imul(k,ft)|0,i=(i=i+Math.imul(k,ht)|0)+Math.imul(T,ft)|0,a=a+Math.imul(T,ht)|0;var Et=(c+(n=n+Math.imul(b,dt)|0)|0)+((8191&(i=(i=i+Math.imul(b,gt)|0)+Math.imul(_,dt)|0))>13)|0)+(Et>>>26)|0,Et&=67108863,n=Math.imul(B,Q),i=(i=Math.imul(B,tt))+Math.imul(N,Q)|0,a=Math.imul(N,tt),n=n+Math.imul(P,rt)|0,i=(i=i+Math.imul(P,nt)|0)+Math.imul(R,rt)|0,a=a+Math.imul(R,nt)|0,n=n+Math.imul(O,at)|0,i=(i=i+Math.imul(O,ot)|0)+Math.imul(I,at)|0,a=a+Math.imul(I,ot)|0,n=n+Math.imul(C,lt)|0,i=(i=i+Math.imul(C,ct)|0)+Math.imul(L,lt)|0,a=a+Math.imul(L,ct)|0,n=n+Math.imul(M,ft)|0,i=(i=i+Math.imul(M,ht)|0)+Math.imul(S,ft)|0,a=a+Math.imul(S,ht)|0;var Ct=(c+(n=n+Math.imul(k,dt)|0)|0)+((8191&(i=(i=i+Math.imul(k,gt)|0)+Math.imul(T,dt)|0))>13)|0)+(Ct>>>26)|0,Ct&=67108863,n=Math.imul(B,rt),i=(i=Math.imul(B,nt))+Math.imul(N,rt)|0,a=Math.imul(N,nt),n=n+Math.imul(P,at)|0,i=(i=i+Math.imul(P,ot)|0)+Math.imul(R,at)|0,a=a+Math.imul(R,ot)|0,n=n+Math.imul(O,lt)|0,i=(i=i+Math.imul(O,ct)|0)+Math.imul(I,lt)|0,a=a+Math.imul(I,ct)|0,n=n+Math.imul(C,ft)|0,i=(i=i+Math.imul(C,ht)|0)+Math.imul(L,ft)|0,a=a+Math.imul(L,ht)|0;var Lt=(c+(n=n+Math.imul(M,dt)|0)|0)+((8191&(i=(i=i+Math.imul(M,gt)|0)+Math.imul(S,dt)|0))>13)|0)+(Lt>>>26)|0,Lt&=67108863,n=Math.imul(B,at),i=(i=Math.imul(B,ot))+Math.imul(N,at)|0,a=Math.imul(N,ot),n=n+Math.imul(P,lt)|0,i=(i=i+Math.imul(P,ct)|0)+Math.imul(R,lt)|0,a=a+Math.imul(R,ct)|0,n=n+Math.imul(O,ft)|0,i=(i=i+Math.imul(O,ht)|0)+Math.imul(I,ft)|0,a=a+Math.imul(I,ht)|0;var zt=(c+(n=n+Math.imul(C,dt)|0)|0)+((8191&(i=(i=i+Math.imul(C,gt)|0)+Math.imul(L,dt)|0))>13)|0)+(zt>>>26)|0,zt&=67108863,n=Math.imul(B,lt),i=(i=Math.imul(B,ct))+Math.imul(N,lt)|0,a=Math.imul(N,ct),n=n+Math.imul(P,ft)|0,i=(i=i+Math.imul(P,ht)|0)+Math.imul(R,ft)|0,a=a+Math.imul(R,ht)|0;var Ot=(c+(n=n+Math.imul(O,dt)|0)|0)+((8191&(i=(i=i+Math.imul(O,gt)|0)+Math.imul(I,dt)|0))>13)|0)+(Ot>>>26)|0,Ot&=67108863,n=Math.imul(B,ft),i=(i=Math.imul(B,ht))+Math.imul(N,ft)|0,a=Math.imul(N,ht);var It=(c+(n=n+Math.imul(P,dt)|0)|0)+((8191&(i=(i=i+Math.imul(P,gt)|0)+Math.imul(R,dt)|0))>13)|0)+(It>>>26)|0,It&=67108863;var Dt=(c+(n=Math.imul(B,dt))|0)+((8191&(i=(i=Math.imul(B,gt))+Math.imul(N,dt)|0))>13)|0)+(Dt>>>26)|0,Dt&=67108863,l[0]=vt,l[1]=mt,l[2]=yt,l[3]=xt,l[4]=bt,l[5]=_t,l[6]=wt,l[7]=kt,l[8]=Tt,l[9]=At,l[10]=Mt,l[11]=St,l[12]=Et,l[13]=Ct,l[14]=Lt,l[15]=zt,l[16]=Ot,l[17]=It,l[18]=Dt,0!==c&&(l[19]=c,r.length++),r};function d(t,e,r){return(new g).mulp(t,e,r)}function g(t,e){this.x=t,this.y=e}Math.imul||(p=h),a.prototype.mulTo=function(t,e){var r=this.length+t.length;return 10===this.length&&10===t.length?p(this,t,e):r>>26,o&=67108863}r.words[a]=s,n=o,o=i}return 0!==n?r.words[a]=n:r.length--,r.strip()}(this,t,e):d(this,t,e)},g.prototype.makeRBT=function(t){for(var e=new Array(t),r=a.prototype._countBits(t)-1,n=0;n>16,this[e+2]=t>>>8,this[e+3]=255&t,e+4},s.prototype.writeIntLE=function(t,e,r,n){if(t=+t,e>>>=0,!n){var i=Math.pow(2,8*r-1);z(this,t,e,r,i-1,-i)}var a=0,o=1,s=0;for(this[e]=255&t;++a0)-s&255;return e+r},s.prototype.writeIntBE=function(t,e,r,n){if(t=+t,e>>>=0,!n){var i=Math.pow(2,8*r-1);z(this,t,e,r,i-1,-i)}var a=r-1,o=1,s=0;for(this[e+a]=255&t;--a>=0&&(o*=256);)t>0)-s&255;return e+r},s.prototype.writeInt8=function(t,e,r){return t=+t,e>>>=0,r||z(this,t,e,1,127,-128),t>>=0,r||z(this,t,e,2,32767,-32768),this[e]=255&t,this[e+1]=t>>>8,e+2},s.prototype.writeInt16BE=function(t,e,r){return t=+t,e>>>=0,r||z(this,t,e,2,32767,-32768),this[e]=t>>>8,this[e+1]=255&t,e+2},s.prototype.writeInt32LE=function(t,e,r){return t=+t,e>>>=0,r||z(this,t,e,4,2147483647,-2147483648),this[e]=255&t,this[e+1]=t>>>8,this[e+2]=t>>>16,this[e+3]=t>>>24,e+4},s.prototype.writeInt32BE=function(t,e,r){return t=+t,e>>>=0,r||z(this,t,e,4,2147483647,-2147483648),t>>24,this[e+1]=t>>>16,this[e+2]=t>>>8,this[e+3]=255&t,e+4},s.prototype.writeFloatLE=function(t,e,r){return I(this,t,e,!0,r)},s.prototype.writeFloatBE=function(t,e,r){return I(this,t,e,!1,r)},s.prototype.writeDoubleLE=function(t,e,r){return D(this,t,e,!0,r)},s.prototype.writeDoubleBE=function(t,e,r){return D(this,t,e,!1,r)},s.prototype.copy=function(t,e,r,n){if(!s.isBuffer(t))throw new TypeError(\"argument should be a Buffer\");if(r||(r=0),n||0===n||(n=this.length),e>=t.length&&(e=t.length),e||(e=0),n>0&&n","link":"/others/plots/pyconkr_2019_naver_201906_similar_gte10.html"}],"posts":[{"title":"pip로 mysqlclient설치 중 mac os x에서 egg_info / OSError 발생시 대처방법","text":"(venv) Beomiui-MacBook:Downloads beomi$ pip install mysqlclient-1.3.7.tar.gz Processing ./mysqlclient-1.3.7.tar.gz Complete output from command python setup.py egg_info: /bin/sh: mysql_config: command not found Traceback (most recent call last): File &quot;&quot;, line 20, in File &quot;/var/folders/52/v_mf5ys167q67b6cn_hnfzg00000gn/T/pip-0zi6xkoz-build/setup.py&quot;, line 17, in metadata, options = get_config() File &quot;/private/var/folders/52/v_mf5ys167q67b6cn_hnfzg00000gn/T/pip-0zi6xkoz-build/setup_posix.py&quot;, line 44, in get_config libs = mysql_config(&quot;libs_r&quot;) File &quot;/private/var/folders/52/v_mf5ys167q67b6cn_hnfzg00000gn/T/pip-0zi6xkoz-build/setup_posix.py&quot;, line 26, in mysql_config raise EnvironmentError(&quot;%s not found&quot; % (mysql_config.path,)) OSError: mysql_config not found ---------------------------------------- Command &quot;python setup.py egg_info&quot; failed with error code 1 in /var/folders/52/v_mf5ys167q67b6cn_hnfzg00000gn/T/pip-0zi6xkoz-build You are using pip version 7.1.2, however version 8.1.2 is available. You should consider upgrading via the &apos;pip install --upgrade pip&apos; command.​ (venv) Beomiui-MacBook:Downloads beomi$ brew install mysql ==&gt; Downloading https://homebrew.bintray.com/bottles/mysql-5.7.12.el_capitan.bottle.tar.gz ######################################################################## 100.0% ==&gt; Pouring mysql-5.7.12.el_capitan.bottle.tar.gz ==&gt; /usr/local/Cellar/mysql/5.7.12/bin/mysqld –initialize-insecure –user=beomi –basedir=/usr/local/Cellar/mysql/5.7.12 –datadir=/usr/local/var/mysql –tmpdir=/tmp ==&gt; Caveats We’ve installed your MySQL database without a root password. To secure it run: mysql_secure_installation To connect run: mysql -uroot To have launchd start mysql now and restart at login: brew services start mysql Or, if you don’t want/need a background service you can just run: mysql.server start ==&gt; Summary 🍺 /usr/local/Cellar/mysql/5.7.12: 13,281 files, 444.8M (venv) Beomiui-MacBook:Downloads beomi$ pip install mysqlclient-1.3.7.tar.gz Processing ./mysqlclient-1.3.7.tar.gz Installing collected packages: mysqlclient Running setup.py install for mysqlclient Successfully installed mysqlclient-1.3.7 You are using pip version 7.1.2, however version 8.1.2 is available. You should consider upgrading via the &apos;pip install --upgrade pip&apos; command.이와 같이 brew install mysql을 진행하면 된다.","link":"/2016/05/27/pipeba19c-mysqlclientec84a4ecb998-eca491-mac-os-xec9790ec849c-egg_info-oserror-ebb09cec839dec8b9c-eb8c80ecb298ebb0a9ebb295/"},{"title":"Fabric for Python3 (Fabric3)","text":"Python3에서 Fabric을 설치해 사용하려하면 이와 같이 &lt;span class=&quot;s1&quot;&gt;ImportError: cannot import name &apos;isMappingType&apos;&lt;/span&gt;라는 임포트 에러를 마주치게 된다. 이렇게 되는 이유는 python2에서 지원하던 isMappingType 모듈이 python3에서는 제거되었기 때문이고, fabric모듈을 제작하는 FabFile(http://www.fabfile.org)에서는 현재 python2.7까지만 지원하기 때문이다. 따라서 python3 환경에서는 fabric을 사용할 수 없게 되는데, 이러한 경우가 많아서 해외 python이용자가 기존 프로젝트를 포크해 python3(3.4+)으로 만든 것이 있다. 바로 Fabric3이라는 이름으로 Pypi에서 제공중인 모듈이라 설치도 간단히, ​ pip3 install fabric3 위 명령만으로 설치할 수 있다. 기존에 fabric을 설치했다면 ​ pip3 uninstall fabric deactivate (virtualenv / pyvenv 등의 가상환경을 이용해 작업중이던 경우, 필수!) source 가상환경/bin/activate (가상환경 활성화) pip3 install fabric (만약 python3버전의 가상환경을 사용중이었다면 pip3대신 pip로 사용해도 무방하다) 가상환경을 이용중일 경우 비활성화 후 다시 활성화 하지 않고 그대로 이용할 경우 메모리에 올라가 있는 python실행 상태로 인하여 여전히 에러가 날 수 있다. 따라서 반드시 비활성화 후 다시 활성화 하기를 권장한다. 위 과정을 마친 경우 fabric을 사용하듯이 자연스럽게 fabric을 이용할수 있다!","link":"/2016/07/17/fabric-for-python3-fabric3/"},{"title":"Windows에서 pip로 mysqlclient 설치 실패시(python3.4/3.5)","text":"윈도우 python3(3.4/3.5)에서 pip로 mysqlclient를 설치하려 시도시 아래와 같은 에러를 만날 수 있다. 에러 내용은 MS VS C++ 10.0(py3.4) / MS VS C++ 14.0(py3.5)를 설치해달라는 내용이다. 하지만 VS C++은 설치하는데 용량도 꽤 크고 설치시간도 오래걸려서 부담이 크다. 그리고 설령 설치를 하더라도 깔끔하게 맵핑이 되지 않는 경우도 많다. 그래서 차선책으로 mingw를 사용해 gcc컴파일러를 설치해 사용하는 경우도 있지만, 시스템에 설치된 python의 설정을 변경해줘야하기 때문에 라이트유저에게는 부담이 된다. 다행히 mysqlclient의 경우에는 win32/64, py3.4/3.5용으로 미리 컴파일된 pip용 whl파일을 제공한다. http://www.lfd.uci.edu/~gohlke/pythonlibs/#mysqlclient위 링크에서 윈도우에 설치한 python버전(3.4인지, 3.5인지 / 32비트인지, 64비트인지)을 확인후 whl파일을 받는다. 다운받은 후 whl 파일이 있는 곳에서 pip install [다운받은파일이름](아래에서는 mysqlclient-1.3.7-cp34-none-win32.whl) 을 입력하면 말끔하게 설치됨을 알 수 있다.","link":"/2016/06/04/windowsec9790ec849c-pipeba19c-mysqlclient-ec84a4ecb998-ec8ba4ed8ca8ec8b9cpython3-43-5/"},{"title":"Ubuntu14.04에 OhMyZsh 설치","text":"이 글은 root 계정에서 작업되었습니다.VPS등으로 SSH접속을 할 때 bash가 아닌 zsh+OhMyZsh조합을 이용하면 좀 더 편한 관리가 가능하다. VPS에 접속 한 후 가장 먼저 해야 하는 작업, apt 패키지를 최신 상태로 업데이트+업그레이드 하기. apt update &amp;&amp; upgrade OhMyZsh 설치에는 git이 사용된다. zsh와 git을 설치해 주자. apt install zsh git Zsh을 좀 더 예쁘게 사용하기 위한 방법, Oh My Zsh. (http://ohmyz.sh) curl을 이용해서 받기 원한다면 sh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;wget을 이용해 받기 원한다면 sh -c &quot;$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)&quot;본 포스트에서는 curl을 이용한다. 설치가 완료되면 아래와 같이 zsh로 연결된다.","link":"/2016/07/22/ubuntu14-04ec9790-ohmyzsh-ec84a4ecb998/"},{"title":"Ubuntu14.04에서 pip로 mysqlclient 설치 실패시","text":"아래 내용은 root 계정 + virtualenv환경에서 진행됩니다. Ubuntu 14.04 LTS / Python3.4.4 에서 pip로 mysqlclient를 설치하려고 하면 다음과 같은 에러가 발행한다. OSError: mysql_config not found이 에러는 다음의 apt 패키지 설치로 해결할 수 있다. apt install libmysqlclient-dev그러나, 이 모듈을 설치한 이후에도 아래와 같은 에러가 발생 할 수 있다. unable to execute &apos;x86_64-linux-gnu-gcc&apos;: No such file or directory즉, GCC가 없어 컴파일을 할 수 없다는 에러이기 때문에 다음과 같이 build-essential을 설치해줘야 한다. apt install build-essential 다음을 모두 설치한 후에는 위와 같이 설치가 깔끔하게 된다.","link":"/2016/07/22/ubuntu14-04ec9790ec849c-pipeba19c-mysqlclient-ec84a4ecb998-ec8ba4ed8ca8ec8b9c/"},{"title":"CustoMac  설치 분투기","text":"기본적으로 커스텀맥(해킨토시) 설치는 큰 어려움이 따른다. 그런데, 이번에 설치한 OS X El Capitan은 기본적인 설정만으로도 상당히 쉽게 설치되었기 때문에(그래도 약간의 설정은 필요하다) 매우 놀랐다. 설치 PC사양은 다음과 같다. CPU: I5-4670 (Haswell, 하스웰) RAM: DDR3 8Gx2 (&lt;span id=&quot;grpDescrip_&quot;&gt;G.SKILL Ripjaws X Series 16GB (2 x 8GB)&lt;/span&gt;) M/B: ASRock ASRock H87 Performance 에즈윈 HDD: WD Blue 1TB (8MB cache / 2.5&quot;) VGA: AMD HD7970 3G (HIS 라데온 HD 7970 기가에디션 OC D5 3GB IceQ X² 잘만테크)설치를 시도한 OS는 OS X 10.11.6 (El Capitan)버전이었고, 설치 디스크 제작은 순정 맥을 이용해 제작했다. 설치에 사용한 Kext와 여러 프로그램들은 아래 드롭박스 링크에서 받을 수 있다. PW:installosx [ 다운로드 ] [설치 과정]가장 먼저 해야 하는 것은 바로 순정 설치 디스크를 제작하는 것이다. 순정 El Capitan을 받기 위해 맥의 AppStore에서 El Capitan을 다운로드 하면 LaunchPad에 저장된다. 저장이 완료되면 설치 창이 뜨는데, 간단히 무시하고 TonyMac의 Unibeast를 통해 GUI로 부팅 디스크를 만든다. El Capitan / UEFI만 선택하면 부팅 디스크를 아주 깔끔하게 만들어준다. 부팅 및 설치는 전면 USB3.0단자를 이용해 이루어졌다.(혹자는 usb2.0으로 진행하라고 했지만, 큰 문제 없이 진행되었다.) USB에는 여러 프로그램(MultiBeast / Clover / CloverConfigurator / 기타 kexts/Scripts)을 담아두면 OS X 설치 이후에 좀더 편안하게 설치 마무리를 진행할 수 있다. 위쪽에 올려둔 드롭박스 링크의 압축파일에는 Clover Configuator : EFI 파티션 자동인식 및 마운트 기능 / config.plist 수정기능 제공 / Clover 자동 확인 및 업데이트 제공 Clover : 클로버 부트로더. OS X으로 부팅 가능하게 만들어준다. MultiBeast : OS X의 각종 드라이버(kext파일들)를 EFI파티션에 잡아준다. 이번설치에서는 굳이 쓰지 않아도 성공적으로 설치됨. Kext파일들 : 위 HW에 필요했던 기본적인 kext들. EFI파티션의 KEXTS폴더에 10.11버전 폴더에 넣어주면 된다. MenuMeters : OS X 설치 후 시스템 상태를 메뉴바에서 그래픽으로 볼 수 있게 지원. OS X설치후 항상 쓰는 프로그램이라 넣었을 뿐, 운영체제 설치 자체와는 관련이 전혀 없다. AudioCloverHDMI : OS X에서 기본적으로 지원하지 않는 HDMI 오디오 출력을 가능하게 만들어준다. 스크립트형. 이렇게 구성되어있다. +alpha: 이번에 사용한 메인보드가 ASRock보드인데, 부트로더가 efi파일을 잡지 않아 버그가 난 경우가 있었다. https://www.x86.co.kr:447/qa/1274453 클로버를 저렇게 설치해주면 된다.","link":"/2016/08/09/customac-ec84a4ecb998-ebb684ed88aceab8b0/"},{"title":"Chrome Native Adblockr 대체하기","text":"오늘 크롬을 켜고 인터넷 브라우징을 하는데 일상적으로라면 Native Adblockr로 인해 광고가 뜨지 않아야 하는 곳에서 광고가 떴다. 크롬에 들어가보니 크롬측에서 앱을 비활성화 한 것. 재활성화 하려 해도 크롬에서 중단시켰다며 활성화가 되지 않았다. 그래서 앱의 최근 리뷰 중 개발자 댓글로 프로그램이 구성된 방법이 uBlock + 커스텀필터 라고 하고, 커스텀 필터는 개발자 깃헙 소스인 https://raw.githubusercontent.com/NativeHyun/HyunGuard/master/General/general.txt이 파일을 참고해 작동한다고 했다. 그래서 커스텀 Chrome Native Adblockr를 구성해보기로 생각. 크롬 웹 스토어에서 uBlock을 검색, 두가지가 나왔다. uBlock orign와 uBlock. 전자가 더 많은 리뷰가 있어 전자로 다운받았다. 그리고 uBlock의 세팅(크롬 우측상단 앱 아이콘 클릭시 나오는 팝업의 좌측 상단에 작은 설정아이콘)에 들어가 아래와 같이 사용자 필터로 위의 깃헙소스를 추가해 준다. 그러면 Native Adblockr와 정확히 동일하게 광고가 차단된다. Ps. 왜 앱이 내려간건지 모르겠다..^^;;","link":"/2016/09/14/chrome-native-adblockr-eb8c80ecb2b4ed9598eab8b0/"},{"title":"Ubuntu14.04에서 Python3기반 virtualenvwrapper 설치","text":"우분투 14.04에는 기본적으로 Python2와 Python3이 설치되어있다. 그러나 pip로 virtualenv를 설치할 경우 기본적으로 python2를 가상환경의 기본 Python으로 잡게 되는데,이번 게시글에서는 mkvirtualenv명령어의 기본값을 python3으로 설정하는 방법을 안내한다. 따라서 virtualenv를 사용하기 위해서는 APT를 통해 다음 모듈들을 설치한다. apt update &amp;&amp; upgrade apt install python-dev python3-dev python3-pip(참고: python-pip는 python2용 pip, python3-pip는 python3용 pip3을 설치한다.) pip3 install virtualenv virtualenvwrapper설치가 완료된 후, nano / vi / vim 등의 편집기로 bash 쉘을 사용할 경우 nano ~/.bashrczsh 쉘을 사용할 경우 nano ~/.zshrc편집기에 들어가 # python virtualenv settings export WORKON_HOME=~/.virtualenvs export VIRTUALENVWRAPPER_PYTHON=&quot;$(command \\which python3)&quot; # location of python3 source /usr/local/bin/virtualenvwrapper.sh위 내용을 붙여준다. 네번째 문장에서의 virtualenvwrapper.sh는 find /usr -name virtualenvwrapper.sh을 통해 나오는 스크립트의 위치로 지정해 주면 된다. bashrc나 zshrc의 수정이 끝난 경우 ​ mkdir ~/.virtualenvs 를 통해 virtualenv들이 담길 폴더를 만든다. 이제 쉘을 종료한 후 다시 연결한 후 ​ mkvirtualenv 가상환경이름 하면 ~/.virtualenvs 안에 가상환경이 생긴다. ​ workon 가상환경이름 을 통해 가상환경을 활성화 시킬 수 있으며, 가상환경 이름을 입력하지 않는 경우 가상환경의 리스트가 출력된다. 가상환경을 빠져나오기 위해서는 ​ deactivate 를 입력하면 된다.","link":"/2016/07/22/ubuntu14-04ec9790ec849c-python3eab8b0ebb098-virtualenvwrapper-ec84a4ecb998/"},{"title":"mac OS X에서 pip virtualenvwrapper 설치 시 uninstalling six 에서 Exception 발생 시","text":"Mac OS X El Capitan(10.11.5)에서 pip로 virtualenvwrapper를 설치 시도시 six-1.4.1버전을 제거하는데 권한이 없다고 나온다. Sudo를 통해 관리자 권한으로 실행해도 같은 오류가 발생하는데, 이것은https://github.com/pypa/pip/issues/3165이슈에서 답을 찾을 수 있다. 바로 Mac OS X El Capitan의 System Integrity Protection때문이다. ROOT 계정으로도 제거하지 못하기 때문에, 해결방법은 다음과 같다. pip install virtualenvwrapper --ignore-installed six위의 옵션으로 내장된 모듈 six를 건너뛰고 설치하게 만드는 것이다.","link":"/2016/07/21/mac-os-xec9790ec849c-pip-virtualenvwrapper-ec84a4ecb998-ec8b9c-uninstalling-six-ec9790ec849c-exception-ebb09cec839d-ec8b9c/"},{"title":"맥에서 윈도RDP로 접속시 한영전환하기.","text":"맥에서 앱스토어에서 받은 윈도 RDP를 통해 원격 윈도 서버로 접속시, 본인의 경우 구름입력기의 조합으로 한영 전환을 Shift + Space로 하고 있다. 이때, 원격 접속된 컴퓨터로 연결을 해도 문자 입력시 문자가 들어가는게 아니라 입력값이 들어가는 방식이기에 원격 서버의 OS가 입력을 받아 처리하게 된다. 즉, 맥북에서는 a라고 쳐도 윈도(서버)에서는 한글입력중이어서 ㅁ이라 받아들일 수 있다는 것이다. 이런 경우 해결책은 아주 간단하다. 위 스크린샷과 같이 한국어 입력기로 전환해 두면, 맥북 키보드의 우측 ALT(option) Key가 윈도의 한/영키로 작용한다!","link":"/2016/05/27/eba7a5ec9790ec849c-ec9c88eb8f84rdpeba19c-eca091ec868dec8b9c-ed959cec9881eca084ed9998ed9598eab8b0/"},{"title":"ReactNative The Basis 번역을 끝냈습니다.","text":"React Native 0.37버전의 The Basis 번역을 마쳤습니다. 현재 작업은 ReactNative Korean Translated v0.37에 올려져 있습니다.","link":"/2016/12/20/ReactNative-Translation-Intro-Finish/"},{"title":"[React Native 번역]#01: 시작하기","text":"React Native의 ​Tutorial번역 시리즈입니다.원문: getting-started이번 번역은 현재(2016.11.15) 최신 Stable인 0.37버전의 문서를 번역하였습니다. #01: 시작하기React Native에 오신 것을 환영합니다!이번 게시글에서는 React Native를 여러분의 시스템에 설치하고 곧바로 실행 할 수 있도록 안내합니다.만약 여러분이 이미 React Native를 설치해두셨다면, 이번 게시글을 건너뛰고 튜토리얼로 가셔도 됩니다. 이번 게시글은 여러분이 어떤 시스템을 사용하는지에 따라 약간 내용이 다르고, iOS/Android중 어떤 것을 사용하느냐에 따라서도 내용이 다르답니다.iOS와 Android 모두 개발하는 것도 당연히 가능합니다! 여기서는 그냥 시작을 어떤 환경으로 할지 정하는 것 뿐입니다. 1주의: iOS개발은 Xcode가 깔려있는 macOS(OS X)환경에서만 가능합니다. 필요한 의존패키지들 설치하기(macOS/OS X에서) 윈도우에서 의존 패키지를 설치하는 것은 문서 아래쪽의 윈도 의존 패키지 설치하기를 참고해주세요. React Native를 개발하기 위해서여러분은 node.js와 Watchman, React Native command line interface와 \u0018Xcode가 필요합니다. Node, Watchman, react-native-cli 설치하기 (iOS, Android 공통)node와 Watchman을 Homebrew를 통해 설치하는 것을 권장합니다.터미널에서 아래와 같이 입력해 설치할 수 있답니다. 12brew install nodebrew install watchman Watchman은 facebook에서 파일 시스템의 변화를 감시하는 용도로 사용하는 툴입니다.더 나은 퍼포먼스를 위해 사용을 강력히 추천합니다! 위에서 node와 함께 npm이 자동적으로 설치되었을거랍니다. 아래 명령어로 react-native-cli를 설치해주세요! 1npm install -g react-native-cli (번역주)-g 옵션은 시스템 전체에서 사용할 수 있도록 시스템 영역에 설치한다는 뜻입니다. 만약, 권한 문제로 설치에 실패하신다면 sudo npm install -g react-native-cli로 설치해보세요! 만약 Cannot find module 'npmlog'같은 에러를 만나셨다면, npm을 다음 명령어로 수동 설치해보세요.: curl -0 -L http://npmjs.org/install.sh | sudo sh. Xcode 설치하기 (iOS개발환경)Xcode를 설치하는 가장 쉬운 방법은 Mac App Store에서 받는 방법입니다. 앱스토어에서 Xcode를 설치하면 iOS시뮬레이터와 iOS App빌드를 위한 여러 툴들이 자동으로 함께 설치됩니다. Android Studio 설치하기 (안드로이드 개발환경)만약 여러분이 안드로이드를 개발하는 것이 처음이라면, 개발환경을 갖추는 것은 약간 까다로울 수 있습니다.이미 안드로이드 개발을 하고있던 환경이라면, 설정을 거의 건드리지 않아도 React Native로 개발을 시작하실 수 있습니다.둘 중 어떻든, 아래 과정을 확인해보세요. 1. Android Studio 받고 설치하기.Android Studio는 여러분의 React Native앱을 실행시킬 Android SDK와 AVD(안드로이드 VM)를 제공해줍니다. Android Studio는 Java Development Kit (JDK) version 1.8 이상을 필요로 합니다. 터미널에서 javac -version 명령어로 몇 버전의 JDK가 깔려있는지 확인해보세요! 2. AVD, HAXM 설치하기Android Studio 설치를 시작할 때 custom옵션으로 설치를 진행해 주세요.다음 문항들이 다 체크되어있는지 꼭 확인해보세요: Android SDK Android SDK Platform Performance (Intel ® HAXM) Android Virtual Device 다 체크되어있다면, “Next”를 눌러 설치를 진행해 주세요. (역자주) HAXM이 없어도 여전히 사용은 가능합니다. 그러나 안드로이드 가상머신의 성능이 저하될 수 있고, 이 옵션은 사용하시는 시스템에 따라 사용가능 유무가 달라지므로 크게 신경쓰지 않으셔도 됩니다. 만약 Android Studio를 이미 설치하셨더라도, Android Studio 재설치 없이 HAXM 설치를 하실 수 있습니다. 3. Android 6.0 (마시멜로) SDK 설치하기안드로이드 스튜디오에서는 기본적으로 최신 버전의 Android SDK를 설치해줍니다. 하지만 React Native(0.37)에서는 안드로이드6.0(마시멜로) SDK(역자주)정확히는 v23.1를 사용합니다. 이 SDK는 “Welcome to Android Studio” 화면에서 SDK Manager를 실행하고, Configure탭에서 설치하실 수 있습니다. 또다른 방법으로는, 안드로이드 스튜디오의 “Preferences” 메뉴 아래 Appearance &amp; Behavior → System Settings → Android SDK 으로 들어갈 수 있습니다. SDK Manager에서 “SDK Platforms”을 누른 후, “Show Package Details”를 눌러보세요. Android 6.0 (Marshmallow)를 찾으신 후, 아래 목록들이 모두 체크되어있는지 확인해 보세요: Google APIs Intel x86 Atom System Image Intel x86 Atom_64 System Image Google APIs Intel x86 Atom_64 System Image 위 그림에서는 7.0만 설치되어있는걸 보실 수 있습니다. 6.0을 체크해주세요! 다 체크되어있다면, “SDK Tools”탭을 눌러보세요. “Android SDK Build Tools”를 펼쳐보시고, Android SDK Build-Tools 23.0.1가 깔려 있는지(체크되어있는지) 확인해 보세요. 마지막으로, Apply버튼을 눌러 SDK와 빌드 도구를 설치해주세요. 4. ANDROID_HOME 환경변수 설정하기React Native command-line interface는 ANDROID_HOME 환경변수를 필요로 합니다. 아래 코드들을 ~/.bashrc (혹은 이와 같은 것들)의 마지막에 넣어주세요. 123export ANDROID_HOME=~/Library/Android/sdkexport PATH=${PATH}:${ANDROID_HOME}/toolsexport PATH=${PATH}:${ANDROID_HOME}/platform-tools ANDROID_HOME이 저 위치가 맞는지 꼭! 확인해주세요. 만약, Homebrew를 통해서 Android SDK를 설치하셨다면 아마도 /usr/local/opt/android-sdk가 ANDROID_HOME이 될겁니다. 등록한 환경변수는 터미널을 재실행 한 이후에 적용됩니다. 5. 안드로이드 가상 머신 시작하기 안드로이드 스튜디오에서 “AVD Manager”를 열어보면 지금 시스템에 깔려있는 이용가능한 안드로이드 VM의 목록이 나타납니다.아래 명령어를 터미널에서 입력해도 볼 수 있어요. 1android avd “AVD Manager”에 들어가신 후 AVD를 선택하고 “Start…”를 클릭하시면 안드로이드 VM이 실행됩니다. 보통의 경우 안드로이드 스튜디오 설치 중 AVD도 설치되지만, 안드로이드 스튜디오 설치 중 AVD가 설치되지 않는 경우는 흔한 경우랍니다. 다음 가이드Android Studio User Guide를 따라서 새로운 AVD를 수동으로 만드실 수도 있습니다. React Native 설치 테스트하기 (iOS VM으로 확인하기)React Native command line interface를 이용해 새로운 React Native 프로젝트를 시작해볼게요.“AwesomeProject”라는 멋진 이름을 가진 프로젝트로요 :)다음 명령어들을 따라치면 프로젝트가 생기고 가상 iOS머신에서 우리의 프로젝트가 곧장 실행될거에요! 123react-native init AwesomeProjectcd AwesomeProjectreact-native run-ios 조금만 기다리면 우리의 AwesomeProject가 실행된 모습을 보실 수 있을거에요. react-native run-ios명령어는 우리의 앱을 실행하는 방법 중 하나일 뿐이랍니다. Xcode에서 실행하셔도 되고, Nuclide를 통해 실행하셔도 됩니다. React Native 설치 테스트하기 (Android VM으로 확인하기)만약 바로 위에있는 iOS로 테스트를 해본 상태라면, 마지막 줄의 react-native run-android만 입력하세요.React Native command line interface를 이용해 새로운 React Native 프로젝트를 시작해볼게요.“AwesomeProject”라는 멋진 이름을 가진 프로젝트로요 :)다음 명령어들을 따라치면 프로젝트가 생기고 가상 안드로이드 머신에서 우리의 프로젝트가 곧장 실행될거에요! 123react-native init AwesomeProjectcd AwesomeProjectreact-native run-android 만약 모든 환경이 제대로 설정되었다면, 안드로이드 VM하나가 실행되고 우리 앱이 안드로이드에 뜰거랍니다.react-native run-android명령어는 우리 앱을 실행하는 방법 중 하나일 뿐이랍니다. Android Studio에서 실행하셔도 되고, Nuclide를 이용하셔도 됩니다. 앱 수정해보기만약 위에서 성공적으로 앱이 실행되었다면, 약간 수정을 해봅시다! iOS 앱 수정하기 index.ios.js 파일을 열고 몇몇 줄을 수정해 보세요. Command⌘ + R을 눌러서 iOS Simulator를 다시 로딩해 어떤 변화가 있는지 확인해보세요. Android 앱 수정하기 index.android.js 파일을 열고 몇몇 줄을 수정해 보세요. R키를 두번 누르거나 Reload를 개발자메뉴에서 눌러 어떤 변화가 있는지 확인해보세요. 이게 끝이에요!축하합니다! 여러분은 성공적으로 React Native앱을 실행하고 수정까지 해 보셨어요. 이젠 뭘해야할까요? 만약 이미 존재하는 앱에 React Native를 적용하고 싶으시다면, Integration guide 문서를 확인해보세요. 만약 위 튜토리얼이 제대로 실행되지 않았다면, Troubleshooting 문서를 확인해 보세요. 만약 React Native에 대해 좀 더 알고싶으시다면, Tutorial문서를 확인해보세요.","link":"/2016/11/15/ReactNative-Translation-#01-getting-started/"},{"title":"[DjangoTDDStudy] #00: 스터디를 시작하며","text":"스터디를 시작하며파이썬을 이용한 클린 코드를 위한 테스트 주도 개발이라는 도서를 처음 보았을 때는 Django와 관련이 있는 책이라고는 생각조차 하지 못했다. 단지, 파이썬을 좀 더 잘 하려면 어떤 것을 알아야 할까 하고U 생각하던 중 TEST를 사용하는 때가 생산성이 올라간다는 말을 듣고서 “테스트 주도 개발”을 하려고 시도해 보려던 참, 이 책을 보게 된 것이다. Python/Django 공부에 관심을 가지고 있던 지환님과 이야기 하던 중, 마침 스터디에 대한 이야기가 나왔고 이 염소책과 TDD에 대한 이야기도 나왔다. 사실상 즉석에서 ‘아 그래요? 그러면 해보죠!’라는 느낌으로 시작했다. 그자리에서 명서님에게 물어보니 바로 동참하신다고 해서 9XD에 글을 바로 글을 올리고 스터디원을 모집했다.사실 이정도로 인기가 많을거라고 생각하지는 못했다 생각보다 빠르게 열명이 찼고, 저번주 화요일에 첫 모임을 가졌다. 첫 모임에서첫 모임에서는 많은 내용을 다루지는 않았다. 하지만 진행에 있어 미흡한 준비가 아쉬웠다.모임 전 계획은 서로 인사하기 / 왜 우리는 여기에 왔는가 / 우리의 목표 등을 이야기하고, Django를 사용하지 않은 분들을 위해 개발환경을 설치하고, 1장 정도(Selenium을 이용해 크롬드라이버로 테스트 한번 돌려보기)를 진행하려 했다.하지만 위 내용이 생각보다 빨리 진행되어 1시간내로 모든게 끝나버렸고, 남은 1시간동안은 뭘 해야할지 고민이 들어 2장을 조금 진행해 보려고 했는데 사실 이게 욕심이었다고 생각한다. 물론, 2~3단원까지 끝낸다면 진도상으로는 좋다고 말할 수 있지만 구성원들이 스터디를 따라오지 못할 수 있다고 겁을 먹을 수 있었기 때문이다.다음에 스터디를 처음 시작한다면, 좀더 여유로운 시간을 가지고 ‘스터디’가 아닌 ‘모임’으로 첫 만남을 카페 등에서 가지면 더 좋지 싶다.","link":"/2016/12/26/Django-TDD-Study-00-Starting-Study/"},{"title":"Fabric Put 커맨드가 No Such File Exception을 반환할 때 해결법","text":"환경12Python 3.5+Fabric3 문제 발생 상황12def _put_envs(): put('envs.json', '~/{}/envs.json'.format(PROJECT_NAME)) 이와 같이 로컬에는 envs.json파일이 명확히 존재하고 있었다.그러나 Fabric에서는 1Fatal error: put() encountered an exception while uploading 'envs.json' 위와 같은 에러를 여전히 뿜고 있었다. 하지만 StackOverflow:Fabric put command gives fatal error: ‘No such file’ exception 게시글을 살펴보면 이 문제는 Fabirc의 에러 창이 잘못되었다는 것을 말해준다. 즉, 위 에러에서는 로컬 위치에 envs.json이 없다고 말하지만 실제로는 서버, 그러니까 '~/{}/envs.json'.format(PROJECT_NAME)에 해당하는 위치가 원격 서버 상에 존재하지 않아서 에러를 내는 것이다. 그래서 Fabric코드의 순서를 바꾸어 주었다. 기존 순서가 12345678910def _put_envs(): put(os.path.join(PROJECT_DIR, 'envs.json'), '~/{}/envs.json'.format(PROJECT_NAME))def _get_latest_source(): if exists(project_folder + '/.git'): run('cd %s &amp;&amp; git fetch' % (project_folder,)) else: run('git clone %s %s' % (REPO_URL, project_folder)) current_commit = local(\"git log -n 1 --format=%H\", capture=True) run('cd %s &amp;&amp; git reset --hard %s' % (project_folder, current_commit)) 와 같이 envs를 업로드 후 github소스를 받아오는 것이었다면, 이제는 소스를 먼저 가져온 후 (_get_latest_source를 먼저 실행 후) envs를 업로드 하도록 바꾸었다. 이 경우 정상적으로 실행 되었다.","link":"/2016/12/22/Fabric-Put-Command-No-Such-File-Exception/"},{"title":"CKEditor의 라이센스와 오픈소스 라이센스","text":"CKEditor는 더이상 오픈소스만으로 제공되지 않는다오픈소스 프로젝트를 이용하는데 있어 주의해야 하는 것 중 하나가 LISENCE를 잘 살펴보아야 하는 것인데, 이 프로젝트는 프로젝트를 사용함으로 인해 이 사용한 프로젝트에 법적인 제약이 걸리게 된다.단적으로 GPL Lisence의 경우에는 이 소스를 사용한 프로젝트의 전체 소스를 공개해야 한다.반면 MIT Lisence의 경우에는 이러한 제약이 거의 없고 상업적 이용되 허용된다. CKEditor Pricing을 살펴보면, OpenSource Project와 상업적 용도가 구별되어있다는 것을 알 수 있다.CKEditor가 상당히 안정화됨에 따라 이와 같은 가격정책을 취한 것으로 생각되는데, 일반적으로 소스를 오픈하지 않는 프로젝트라면 SummerNote를 쓸 수 밖에 없는 상황이 아닌가 싶다. CKEditor도 Lisence가 GPL, LGPL, MPL 이 세 가지로 나누어져 있는데 이 세 라이센스 중 한 가지를 따라야 하며, 법적(legal)하게 SourceCode를 public하게 만들어두어야만 한다.Standard Lisence의 경우에는 OpenSource용이고, 이 경우 Commercial하게 사용하지는 못한다. 즉, 오픈소스 프로젝트에 Support 가격이 들어간 것일 뿐인 것이다. 오픈소스는 어떻게 운영되는가오픈소스 프로젝트는 기본적으로 개발자들의 열정으로 이루어진다.사회 공헌적 성격도 있고, 자신이 만든 소스를 여러 사람들이 사용해 더 큰 생태계로 발전해 나가기를 희망하는 것도 있다.기업이 공개하는 오픈소스의 경우에는 자사의 서비스와 연계되어있는 경우나 자사가 이용하는 라이브러리/프레임워크가 전 세계에서 널리 사용되어 보편화 되고 이로 인해 시장 점유율이라는 것과 이쪽 분야에 대한 전반적 발전을 이끌어 낼 수 있다는 점에서 의미가 있다. (단적인 예시로 구글의 TensorFlow가 공개된 이후 TensorFlow가 학계에서 무척 많이 이용되고 있으며 관련 연구의 진척이 폭발적으로 일어나고있다.) 오픈소스 프로젝트 개발팀은 어떻게 밥먹고 사는가MySql와 같은 유명 오픈소스 프로젝트들은 크게 두가지 방법으로 서비스를 제공한다.일반 사용자들을 위한 것, 그리고 기업 사용자들을 위한 것.회사나 공공기관의 경우에는 솔루션 도입에 앞서 염두에 두는 것이 ‘가격’만이 아니라 ‘유지보수’, 즉 이용중에 접하는 버그 혹은 에러상황에 대해 내부에서 프로젝트 전체를 관리하지 않고서도 자사의 서비스를 안정적으로 유지할 수 있느냐, 그리고 이 솔루션 도입시 에러를 내지 않는다는 (혹은 에러를 빠르게 해결 가능하다는) 보증이 필요한 것이다. 따라서 개발팀은 프로젝트는 공개하되, 개발팀 차원에서 지원이 필요한 경우에 계약을 통해 돈을 받고 유지보수를 계속 하는 것이다. 여담오픈소스 프로젝트는 사실 PullRequest에 의해 진행되는 면도 무시할 수 없다. 그러나, Core개발자들이 풀타임으로 일하지 않는다면 OpenSSL의 HeartBleed와 같은 문제가 언제든 발생할 수 있다. OpenSSL는 대다수의 프로젝트에서 의존성으로 사용되는 어머어마하게 많이 사용되는 프로젝트임에도 풀타임 개발자가 한명뿐이었다는 점, 오픈소스 프로젝트가 갖게 되는 가장 심각한 문제를 명백하게 보여준다.모두가 관리하는 것이 아니라 모두가 방치했을 수 있다는 점, 그리고 빠른 패치와 업그레이드가 오히려 문제를 가져올 수 있다는 점, 그리고 이 문제는 소수가 사용하는 내부 시스템이 아니라 전세계의 대다수의 서비스가 사용하는 의존성 패키지였다는 점.공유지의 비극이 일어난 상황이라고 보아도 무방하지 않을까…","link":"/2016/12/22/CKEditor-Lisence-and-Pricing/"},{"title":"[DjangoTDDStudy] #02: UnitTest 이용해 기능 테스트 하기","text":"최소기능 앱TDD를 하는데 있어 가장 기본은 당연히 테스트코드를 짜는 것이다. 하지만 그 전에 먼저, 테스트 시나리오를 작성해야 한다. 예를들어, “인터넷 쇼핑몰에 들어간 후, 상품을 검색하고, 상품을 선택하고, 장바구니에 담고, 카트에 간 후, 결제를 한다.”라는 시나리오도 가능하다. 그리고, 이 시나리오를 충족하면서 가장 간단한 기능으로만 구성되지만 실제로 ‘동작’하는 앱을 만드는 것이다. 일단 이 최소기능 앱을 만들기 전에 우리 코드를 약간 바꿔보자. Python 기본 라이브러리: unittest지금까지는 selenium의 webdriver만을 직접 이용해왔지만, 만약 테스트 할 내용이 단순히 인터넷 창 하나를 띄우는것이 아니라 여러가지로 테스트를 늘려야 한다면 이전시간에 짠 코드는 딱히 도움이 되지는 않을 듯 하다. 그리고, 열려진 브라우저를 일일히 닫아주는 것도 상당히 귀찮다. 그러니까 unittest를 이용해 보자. 방금까지 사용한 functional_tests.py파일을 아래와 같이 바꿔보자. 1234567891011121314151617181920# functional_tests.pyfrom selenium import webdriverimport unittestclass NewVisitorTest(unittest.TestCase): def setUp(self): self.browser = webdriver.Chrome('chromedriver') def tearDown(self): self.browser.quit() def test_can_start_a_list_and_retrieve_it_later(self): self.browser.get('http://localhost:8000') self.assertIn('To-Do', self.browser.title) self.fail('Finished the Test')if __name__=='__main__': unittest.main(warning='ignore') 위 코드중 마지막의 if __name__=='__main__'은, 이 파이썬 코드가 다른 파이썬 파일에서 import 되어 사용되지 않고 python functional_tests.py라고 직접 실행한 경우에만 아래 코드를 실행한다. 만약 위 파일이 import functional_tests의 방식으로 import되었다면 위 코드의 __name__은 functional_tests가 된다. 12345678from abc import some_thing# __name__ 은 some_thingimport abc# __name__ 은 abcfrom abc import some_thing as st# __name__ 은 st 이와 같은 __name__을 가진다. 다시한번 위 코드를 살펴보면, NewVisitorTest 클래스는 unittest라이브러리의 TestCase클래스를 상속받고 내장함수는 setUp, teatDown, test_can_start_a_list_and_retrieve_it_later가 있다. unittest.main()을 통해 실행되면 ~~Test라는 class들이 모두 테스트 클래스로 지정되고 실행되는데, 이 클래스 안에 있는 test_로 시작하는 함수 하나하나가 테스트 함수로 인식된다. (만약 test_로 시작하지 않으면 테스트 코드라고 인식하지 않는다.) 또한, 테스트 함수 하나하나가 실행되기 전에 setUp이 실행되고 테스트 함수가 끝날때 마다 tearDown이 실행된다. 암묵적 대기 / 명시적 대기셀레늄이 URL에 들어가 페이지 로딩이 끝날때까지 기다렸다가 동작하기는 하지만, 완벽하지는 않기때문에 (ajax call로 DOM을 재구성하는 경우 등) 얼마정도 우리가 지정한 element가 나올때까지 기다리게 할 수 있다. 바로 implicit_wait()이라는 암시적 대기를 주는 것이다. 123456789101112131415161718192021# functional_tests.pyfrom selenium import webdriverimport unittestclass NewVisitorTest(unittest.TestCase): def setUp(self): self.browser = webdriver.Chrome('chromedriver') self.browser.implicit_wait(3) def tearDown(self): self.browser.quit() def test_can_start_a_list_and_retrieve_it_later(self): self.browser.get('http://localhost:8000') self.assertIn('To-Do', self.browser.title) self.fail('Finished the Test')if __name__=='__main__': unittest.main(warning='ignore') browser에 implicit_wait을 3초로 두었다.그런데, 이 implicit_wait은 이 테스트 코드 전체 행동에 영향을 준다. 즉, 어떤 동작을 하기 전에 일단 3초는 허용하고 본다는 것이다. 그래서 좀 더 복잡한 코드의 경우에는 명시적 대기를 줘야만 한다. 아래는 selenium의 공식 문서 webdriver_advanced의 코드를 일부 변형한 것이다.(Driver/URL) 1234567891011from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWait # v2.4.0 이상from selenium.webdriver.support import expected_conditions as EC # v2.26.0 이상browser = webdriver.Chrome('chromedriver')browser.get(\"http://localhost:8000\")try: element = WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.ID, \"myDynamicElement\")))finally: browser.quit() 위 코드에서 브라우저는 0.5초마다 ‘myDynamicElement’라는 ID를 가진 요소가 존재하는지를 체크한다. 만약 10초 내로 나타난다면 정상적으로 진행되고, 나타나지 않으면 TimeoutException을 내뱉는다. 여담하지만 selenium문서에서는 이 코드는 implicit_wait와 크게 다르지 않다고 한다. 좀더 공부가 필요할 듯 하다.","link":"/2016/12/27/Django-TDD-Study-02-Using-UnitTest/"},{"title":"[번역] 장고(Django)와 함께하는 Celery 첫걸음","text":"원문: http://docs.celeryproject.org/en/latest/django/first-steps-with-django.html 12유의사항: 현재(11.14) celery가 4.0버전으로 stable 릴리즈가 되었습니다.아래 문서는 3.1의 마지막 버전의 문서를 번역한 것입니다. 최신 문서는 곧 업데이트 될 예정입니다. Celery는 공식적인 패키지로 django-celery를 제공합니다. 이 문서는 celery의 현재(2016.11)의 최신 버전인 3.1버전을 기준으로 하고 있습니다. 장고(Django)와 함께하는 Celery 첫걸음원제: First steps with Django 장고와 함께 Celery사용하기 유의점:이전 버전의 celery는 장고와 독립적인 패키지를 필요로 했지만, 3.1버전부터는 더이상 그렇지 않습니다. 이제 장고에서 공식적으로 celery를 바로 사용할 수 있도록 지원하며, 따라서 이 문서는 celery와 장고를 간단하게 연결하는 부분만을 다루고 있습니다. 즉, 장고와 함께 사용하지 않고 독립적으로 사용하는 celery와 완전히 같은 API를 사용하고 있기 때문에, 현재는 First Steps with Celery를 읽고나서 다시 이 문서로 오시는 것을 추천합니다. 이미 작동하고 있는 celery 앱을 갖고 계시다면, Next Steps 가이드를 참고하시기 바랍니다. Celery를 장고 프로젝트에서 사용하시려면, 우선 Celery 라이브러리의 인스턴스를 정의해야 합니다. (이 인스턴스는 아래에서 ‘app’이라고 불립니다.) 만약 최신 장고프로젝트(django 1.10)의 형식을 따라 사용하고 계시다면, 장고 프로젝트는 아래와 같은 형태일 것입니다. (프로젝트 이름: ‘proj’) 12345- proj/ - proj/__init__.py - proj/settings.py - proj/urls.py- manage.py 만약 이런 구조로 되어있다면, 추천하는 방법은 장고 프로젝트 폴더(proj/proj/)안에 celery.py파일을 생성하는 것입니다. 아래와 같이요. 123456- proj/ - proj/__init__.py - proj/settings.py - proj/urls.py - proj/celery.py- manage.py 파일: proj/proj/celery.py 12345678910111213141516171819202122from __future__ import absolute_importimport osfrom celery import Celery# Django의 세팅 모듈을 Celery의 기본으로 사용하도록 등록합니다.os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'proj.settings')from django.conf import settings # noqaapp = Celery('proj')# 문자열로 등록한 이유는 Celery Worker가 Windows를 사용할 경우# 객체를 pickle로 묶을 필요가 없다는 것을 알려주기 위함입니다.app.config_from_object('django.conf:settings')app.autodiscover_tasks(lambda: settings.INSTALLED_APPS)@app.task(bind=True)def debug_task(self): print('Request: {0!r}'.format(self.request)) 위와같이 celery.py파일을 만드신 후에, 장고 프로젝트 폴더의 init.py 모듈에서 이 celery 앱을 import해와야 합니다. 이 과정은 장고가 시작될 때 @shared_task 데코레이터의 사용을 가능하게 합니다. 파일: proj/proj/init.py 12345from __future__ import absolute_import# 아래 import는 장고가 시작될 때 항상 import되기 때문에# shared_task가 장고에서 작동하는 것을 가능하게 해 줍니다.from .celery import app as celery_app # Celery를 import합니다. 참고로, 위에서 제시한 장고 프로젝트의 구조는 거대한 프로젝트에 적합합니다. 만약 First Steps with Celery 튜토리얼처럼 작고 간단한 프로젝트라면, 한 모듈(한 파이썬 파일)에서 App과 Task를 모두 다루는 것도 괜찮습니다. 자, 이제 우리가 여기서 처음으로 만든 모듈(celery.py)를 뜯어 보도록 합시다. 우선, 우리는 absolute import를 future에서 import 할 것입니다. 다른 library와 꼬이지 않게 위해서요. 1from __future__ import absolute_import 그 다음에, 우리는 Celery의 커맨드라인 프로그램을 위해 기본 DJANGO_SETTINGS_MODULE을 가져와서 설정해줍니다. 1os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'proj.settings') 이렇게 장고의 세팅파일을 명시해 주는 것은 터미널에서 사용하는 celery커맨드가 장고 프로젝트가 도대체 어디에 있는 장고 프로젝트를 말하는 건지를 알 수 있게 도와줍니다. 이 코드는 항상! App Instance가 생성되지 전에 적어줘야 합니다. 이 다음에 우리가 해야하는 것은 App Instance 객체를 생성하는 겁니다. 1app = Celery('proj') 위 코드는 celery 라이브러리의 인스턴스가 됩니다. 물론, 여러 다른이름으로 여러개의 인스턴스를 만들 수도 있습니다. 그런데, Django와 Celery를 사용할 때에는 하나만 만들어도 충분합니다. 우리는 장고 세팅 모듈을 Celery의 설정 소스로 삼아줄 것인데요, 바로 이게 여러개의 인스턴스를 만들 필요가 없는 이유랍니다. Celery를 장고 세팅에서 바로 설정할 수 있게 해주기 때문이죠! 이곳에 Object를 바로 넣어줄 수도 있지만, 문자열(Str)로 넘겨주는 것이 더 나은데요, 이렇게 해주면 worker가 Windows나 execv를 사용할 때 Object를 serialize할 필요가 없기 때문입니다. 1app.config_from_object('django.conf:settings') 이렇게 문자열로 넣어줍시다. 일반적으로, 재사용 가능한 앱을 만드는 것은 모든 작업 코드들을 다른 파일인, 예를들어 tasks.py와 같은 파일에 몰아두는 것입니다. 그리고 celery는 이 모듈들을 자동으로 찾을 수 있답니다. 1app.autodiscover_tasks(lambda: settings.INSTALLED_APPS) 위 코드로 celery는 자동적으로 장고 세팅에 재사용 가능한 앱에서 tasks.py를 찾을 겁니다.(단, task들이 있는 파일 이름이 tasks.py여야 자동으로 찾을 수 있습니다.) 123456- app1/ - app1/tasks.py - app1/models.py- app2/ - app2/tasks.py - app2/models.py 이와 같은 모양으로, 각 앱 아래에 tasks.py를 두는 것입니다. 이 방법을 통해 개별적 모듈들을 CELERY_IMPORTS 세팅값에 등록해주지 않아도 됩니다. lambda를 통해 필요한 경우에만(재사용 가능한 앱이 있는 경우에만) 자동탐색이 동작하게 되기 때문에, Celery App을 import한다고 해서 장고 세팅 객체를 검사(evaluate)하지는 않습니다. 마지막으로, debug_task라는 예제는 request받은 정보를 단순하게 dump하는 작업만 합니다. 그리고, bind=True라는 Task옵션은 Celery 3.1에서 도입된 기능으로, 현재 작업 인스턴스를 좀 더 쉽게 refer하도록 도와줍니다. @shared_task 데코레이터 이용하기우리가 작성한(혹은 지금 보고있는 분이 작성한) task들은 재사용 가능한 앱들에 있을텐데, 이러한 재사용가능한 앱들은 Project자체에 의존하기는 어렵습니다. 그리고 app을 단독으로 직접적으로 import할 수도 없죠. @shared_task 데코레이이터는 딱딱한(concrete) 앱 없이도 task를 만들 수 있도록 해 줍니다. demoapp/tasks.py 123456789101112131415from __future__ import absolute_importfrom celery import shared_task@shared_taskdef add(x, y): return x + y@shared_taskdef mul(x, y): return x * y@shared_taskdef xsum(numbers): return sum(numbers) 참고:장고-셀러리 예제 프로젝트의 전체 코드는 아래에서 보실 수 있습니다.https://github.com/celery/celery/tree/3.1/examples/django/ 장고 ORM와 Cache를 Celery결과 백엔드로 이용해 봅시다만약 task의 결과를 장고 database에 저장하고 싶다면 우선 django-celery 라이브러리를 먼저 설치해야 합니다.(위에서 이미 설치했겠지만요!)(혹은 SQLAlchemy Result Backend를 이용해도 됩니다.) django-celery라이브러리는 Django ORM와 Django Cache Framework를 Result Backend로 사용하게 해 줍니다. 이걸 사용하려면, django-celery 라이브러리를 설치해 줍니다. 1$ pip install django-celery djcelery를 INSTALLED_APPS(장고 프로젝트 settings.py)에 추가해 줍시다. 데이터베이스 테이블을 만들어 줍시다 1$ python manage.py migrate djcelery Celery가 django-celery backend를 사용하도록 설정해줍시다.4-1. 만약 Database Backend를 이용하고 싶다면.. 123app.conf.update( CELERY_RESULT_BACKEND='djcelery.backends.database:DatabaseBackend',) 4-2. 만약 Cache Backend를 이용하고 싶다면.. 123app.conf.update( CELERY_RESULT_BACKEND='djcelery.backends.cache:CacheBackend',) 4-3. 만약 Celery를 Django settings에 직접 연결해 두었다면 app.conf.update부분 없이 바로 괄호 안의 문구를 settings.py안에 넣어두기만 하면 됩니다. (유의점)상대적 import:____import를 하는 방법은 항상 일치해야 합니다. 만약, project.app 을 INSTALLED_APPS 에 import 해 주었다면 from project.app 와 같은 형식으로 import해주어야 합니다. 혹은 각각의 task들이 이름이 모두 다르게 하는 방법도 있습니다.__추가정보: Automatic naming and relative imports 워커 프로세스 실행해보기실 배포 환경에서는 worker를 시스템 daemon으로 사용해야합니다. (Running the worker as a daemon 을 참고하세요.) 하지만, 테스트할 때에는 worker instance를 celery worker manage 커맨드를 통해 시작하는 것이 더 편리합니다. 장고의 runserver를 이용하는 것 처럼요 :) 1$ celery -A proj worker -l info 위 코드로 간단하게 실행이 가능하고, command-line 옵션을 모두 보려면 1$ celery help 라고 쳐 봅시다. 이제 어디로 가야할까요?만약 좀 더 배우고 싶다면, Next Steps 튜토리얼을 보세요. 그 튜토리얼을 마친 이후에는 User Guide를 보고 공부할 수 있을거에요.","link":"/2016/11/04/ebb288ec97ad-ec9ea5eab3a0djangoec9980-ed95a8eabb98ed9598eb8a94-celery-ecb2abeab1b8ec9d8c/"},{"title":"Celery로 TelegramBot 알림 보내기","text":"Celery는 비동기 큐이지만 주기적 Task도 잘한다Celery는 async/비동기적으로 특정한 작업을 돌리기 위해 자주 사용한다. 특히, django와는 찰떡궁합이라고 알려져있다.하지만 이 celery는 설정이 어렵다면 어렵고, 쉽다면 쉬운편이다. 먼저 Celery를 “쓰려면” 어떤 것들이 필요한지 체크해보자. Celery 준비물 pip를 통해 설치된 Celery가 필요하다. 1$ pip install celery RabbitMQ나 Redis등의 큐 중간 저장소가 필요하다. RabbitMQ를 설치해보자. 1$ brew install rabbitmq 이후 .bashrc나 .zshrc의 마지막 줄에 아래 코드를 추가해준다. 1export PATH=$PATH:/usr/local/sbin 이로서 rabbitmq-server라는 명령어로 rabbitmq를 실행할 수 있다. 셀러리가 돌아갈 파이썬 파일 만들기아래와 같이 코드를 작성해 보자. (celery_parser.py) 1234567891011121314# celery_parser.pyfrom celery import Celery# Celery Setupapp = Celery()app.conf.timezone = 'Asia/Seoul'@app.on_after_configure.connectdef periodic_parser(sender, **kwargs): sender.add_periodic_task(5.0, hello(), name='hello?')@app.taskdef hello(): print('hello!') 위 코드를 작성한 후, 쉘을 두 창을 켠 후 각각 아래 코드를 입력해 준다. (celery_parser.py와 같은 폴더에서) 1234celery worker -A celery_parser --loglevel=info&lt;이용법&gt;celery worker -A 파이썬파일이름 --loglevel=info 위 코드는 Celery의 get_url함수, 즉 app의 Task함수가 실제로 구동될 worker이며,아래 코드는 periodic_parser함수 안에서 정의된 sender.add_periodic_task에 의해 첫번째 인자로 전달된 5.0초, 두번째 인자로 전달된 hello 함수를 실행하게 하는 Celery의 beat이다. 1234celery beat -A celery_parser --loglevel=info&lt;이용법&gt;celery beat -A 파이썬파일이름 --loglevel=info TelegramBot 설정하기python에서 telegram bot 사용 가이드텔레그램 봇을 Python에서 이용하는 좋은 가이드가 있다. python에서 telegram bot 사용하기 위 링크를 참고해서 pip로 python-telegram-bot을 설치하고, 새 봇을 만든 후 token과 id값을 받아오자. requests를 이용해 site의 변화 유무 체크하기우선 파이썬 파일을 수정하기 전에 target.json라는 환경변수용 json파일을 아래와 같이 만들어 주자. 12345{ \"BOT_TOKEN\":\"위에서 받은 숫자9자리:영문+숫자+특수문자 긴것\", \"URL\":\"변화유무를 체크할 URL\", \"CHAT_ID\":\"위에서 받은 id\"} 이제 celery_parser.py에서 target.json파일을 불러온 후, 변수로 등록해주고, telegram bot 객체를 만들어 준 후 sendMessage를 이용해 보자. 1234567891011121314151617181920212223242526272829303132333435363738394041424344# celery_parser.pyfrom celery import Celeryimport requestsimport jsonimport osimport datetimeimport telegram# Celery Setupapp = Celery()app.conf.timezone = 'Asia/Seoul'BASE_DIR = os.path.dirname(os.path.abspath(__file__))# Parsing/Telegram Environ loadswith open(os.path.join(BASE_DIR, \"target.json\")) as f: env = json.loads(f.read()) BOT_TOKEN = env['BOT_TOKEN'] URL = env['URL'] CHAT_ID = env['CHAT_ID']bot = telegram.Bot(token=BOT_TOKEN)bot.sendMessage(chat_id=CHAT_ID, text='Started!')@app.on_after_configure.connectdef periodic_parser(sender, **kwargs): sender.add_periodic_task(5.0, get_url.s(URL), name='send working time')@app.taskdef get_url(url): req = requests.get(url) f = open('temp/req.txt', 'w+') previous_html = f.read() new_html = req.text bot.sendMessage(chat_id=CHAT_ID, text='Working/{}'.format(datetime.datetime.now())) if previous_html == new_html: bot.sendMessage(chat_id=CHAT_ID, text='working...') else: #TODO: BOT NOTICE bot.sendMessage(chat_id=CHAT_ID, text='{} 에 변경이 있습니다.'.format(url)) 위 코드는 URL에 접속해 html로 저장 후 5초 후 다음 접속 시 사이트에 변동사항이 있으면 변동이 있다는 Telegram 알림을 보내준다.","link":"/2016/12/28/TelegramBot-with-Celery/"},{"title":"나만의 웹 크롤러 만들기(2): Login with Session","text":"좀 더 보기 편한 깃북 버전의 나만의 웹 크롤러 만들기가 나왔습니다! 이전게시글: 나만의 웹 크롤러 만들기 with Requests/BeautifulSoup @2017.07.12 Update: 뉴클리앙으로 업데이트 됨에 따라 코드와 스크린샷이 업데이트 되었습니다. 웹 사이트를 로그인 하는데 있어 쿠키와 세션을 빼놓고 이야기하는 것은 불가능합니다. 이번 포스팅에서는 requests모듈을 이용해 로그인이 필요한 웹 사이트를 크롤링 하는 예제를 다룹니다. 쿠키(Cookie)? 세션(Session)?웹은 대다수가 HTTP기반으로 동작합니다. 하지만 HTTP가 구현된 방식에서 웹 서버와 클라이언트는 지속적으로 연결을 유지한 상태가 아니라 요청(request)-응답(response)의 반복일 뿐이기 때문에, 이전 요청과 새로운 요청이 같은 사용자(같은 브라우저)에서 이루어졌는지를 확인하는 방법이 필요합니다. 이 때 등장하는 것이 ‘쿠키’와 ‘세션’입니다. 쿠키는 유저가 웹 사이트를 방문할 때 사용자의 브라우저에 심겨지는 작은 파일인데, Key - Value 형식으로 로컬 브라우저에 저장됩니다. 서버는 이 쿠키의 정보를 읽어 HTTP 요청에 대해 브라우저를 식별합니다. 그러나, 쿠키는 로컬에 저장된다는 근원적인 문제로 인해 악의적 사용자가 쿠키를 변조하거나 탈취해 정상적이지 않은 쿠키로 서버에 요청을 보낼 수 있습니다. 만약 ‘로그인 하였음’이라는 식별을, 로컬 쿠키만을 신뢰해 로그인을 한 상태로 서버가 인식한다면 쿠키 변조를 통해 마치 관리자나 다른 유저처럼 행동할 수도 있는 것이죠.(굉장히 위험합니다.) 이로 인해 서버측에서 클라이언트를 식별하는 ‘세션’을 주로 이용하게 됩니다. 세션은 브라우저가 웹 서버에 요청을 한 경우 서버 내에 해당 세션 정보를 파일이나 DB에 저장하고 클라이언트의 브라우저에 session-id라는 임의의 긴 문자열을 줍니다. 이때 사용되는 쿠키는 클라이언트와 서버간 연결이 끊어진 경우 삭제되는 메모리 쿠키를 이용합니다. Requests의 Session이전 게시글에서 다룬 requests모듈에는 Session이라는 도구가 있습니다. 1234import requests# Session 생성s = requests.Session() Session은 위와 같은 방식으로 만들 수 있습니다. 이렇게 만들어진 세션은 이전 게시글에서의 requests위치를 대신하는데, 이전 게시글의 코드를 바꿔본다면 아래와 같습니다. 1234567891011121314151617# parser.pyimport requests# Session 생성s = requests.Session()# HTTP GET Request: requests대신 s 객체를 사용한다.req = s.get('https://www.clien.net/service/')# HTML 소스 가져오기html = req.text# HTTP Header 가져오기header = req.headers# HTTP Status 가져오기 (200: 정상)status = req.status_code# HTTP가 정상적으로 되었는지 (True/False)is_ok = req.ok 코드를 with구문을 사용해 좀 더 정리하면 아래와 같습니다. 위 코드와 아래코드는 정확히 동일하게 동작하지만, 위쪽 코드의 경우 Session이 가끔 풀리는 경우가 있어 (5번중 한번 꼴) 아래 코드로 진행하는 것을 추천합니다. 123456789101112131415# parser.pyimport requests# Session 생성, with 구문 안에서 유지with requests.Session() as s: # HTTP GET Request: requests대신 s 객체를 사용한다. req = s.get('https://www.clien.net/service/') # HTML 소스 가져오기 html = req.text # HTTP Header 가져오기 header = req.headers # HTTP Status 가져오기 (200: 정상) status = req.status_code # HTTP가 정상적으로 되었는지 (True/False) is_ok = req.ok 로그인하기로그인을 구현하기 위한 예시로 클리앙에 로그인 해 클리앙 장터를 크롤링 해 봅시다. 크롬 개발자 도구 중 Inspect(검사)를 이용해 로그인 폼 필드의 name값들을 알아봅시다.(폼 위에서 마우스 오른쪽 버튼을 클릭하고 검사를 눌러주세요.) 아래 스크린샷 우측을 확인해 봅시다. form 태그 안에 input필드가 여러개가 있는 것을 알 수 있습니다. 조금 더 상세하게 뜯어봅시다. 아래 스크린샷을 보시면 input필드들의 name이 _csrf,userID,userPassword,remember-me가 있는 것을 볼 수 있습니다. 또한, 로그인 버튼을 누르면 auth.login()라는 자바스크립트 함수가 먼저 실행되는 것을 볼 수 있습니다. 로그인을 구현하기 전, HTML form에 대해 간단하게 알아봅시다. HTML form Field에서는 name:입력값이라는 Key:Value식으로 데이터를 전달합니다.(주로 POST방식) 클리앙 로그인 폼 필드의 경우 userID:사용자id, userPassword:사용자pw라는 세트로 입력을 받는 것을 볼 수 있습니다. 그리고 약간 특이해 보이는 _csrf이라는 것도 있습니다. 원래 CSRF는 사용자의 요청이 악의적이거나 제 3자에 의해 변조된(해킹된) 요청이 아닌지 확인해주는 보안 도구중 하나입니다. 세션과 연결되어 폼을 전달할때 폼의 안정성을 높여줍니다. 새로고침하시면 매번 달라지는 CSRF값을 보실 수 있습니다. 그리고 CSRF를 사용하는 경우 CSRF값이 없는 폼 전송은 위험한 요청으로 생각하고 폼을 받아들이지 않습니다.(즉, 로그인이 되지 않습니다.) 따라서 우리는 _csrf라는 것도 함께 전송해 줘야 합니다. 따라서 메인 화면을 먼저 가져와 _csrf필드를 가져오고 로그인을 해야 합니다. 이전 클리앙은 CSRF검증이 없었습니다. 이번 업데이트를 하면서 클리앙의 보안이 전반적으로 올라갔습니다. 좋은 변화입니다! 다음으로는 auth.login()이라는 함수를 살펴봅시다. 함수를 살펴보면 그냥 입력 유무만 확인하는 심플한 함수입니다. 사실 이것보다 더 길지만, 실제로 login함수에서 사용되는 코드 부분은 이부분이 전부이기 때문에 뒷부분을 잘랐습니다. 1234567891011121314151617181920212223242526272829303132333435function Auth() { var _this = this; // _this에 Auth라는 함수를 넣었습니다. _this.env = {}; _this.env.form = $('#loginForm'); // 로그인 폼입니다. id, pw, _csrf 등을 받는다고 위에서 확인했죠? _this.env.iptUserId = _this.env.form.find('*[name=userId]'); // 사용자가 폼에 입력한 ID입니다. _this.env.iptUserPassWord = _this.env.form.find('*[name=userPassword]'); // 사용자가 폼에 입력한 PW입니다. _this.loginValidate = function() { var isValid = true; // 아무 문제가 없다면(id나 pw가 빈칸이 아니라면) true를 반환하는 함수입니다. if (_this.env.iptUserId.val().trim() == '') { // 아이디가 빈칸이면 false죠? alert('아이디를 입력하세요.'); _this.env.iptUserId.focus(); isValid = false; return isValid; } if (_this.env.iptUserPassWord.val().trim() == '') { // 비번이 빈칸이어도 false가 됩니다. alert('비밀번호를 입력하세요.'); _this.env.iptUserPassWord.focus(); isValid = false; return isValid; } return isValid; }; _this.login = function() { var isValid = _this.loginValidate(); // 방금 본 아이디/비번이 빈칸인지 확인하기 if (isValid) { // 빈칸이 아니라면 -&gt; _this.env.form.attr({ // 폼 속성을 정의해 줍시다. method: 'POST', // 폼 전송 방식은 'POST'이고, action: BASE_URL + '/login' // 폼 전송하는 주소는 https://www.clien.net/service/login 이네요! }); _this.env.form.submit(); // 진짜로 폼을 전송해줍니다. } };} 위 자바스크립트 코드에서 알게된 것은 아이디와 비밀번호 폼에 빈칸이 없다면 POST방식으로 https://www.clien.net/service/login에 폼을 전송해 로그인을 한다는 것입니다. 한번 이 주소에 폼 값들만 넣어서 전송해 봅시다. 123456789101112131415# parser.pyimport requests# 로그인할 유저정보를 넣어주자 (모두 문자열)LOGIN_INFO = { 'userId': '사용자이름', 'userPassword': '사용자패스워드'}# Session 생성, with 구문 안에서 유지with requests.Session() as s: # HTTP POST request: 로그인을 위해 POST url와 함께 전송될 data를 넣어주자. login_req = s.post('https://www.clien.net/service/login', data=LOGIN_INFO) # 어떤 결과가 나올까요? print(login_req.status_code) 이런! 404가 나와버렸네요. 제대로 로그인이 되지 않은 것 같아요. 아마 _csrf값이 없어서가 아닐까요? 12&gt; python parsing.py404 그렇다면 코드를 조금 더 수정해 봅시다. 우선 클리앙 공식 홈페이지에 들어가 form에 들어있는 _csrf값을 가져와 봅시다. 12345678910111213141516171819202122232425262728# parser.pyimport requestsfrom bs4 import BeautifulSoup as bs# 로그인할 유저정보를 넣어줍시다. (모두 문자열입니다!)LOGIN_INFO = { 'userId': 'myidid', 'userPassword': 'mypassword123'}# Session 생성, with 구문 안에서 유지with requests.Session() as s: # 우선 클리앙 홈페이지에 들어가 봅시다. first_page = s.get('https://www.clien.net/service') html = first_page.text soup = bs(html, 'html.parser') csrf = soup.find('input', {'name': '_csrf'}) # input태그 중에서 name이 _csrf인 것을 찾습니다. print(csrf['value']) # 위에서 찾은 태그의 value를 가져옵니다. # 이제 LOGIN_INFO에 csrf값을 넣어줍시다. # (p.s.)Python3에서 두 dict를 합치는 방법은 {**dict1, **dict2} 으로 dict들을 unpacking하는 것입니다. LOGIN_INFO = {**LOGIN_INFO, **{'_csrf': csrf['value']}} print(LOGIN_INFO) # 이제 다시 로그인을 해봅시다. login_req = s.post('https://www.clien.net/service/login', data=LOGIN_INFO) # 어떤 결과가 나올까요? (200이면 성공!) print(login_req.status_code) 와우! 200이 나온걸 보니 성공적으로 로그인이 된 것 같아요. 진짜 데이터를 가져와봅시다이제 우리 코드를 좀 더 멋지게 만들어 봅시다. 로그인이 실패한 경우 Exception을 만들고, 성공일 경우에는 회원 장터의 게시글을 가져와봅시다. 위 스크린샷처럼 오른쪽 버튼을 누르고 Copy &gt; Copy selector를 눌러주면 #div_content &gt; div.post-title &gt; div.title-subject &gt; div라는 CSS Selector가 나옵니다. 이 HTML문서에서 이 제목만을 콕 하고 찾아줍니다. 본문도 같은 방식으로 찾아줍시다. 다만 p태그가 아니라 글 전체를 담고있는 #div_content &gt; div.post.box &gt; div.post-content &gt; div.post-article.fr-view을 가져와봅시다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344# parser.pyimport requestsfrom bs4 import BeautifulSoup as bs# 로그인할 유저정보를 넣어줍시다. (모두 문자열입니다!)LOGIN_INFO = { 'userId': 'myidid', 'userPassword': 'mypassword123'}# Session 생성, with 구문 안에서 유지with requests.Session() as s: # 우선 클리앙 홈페이지에 들어가 봅시다. first_page = s.get('https://www.clien.net/service') html = first_page.text soup = bs(html, 'html.parser') csrf = soup.find('input', {'name': '_csrf'}) # input태그 중에서 name이 _csrf인 것을 찾습니다. print(csrf['value']) # 위에서 찾은 태그의 value를 가져옵니다. # 이제 LOGIN_INFO에 csrf값을 넣어줍시다. # (p.s.)Python3에서 두 dict를 합치는 방법은 {**dict1, **dict2} 으로 dict들을 unpacking하는 것입니다. LOGIN_INFO = {**LOGIN_INFO, **{'_csrf': csrf['value']}} print(LOGIN_INFO) # 이제 다시 로그인을 해봅시다. login_req = s.post('https://www.clien.net/service/login', data=LOGIN_INFO) # 어떤 결과가 나올까요? (200이면 성공!) print(login_req.status_code) # 로그인이 되지 않으면 경고를 띄워줍시다. if login_req.status_code != 200: raise Exception('로그인이 되지 않았어요! 아이디와 비밀번호를 다시한번 확인해 주세요.') # -- 여기서부터는 로그인이 된 세션이 유지됩니다 -- # 이제 장터의 게시글 하나를 가져와 봅시다. 아래 예제 링크는 중고장터 공지글입니다. post_one = s.get('https://www.clien.net/service/board/rule/10707408') soup = bs(post_one.text, 'html.parser') # Soup으로 만들어 줍시다. # 아래 CSS Selector는 공지글 제목을 콕 하고 집어줍니다. title = soup.select('#div_content &gt; div.post-title &gt; div.title-subject &gt; div') contents = soup.select('#div_content &gt; div.post.box &gt; div.post-content &gt; div.post-article.fr-view') # HTML을 제대로 파싱한 뒤에는 .text속성을 이용합니다. print(title[0].text) # 글제목의 문자만을 가져와봅시다. # [0]을 하는 이유는 select로 하나만 가져와도 title자체는 리스트이기 때문입니다. # 즉, 제목 글자는 title이라는 리스트의 0번(첫번째)에 들어가 있습니다. print(contents[0].text) # 글내용도 마찬가지겠지요? 코드를 실행해 봅시다. 잘 가져옵니다 :) 그러나, 위 코드가 안먹힌다면?일부 사이트의 경우 프론트 브라우저 단에서 ID와 PW를 이용해 암호화된 전송값을 보내는 경우가 있습니다.(대표적으로 네이버가 이렇습니다.) 또한, SPA등으로 인해 PageSource을 가져오는 것이 불충분한 경우가 자주 있습니다. 물론 오늘처럼 JS파일을 분석해 수동으로 data에 넣어주는 방법도 있지만, 브라우저를 직접 다뤄서 사람이 로그인하듯 크롤링을 해보면 어떨까요? 다음 포스팅에서 좀 더 간편히 실제 브라우저(혹은 Headless브라우저)를 이용해 로그인부터 크롤링까지, 간편하게 해보는 방법을 알아봅시다. 다음 가이드: 나만의 웹 크롤러 만들기(3): Selenium으로 무적 크롤러 만들기 업데이트 후기2017년 7월 12일, 올해 초(1월 20일)에 작성한 인기 크롤링글 대상인 클리앙이 바뀌어 업데이트가 필요했습니다. 사실 예전 코드를 업데이트 하는 것도 사실상 새 글을 쓰는 것과 같은 시간과 노력이 듭니다. 하지만 오래된 정보를 두는 것보다 새로운 정보를 두는 것이 낫다고 생각해 업데이트를 했으나..! 클리앙을 이용하는데 덧글 쓰기/글 쓰기 빼고 글을 읽는 것에 제한은 회원장터조차도 제한이 없어졌더군요. (눈물) 그래도 이 가이드를 기반으로 다른 사이트 로그인 하는데 조금 더 쉬워지기를 바랍니다.","link":"/2017/01/20/HowToMakeWebCrawler-With-Login/"},{"title":"나만의 웹 크롤러 만들기 with Requests/BeautifulSoup","text":"좀 더 보기 편한 깃북 버전의 나만의 웹 크롤러 만들기가 나왔습니다! (@2017.03.18) 본 블로그 테마가 업데이트되면서 구 블로그의 URL은 https://beomi.github.io/beomi.github.io_old/로 변경되었습니다. 예제 코드에서는 변경을 완료하였지만 캡쳐 화면은 변경하지 않았으니 유의 바랍니다. 웹 크롤러란?우리가 어떤 정보를 브라우저에서만 보는 것 뿐 아니라 그 정보들을 내가 이용하기 편한 방식(ex: json)으로 로컬에 저장하고 싶을 때가 있다. HTTrack의 경우에는 웹을 그대로 자신의 컴퓨터로 복사를 해오지만, 내가 원하는 방식으로의 가공까지는 제공해주지 않는다. Python을 이용하면 간단한 코드 몇줄 만으로도 쉽게 웹 사이트에서 원하는 정보만을 가져올 수 있다. 웹에서 정보 가져오기RequestsPython에는 requests라는 유명한 http request 라이브러리가 있다. 설치하기1pip install requests pip로 간단하게 설치가 가능하다. 이용방법Python 파일 하나(ex: parser.py)를 만들어 requests를 import 해준다. 1234567891011121314# parser.pyimport requests# HTTP GET Requestreq = requests.get('https://beomi.github.io/beomi.github.io_old/')# HTML 소스 가져오기html = req.text# HTTP Header 가져오기header = req.headers# HTTP Status 가져오기 (200: 정상)status = req.status_code# HTTP가 정상적으로 되었는지 (True/False)is_ok = req.ok 위 코드에서 우리가 사용할 것은 HTML 소스를 이용하는 것이다. 따라서 html=req.text를 이용한다. BeautifulSoupRequests는 정말 좋은 라이브러리이지만, html을 ‘의미있는’, 즉 Python이 이해하는 객체 구조로 만들어주지는 못한다. 위에서 req.text는 python의 문자열(str)객체를 반환할 뿐이기 때문에 정보를 추출하기가 어렵다. 따라서 BeautifulSoup을 이용하게 된다. 이 BeautifulSoup은 html 코드를 Python이 이해하는 객체 구조로 변환하는 Parsing을 맡고 있고, 이 라이브러리를 이용해 우리는 제대로 된 ‘의미있는’ 정보를 추출해 낼 수 있다. 설치하기1pip install bs4 BeautifulSoup을 직접 쳐서 설치하는 것도 가능하지만, bs4라는 wrapper라이브러리를 통해 설치하는 방법이 더 쉽고 안전하다. 이용방법위에서 이용한 parser.py파일을 좀 더 다듬어 보자. 123456789101112# parser.pyimport requestsfrom bs4 import BeautifulSoup# HTTP GET Requestreq = requests.get('https://beomi.github.io/beomi.github.io_old/')# HTML 소스 가져오기html = req.text# BeautifulSoup으로 html소스를 python객체로 변환하기# 첫 인자는 html소스코드, 두 번째 인자는 어떤 parser를 이용할지 명시.# 이 글에서는 Python 내장 html.parser를 이용했다.soup = BeautifulSoup(html, 'html.parser') 이제 soup 객체에서 원하는 정보를 찾아낼 수 있다. BeautifulSoup에서는 여러가지 기능을 제공하는데, 여기서는 select를 이용한다. select는 CSS Selector를 이용해 조건과 일치하는 모든 객체들을 List로 반환해준다. 예시로 이 블로그의 모든 제목을 가져와 보도록 하자. 크롬에 내장된 검사도구(요소 위에서 우측 클릭 후 검사)를 이용해보면 현재 title은 a 태그로 구성되어있다는 것을 알 수 있다. 이 상황에서 모든 a 태그를 가져올 수도 있지만, 보다 정확하게 가져오기 위해 CSS Selector를 확인해 보자. 확인해보니 아래와 같은 코드가 나왔다. 1body &gt; h3:nth-child(4) &gt; a 하지만 :nth-child(4) 등이 붙어있는 것으로 보아 현재 요소를 ‘정확하게’ 특정하고 있기 때문에, 좀 더 유연하게 만들어 주기 위해 아래와 같이 selector를 바꿔준다.(위 코드는 단 하나의 링크만을 특정하고, 아래 코드는 css selector에 일치하는 모든 요소를 가리킨다.) 1h3 &gt; a 이제 parsing.py파일을 더 다듬어 보자. 1234567891011# parser.pyimport requestsfrom bs4 import BeautifulSoupreq = requests.get('https://beomi.github.io/beomi.github.io_old/')html = req.textsoup = BeautifulSoup(html, 'html.parser')# CSS Selector를 통해 html요소들을 찾아낸다.my_titles = soup.select( 'h3 &gt; a' ) 위 코드에서 my_titles는 string의 list가 아니라 soup객체들의 list이다. 따라서 태그의 속성들도 이용할 수 있는데, a 태그의 경우 href속성이 대표적인 예시다. soup객체는 &lt;태그&gt;&lt;/태그&gt;로 구성된 요소를 Python이 이해하는 상태로 바꾼 것이라 볼 수 있다. 따라서 여러가지로 조작이 가능하다. 12345678910111213141516# parser.pyimport requestsfrom bs4 import BeautifulSoupreq = requests.get('https://beomi.github.io/beomi.github.io_old/')html = req.textsoup = BeautifulSoup(html, 'html.parser')my_titles = soup.select( 'h3 &gt; a' )# my_titles는 list 객체for title in my_titles: # Tag안의 텍스트 print(title.text) # Tag의 속성을 가져오기(ex: href속성) print(title.get('href')) 위와 같이 코드를 처리할 경우 a 태그 안의 텍스트와 a 태그의 href속성의 값을 가져오게 된다. 위 코드에서 title 객체는 python의 dictionary와 같이 태그의 속성들을 저장한다. 따라서 title.get('속성이름')나 title['속성이름']처럼 이용할 수 있다. select를 통해 요소들을 가져온 이후에는 각자가 생각하는 방식으로 python코드를 이용해 저장하면 된다. 정리 예제아래 코드는 크롤링한 데이터를 Python파일와 같은 위치에 result.json을 만들어 저장하는 예제다. 1234567891011121314151617181920212223# parser.pyimport requestsfrom bs4 import BeautifulSoupimport jsonimport os# python파일의 위치BASE_DIR = os.path.dirname(os.path.abspath(__file__))req = requests.get('https://beomi.github.io/beomi.github.io_old/')html = req.textsoup = BeautifulSoup(html, 'html.parser')my_titles = soup.select( 'h3 &gt; a' )data = {}for title in my_titles: data[title.text] = title.get('href')with open(os.path.join(BASE_DIR, 'result.json'), 'w+') as json_file: json.dump(data, json_file) 다음 가이드다음 가이드에서는 Session을 이용해 어떻게 웹 사이트에 로그인을 하고, 로그인 상태를 유지하며 브라우징을 하는지에 대해 알아보겠습니다. 다음 가이드: 나만의 웹 크롤러 만들기(2): Login With Session","link":"/2017/01/20/HowToMakeWebCrawler/"},{"title":"Django에 Custom인증 붙이기","text":"들어가기 전widgets:Django는 기본적으로 authentication을 내장하고 있고, User Model을 장고 자체가 가지고 있다. UserModel의 경우 settings.py에서 AUTH_USER_MODEL을 커스텀 유저 모델로 지정해주면 프로젝트 전역에서 사용 가능하지만, 이번 글에서는 이 부분이 아니라 AUTH 처리를 추가할 수 있는지에 대해 알아볼 것이다. 프로젝트 폴더 구조는 아래와 같다. (django-admin startproject sample_project로 생성한 것과 같다. my_user라는 폴더를 만들고 안에 custom_auth.py와 my_auth.py\u001d파일을 만든다.) 12345678910.├── manage.py├── my_user # 유저 모델을 다룰 곳│ ├── custom_auth.py│ └── my_auth.py└── sample_project # 프로젝트 디렉토리 ├── __init__.py ├── settings.py # 장고 프로젝트 settings ├── urls.py └── wsgi.py 사용하는 경우widgets:예를들어, “OO커뮤니티 소속이라면, 우리 서비스에서도 커뮤니티 id와 pw로 로그인이 가능하게 하자”가 대표적인 예시가 될 수 있다. 위 문장을 좀더 풀어쓴다면 “OO커뮤니티에 로그인이 가능한 ID”를 받아 “OO커뮤니티의 인증”으로 “우리 서비스에도 로그인” 할 수 있게 하는 것이다. 만들어봅시다widgets:1. check_if_user 함수 만들기우선 “OO커뮤니티 사이트에 로그인이 가능한 유저인지”를 확인해야 한다.예를들어 “community-dummy”라는 사이트에 로그인하는 url이 /login이고, 유저만 볼 수 있는 페이지가 /login_requited_page라고 가정하자.이 사이트에서는 /login_requited_page에 접속시 로그인된 상태라면 HTTP 200코드를, 로그인 되어있지 않다면 HTTP 401등의 에러코드를 전송한다고 가정하자. 그렇다면 우리는 파이썬의 requests모듈을 이용해\u001c /login에 로그인 정보를 POST방식으로 전송하고 /login_requited_page에 GET방식으로 접근해 HTTP코드를 .status_code를 통해 확인해보면 된다.아래 코드를 확인해보자. 참고: requests는 pip install requests로 설치 가능하다. 123456789101112131415# custom_auth.pyimport requestsdef check_if_user(user_id, user_pw): payload = { 'user_id': str(user_id), 'user_pw': str(user_pw) } with requests.Session() as s: s.post('https://community-dummy.com/login', data=payload) auth = s.get('https://community-dummy.com/login_requited_page') if auth.status_code == 200: # 성공적으로 가져올 때 return True else: # 로그인이 실패시 return False 우리는 이제 이 코드를 통해 유저가 우리 사이트에 입력한 id와 pw가 정확한(OO커뮤니티에 로그인 가능한)것인지를 확인할 수 있다. 2. 커스텀 UserBackend 만들기우선 django 프로젝트가 사용하는 User모델을 가져오자. 1234# my_auth.pyfrom django.contrib.auth import get_user_modelUserModel = get_user_model() 위 방식으로 사용할 경우 Django의 기본 UserModel인 django.contrib.auth.models.User 뿐 아니라 settings.py에 따로 지정한 AUTH_USER_MODEL 클래스를 가져오게 된다. 참고: get_user_model와 AUTH_USER_MODEL은 다르다.django.contrib.auth\u001d의 get_user_model은 유저모델 class를 반환하는 반면,django.conf.settings의 AUTH_USER_MODEL은 유저모델 지정을 str로 반환한다. 그리고 위에서 만든 custom_auth.py에서 check_if_user를 import해주자. 12345# my_auth.pyfrom django.contrib.auth import get_user_modelfrom .custom_auth import check_if_user # custom Auth성공시 True 아니면 FalseUserModel = get_user_model() # django.contrib.auth.models.User대신 사용 이제 장고가 AUTHENTICATION_BACKENDS로서 추가적으로 사용할 UserBackend class를 만들어보자. UserBackend클래스는 최소한 authenticate, user_can_authenticate, get_user라는 함수는 있어야 동작한다. authenticate함수는 self, username, password를 인자로 받은 후, 정상적으로 인증된 경우 user 객체를 ‘하나’ 반환해야 하고, 없는 경우 None값을 반환해야 한다. user_can_authenticate함수는 user\u001d객체를 인자로 받아서 is_active값을 가져와 활성화된 유저인지를 체크한다. (유저가 없거나 활성화된 경우 True, 비활성화된 경우 False) get_user함수는 user_id를 인자로 받아 User객체를 pk로 참조해 user객체를 반환한다. 없는경우 None을 반환한다. 위 함수들을 작성하면 아래와 같다. 1234567891011121314151617181920212223242526272829303132333435# my_auth.pyfrom django.contrib.auth import get_user_modelfrom .custom_auth import check_if_user # custom Auth성공시 True 아니면 FalseUserModel = get_user_model()class UserBackend(object): def authenticate(self, username=None, password=None): if check_if_user(username, password): # OO커뮤니티 사이트 인증에 성공한 경우 try: # 유저가 있는 경우 user = UserModel.objects.get(username=username) except UserModel.DoesNotExist: # 유저 정보가 없지만 인증 통과시 user 생성 user = UserModel(username=username) user.is_staff = False user.is_superuser = False user.save() # 여기서는 user.password를 저장하는 의미가 없음.(장고가 관리 못함) return user else: # OO 커뮤니티 사이트 인증에 실패한 경우, Django기본 User로 감안해 password검증 try: user = UserModel.objects.get(username=username) if user.check_password(password) and self.user_can_authenticate(user): return user except: return None def user_can_authenticate(self, user): is_active = getattr(user, 'is_active', None) # 유저가 활성화 되었는지 return is_active or is_active is None # 유저가 없는 경우 is_active는 None이므로 True def get_user(self, user_id): try: return UserModel.objects.get(pk=user_id) # 유저를 pk로 가져온다 except UserModel.DoesNotExist: return None 3. settings.py에 AUTHENTICATION_BACKENDS 추가하기장고에서 기본적으로 관리해주는 AUTHENTICATION_BACKENDS에는 django.contrib.auth.backends.ModelBackend가 있다. 하지만 위에서 우리가 만든 UserBackend를 추가해줘야 한다. AUTHENTICATION_BACKENDS는 기본적으로 list로 구성되어있으며, 적혀진 순서대로 위에서부터 Auth을 진행한다.(실패시 다음 auth backend를 이용) 아래 코드와 같이 settings.py 파일 아래에 추가해 주자. 12345# settings.pyAUTHENTICATION_BACKENDS = [ 'my_user.my_auth.UserBackend', # 우리가 만든 AUTH를 먼저 검사 'django.contrib.auth.backends.ModelBackend', # Django가 관리하는 AUTH] 이렇게 추가해 줌으로서 django는 우리의 UserBackend를 이용해 유저를 관리하게 된다. 마무리 코드widgets:custom_auth 파일(진짜 OO커뮤니티 유저인가?) 123456789101112131415# custom_auth.pyimport requestsdef check_if_user(user_id, user_pw): payload = { 'user_id': str(user_id), 'user_pw': str(user_pw) } with requests.Session() as s: s.post('https://community-dummy.com/login', data=payload) auth = s.get('https://community-dummy.com/login_requited_page') if auth.status_code == 200: # 성공적으로 가져올 때 return True else: # 로그인이 실패시 return False my_auth 파일 (우리가 만든 UserBackend) 1234567891011121314151617181920212223242526272829303132333435# my_auth.pyfrom django.contrib.auth import get_user_modelfrom .custom_auth import check_if_user # custom Auth성공시 True 아니면 FalseUserModel = get_user_model()class UserBackend(object): def authenticate(self, username=None, password=None): if check_if_user(username, password): # OO커뮤니티 사이트 인증에 성공한 경우 try: # 유저가 있는 경우 user = UserModel.objects.get(username=username) except UserModel.DoesNotExist: # 유저 정보가 없지만 인증 통과시 user 생성 user = UserModel(username=username) user.is_staff = False user.is_superuser = False user.save() # 여기서는 user.password를 저장하는 의미가 없음.(장고가 관리 못함) return user else: # OO 커뮤니티 사이트 인증에 실패한 경우, Django기본 User로 감안해 password검증 try: user = UserModel.objects.get(username=username) if user.check_password(password) and self.user_can_authenticate(user): return user except: return None def user_can_authenticate(self, user): is_active = getattr(user, 'is_active', None) # 유저가 활성화 되었는지 return is_active or is_active is None # 유저가 없는 경우 is_active는 None이므로 True def get_user(self, user_id): try: return UserModel.objects.get(pk=user_id) # 유저를 pk로 가져온다 except UserModel.DoesNotExist: return None 장고의 프로젝트 settings.py파일 12345# settings.pyAUTHENTICATION_BACKENDS = [ 'my_user.my_auth.UserBackend', # 우리가 만든 AUTH를 먼저 검사 'django.contrib.auth.backends.ModelBackend', # Django가 관리하는 AUTH]","link":"/2017/02/02/Django-CustomAuth/"},{"title":"Virtualenv/VirtualenvWrapper OS별 설치&이용법","text":"Virtualenv란?Virtualenv란 시스템 OS에 설치된 주 python뿐만 아니라 여러 버전의 Python과 프로젝트별로 다른 종류의 라이브러리를 사용하는 것에 있어 가장 핵심된 기능을 제공합니다. 예를들어, 어떤 옛날 프로젝트에서는 Python2.7버전에 pip로 Django1.6을 사용했다고 가정해봅시다. 하지만 이번에 새로 시작하는 프로젝트는 Python3.6에 pip로 Django1.10을 사용하려고 합니다. 물론 가장 쉬운 방법은 개발 환경별로 다른 컴퓨터를 사용하는 것이지만, 공간적/금전적/편의적으로 어렵습니다. 따라서 우리는 Python실행파일과 pip로 설치된 라이브러리들을 독립된 폴더에 넣어버리는 방법을 선택할 수 있는데, 이것이 Virtualenv의 핵심입니다. 아래 가이드는 OS별로 나누어져있습니다. [MAC OS가이드] [LINUX 가이드(UBUNTU)] [WINDOWS 가이드] 2016.12.30 기준 MAC OS가 완성되어있습니다. MAC OS(OS X)의 경우Virtualenv를 설치해보자 [MAC OS]MAC OS에는 시스템 전역에 기본적으로 Python2가 설치되어있기 때문에 아래 명령어로 쉽게 pip를 설치할 수 있습니다. 1$ sudo easy_install pip 만약 sudo로 시스템 전역에 설치하기가 싫다면 HomeBrew를 이용해 Python을 유저영역에 설치할 수도 있습니다. 12$ brew install python#Python3의 경우는 brew install python3) pip가 성공적으로 설치되었는지 확인하려면 다음 명령어로 pip의 버전을 확인해 보면 됩니다. 12$ pip -V#Python3 pip의 경우에는 pip3 -V 만약 pip나 pip3이라는 명령어가 먹히지 않는다면 아래의 명령어로 Python의 모듈로서 pip를 호출할 수 있습니다. 12345# Python2의 경우$ python -m pip -V# Python3의 경우$ python3 -m pip -V Virtualenv와 VirtualenvWrapper는 pip를 통해 설치가 가능합니다. 12345# Python2의 경우$ pip install virtualenv virtualenvwrapper# Python3의 경우$ pip3 install virtualenv virtualenvwrapper 만약 pip/pip3 명령이 먹지 않는다면 아래 명령어로 대체할 수 있습니다.(시스템에 easy_install로 pip를 설치한 경우 sudo권한이 필요할 수 있는데, 이때는 sudo pip install으로 명령어 앞에 sudo를 붙여줍시다.) 12345# Python2 pip의 경우$ python -m pip install virtualenv virtualenvwrapper# Python3 pip의 경우$ python3 -m pip install virtualenv virtualenvwrapper 지금까지 사용한 pip와 pip3은 virtualenv를 어느 pip에 설치할까에 대한 내용일 뿐, 파이썬 가상환경에 어떤 Python이 설치될지와는 무관합니다. Virtualenv의 기본적 명령어 [MAC OS]Virtualenv는 기본적으로 아래의 명령어로 동작합니다. 1234$ virtualenv --python=파이썬버전 가상환경이름# ex)# $ virtualenv --python=python3.5 test_env# $ virtualenv --python=python2.7 test_env2 이와 같이 Python버전을 명시해주고 가상환경을 만들 수 있습니다. (단, 선택할 Python은 시스템에 깔려있는 버전이어야 합니다.) 만약 와 같이 The path x.x does not exist라는 에러가 난다면 PYTHON의 PATH을 절대경로로 맞춰줘야 합니다. which python3을 했을 때 /usr/bin/python3이 나왔다면, virtualenv --python=/usr/bin/python3와 같이 절대경로로 입력해주시면 됩니다. 만든 가상환경에 진입(가상환경을 활성화)하려면 아래 명령어를 이용하면 됩니다. 1$ source 가상환경이름/bin/activate Python3이 설치된 test_env로 진입한 경우 Python2가 설치된 test_env2로 진입한 경우 각각 다른 python버전이 실행되고 있다는 것을 알 수 있습니다. 이후 pip를 통해 외부 모듈과 라이브러리들을 설치하는 경우, source 명령어로 가상환경에 진입하지 않으면 라이브러리들을 불러쓸 수 없게됩니다. 즉, 프로젝트 별로 다른 라이브러리만이 설치된 환경을 구성한 것이죠. VirtualenvWrapper 설정하기VirtualEnv를 사용하기 위해서는 source를 이용해 가상환경에 진입합니다. 그러나, 이 진입 방법은 가상환경이 설치된 위치로 이동해야되는 것 뿐 아니라 가상환경이 어느 폴더에 있는지 일일이 사용자가 기억해야 하는 단점이 있습니다. 이를 보완하기 위해 VirtualenvWrapper를 사용합니다. 또한, VirtualenvWrapper를 사용할 경우 터미널이 현재 위치한 경로와 관계없이 가상환경을 활성화할 수 있다는 장점이 있습니다. VirtualenvWrapper는 .bashrc나 .zshrc\u001d에 약간의 설정과정을 거쳐야 합니다. 우선 홈 디렉토리로 이동해보세요. 1$ cd ~ 가상환경이 들어갈 폴더 .virtualenvs를 만들어주세요. 1$ mkdir ~/.virtualenvs 그리고 홈 디렉토리의 .bashrc나 .zshrc의 파일 제일 마지막에 아래 코드를 복사해 붙여넣어줍시다.(파일이 없다면 만들어 사용하시면 됩니다.) 1234# python virtualenv settingsexport WORKON_HOME=~/.virtualenvsexport VIRTUALENVWRAPPER_PYTHON=\"$(which python3)\" # Usage of python3source /usr/local/bin/virtualenvwrapper.sh 저장하고 나온 후 터미널을 종료후 새로 켜주면, VirtualenvWrapper의 명령어들을 사용할 수 있습니다. 만약 /usr/local/bin/virtualenvwrapper.sh파일이 존재하지 않는다면 다음 명령어로 virtualenvwrapper.sh파일을 찾아서 위 코드를 바꿔 사용하세요. 1find /usr -name virtualenvwrapper.sh VirtualenvWrapper 명령어들VirtualenvWrapper의 명령어는 여러가지가 존재하지만, 이 포스팅에서는 기본적인 것만 다루고 넘어갑니다. 가상환경 만들기 123$ mkvirtualenv 가상환경이름# 예시# $ mkvirtualenv test_env3 mkvirtualenv 명령어를 사용할 경우 홈 디렉토리의 .virtualenvs폴더 안에 가상환경이름을 가진 폴더(test_env3)가 생깁니다. 가상환경 지우기 123$ rmvirtualenv 가상환경이름# 예시# $ rmvirtualenv test_env3 rmvirtualenv 명령어를 사용할 경우 mkvirtualenv로 만든 가상환경을 지워줍니다. 만든 가상환경을 지우는 방법은 이방법 뿐 아니라 홈 디렉토리의 .virtualenvs폴더 안의 가상환경이름을 가진 폴더를 지우는 방법도 있습니다. 가상환경 진입하기 + 가상환경 목록 보기 123456$ workon 가상환경이름# 가상환경으로 진입시 앞에 (가상환경이름)이 붙습니다.(가상환경이름) $# 예시# $ workon test_env3# (test_env3) $ workon명령어를 통해 mkvirtualenv로 만든 가상환경으로 진입할 수 있습니다. workon명령어를 가상환경이름 없이 단순하게 칠 경우, 현재 만들어져있는 가상환경의 전체 목록을 불러옵니다. 12$ workontest_env3 가상환경 빠져나오기 12345(가상환경이름) $ deactivate$# 예시# (test_env3) $ deactivate# $ 가상환경에서 빠져나오는 것은 다른것들과 동일하게 deactivate명령어로 빠져나올 수 있습니다. LINUX(UBUNTU)의 경우Virtualenv를 설치해 보자 [LINUX(UBUNTU)]Ubuntu의 경우에는 14버전 기준으로 Python2와 Python3이 기본적으로 설치되어있습니다. Ubuntu16에서는 Python3이 기본입니다. 하지만 pip/pip3이 설치되어있지 않을 수 있기 때문에 python-pip나 python3-pip를 설치해야 합니다. 123456# APT를 업데이트$ sudo apt-get update &amp;&amp; apt-get upgrade -y# Python2를 이용할 경우$ sudo apt-get install python-pip python-dev# Python3을 이용할 경우$ sudo apt-get install python3-pip python3-dev 꼭 python-dev와 python3-dev를 설치하지 않아도 됩니다. 하지만 이후 정상적 동작을 보장할 수 없습니다. pip 설치가 완료되었는지 확인하려면 아래 명령어를 입력해보면 됩니다.(이번 게시글에서는 python3-pip로 진행합니다. Python2의 pip를 이용하시려면 python3-pip대신 python-pip를 설치하셔서 pip명령어를 사용하세요.) 1234# Python2 pip의 경우$ pip -V# Python3 pip의 경우$ pip3 -V 이제 pip설치가 완료되었으므로 Virtualenv와 VirtualenvWrapper를 설치해보겠습니다. 12345# Python2의 경우$ pip install virtualenv virtualenvwrapper# Python3의 경우$ pip3 install virtualenv virtualenvwrapper 만약 pip/pip3 명령이 먹지 않는다면 아래 명령어로 대체할 수 있습니다.(시스템에 root권한으로 pip를 설치한 경우 sudo권한이 필요할 수 있는데, 이때는 sudo pip install으로 명령어 앞에 sudo를 붙여줍시다.) 12345# Python2 pip의 경우$ python -m pip install virtualenv virtualenvwrapper# Python3 pip의 경우$ python3 -m pip install virtualenv virtualenvwrapper 지금까지 사용한 pip와 pip3은 virtualenv를 어느 pip에 설치할까에 대한 내용일 뿐, 파이썬 가상환경에 어떤 Python이 설치될지와는 무관합니다. Virtualenv의 기본적 명령어 [LINUX(UBUNTU)]Virtualenv는 기본적으로 아래의 명령어로 동작합니다. 1234$ virtualenv --python=파이썬버전 가상환경이름# ex)# $ virtualenv --python=python3.5 py3_env# $ virtualenv --python=python2.7 test_env2 만약 virtualenv 라는 명령이 먹히지 않는다면 python3 -m virtualenv(python2는 python -m virtualenv)명령어를 이용하거나, 쉘을 껐다가 다시 켜주세요. 만약 와 같이 The path x.x does not exist라는 에러가 난다면 PYTHON의 PATH을 절대경로로 맞춰줘야 합니다. which python3을 했을 때 /usr/bin/python3이 나왔다면, virtualenv --python=/usr/bin/python3와 같이 절대경로로 입력해주시면 됩니다. 이와 같이 Python버전을 명시해주고 가상환경을 만들 수 있습니다. (단, 선택할 Python은 시스템에 깔려있는 버전이어야 합니다. Ubuntu16의 경우 python2이 깔려있지 않을 수 있습니다.) 만든 가상환경에 진입(가상환경을 활성화)하려면 아래 명령어를 이용하면 됩니다. 1$ source 가상환경이름/bin/activate Python3이 설치된 py3_env로 진입한 경우 이후 pip를 통해 외부 모듈과 라이브러리들을 설치하는 경우, source 명령어로 가상환경에 진입하지 않으면 라이브러리들을 불러쓸 수 없게됩니다. 즉, 프로젝트 별로 다른 라이브러리만이 설치된 환경을 구성한 것이죠. VirtualenvWrapper 설정하기 [LINUX(UBUNTU)]VirtualEnv를 사용하기 위해서는 source를 이용해 가상환경에 진입합니다. 그러나, 이 진입 방법은 가상환경이 설치된 위치로 이동해야되는 것 뿐 아니라 가상환경이 어느 폴더에 있는지 일일이 사용자가 기억해야 하는 단점이 있습니다. 이를 보완하기 위해 VirtualenvWrapper를 사용합니다. 또한, VirtualenvWrapper를 사용할 경우 터미널이 현재 위치한 경로와 관계없이 가상환경을 활성화할 수 있다는 장점이 있습니다. VirtualenvWrapper는 .bashrc나 .zshrc\u001d에 약간의 설정과정을 거쳐야 합니다. 우선 홈 디렉토리로 이동해보세요. 1$ cd ~ 가상환경이 들어갈 폴더 .virtualenvs를 만들어주세요. 1$ mkdir ~/.virtualenvs 그리고 홈 디렉토리의 .bashrc나 .zshrc의 파일 제일 마지막에 아래 코드를 복사해 붙여넣어줍시다.(파일이 없다면 만들어 사용하시면 됩니다.) 1234# python virtualenv settingsexport WORKON_HOME=~/.virtualenvsexport VIRTUALENVWRAPPER_PYTHON='$(command \\which python3)' # Usage of python3source /usr/local/bin/virtualenvwrapper.sh 저장하고 나온 후 터미널을 종료후 새로 켜주면, VirtualenvWrapper의 명령어들을 사용할 수 있습니다. 만약 /usr/local/bin/virtualenvwrapper.sh파일이 존재하지 않는다면 다음 명령어로 virtualenvwrapper.sh파일을 찾아서 위 코드를 바꿔 사용하세요. 1find /usr -name virtualenvwrapper.sh VirtualenvWrapper 명령어들 [LINUX(UBUNTU)]VirtualenvWrapper의 명령어는 여러가지가 존재하지만, 이 포스팅에서는 기본적인 것만 다루고 넘어갑니다. 가상환경 만들기 123$ mkvirtualenv 가상환경이름# 예시# $ mkvirtualenv test_env3 mkvirtualenv 명령어를 사용할 경우 홈 디렉토리의 .virtualenvs폴더 안에 가상환경이름을 가진 폴더(test_env3)가 생깁니다. 가상환경 지우기 123$ rmvirtualenv 가상환경이름# 예시# $ rmvirtualenv test_env3 rmvirtualenv 명령어를 사용할 경우 mkvirtualenv로 만든 가상환경을 지워줍니다. 만든 가상환경을 지우는 방법은 이방법 뿐 아니라 홈 디렉토리의 .virtualenvs폴더 안의 가상환경이름을 가진 폴더를 지우는 방법도 있습니다. 가상환경 진입하기 + 가상환경 목록 보기 123456$ workon 가상환경이름# 가상환경으로 진입시 앞에 (가상환경이름)이 붙습니다.(가상환경이름) $# 예시# $ workon test_env3# (test_env3) $ workon명령어를 통해 mkvirtualenv로 만든 가상환경으로 진입할 수 있습니다. workon명령어를 가상환경이름 없이 단순하게 칠 경우, 현재 만들어져있는 가상환경의 전체 목록을 불러옵니다. 12$ workontest_env3 가상환경 빠져나오기 12345(가상환경이름) $ deactivate$# 예시# (test_env3) $ deactivate# $ 가상환경에서 빠져나오는 것은 다른것들과 동일하게 deactivate명령어로 빠져나올 수 있습니다. Windows의 경우","link":"/2016/12/28/HowToSetup-Virtualenv-VirtualenvWrapper/"},{"title":"Django에 Social Login 붙이기: Django세팅부터 Facebook/Google 개발 설정까지","text":"Django등 웹 서비스를 제공하며 항상 다루게 되는 주제가 있습니다. 유저를 우리 서비스의 유저 모델을 통해 직접 가입시키느냐, 혹은 타사의 oAuth를 이용한 Social Login을 붙여 가입없이(혹은 최소화) 서비스를 이용할 수 있도록 유도 하느냐 등입니다. Django에서 이러한 Social Login을 이용하기 위한 라이브러리는 여러개가 있었고, 대표적으로는 django-social-auth와 python-social-auth가 있었지만, 두 프로젝트 모두 현재(2017.02.08기준) Deprecated되었고 이 프로젝트들은 python-social-auth가 org자체로 이전해 social-auth-app-django로 바뀌었습니다. 한편 -social-auth들의 대체재로 django-allauth가 있는데, 올해 1월에도 새 버전 릴리즈가 있는만큼 활동적인 프로젝트입니다. (하지만 이번 글에서는 다루지 않습니다.) 이번 게시글에서는 social-auth-app-django을 이용해 Django 프로젝트에 social login을 붙여봅니다. 참조한 공식 docs는 python-social-auth configuration django에서 볼 수 있습니다. 참고: social-auth-app-django는 pip패키지 이름이며, 프로젝트 이름은 python-social-auth로 동일합니다. 설치하기widgets:1pip install social-auth-app-django Django의 기본 ORM을 이용하고 있다면 social-auth-app-django를 pip로 설치하면 됩니다. settings.py 설정하기widgets:INSTALLED_APPS 추가하기settings.py에 social_django를 추가해줍니다. 12345INSTALLED_APPS = ( ... 'social_django', ...) 앱 추가후 migrate를 해줘야 정상적으로 Social Auth용 DB Table이 생성됩니다. 1python manage.py migrate AUTHENTICATION_BACKENDS 추가하기Social Login은 기존 유저모델과 함께 사용이 가능합니다. 하지만 기본 유저 ModelBackend를 사용하지 않고 독자적인 ModelBackend를 사용하기 때문에 settings.py의 AUTHENTICATION_BACKENDS에 Social login용 Backends를 추가해줘야 합니다. 123456AUTHENTICATION_BACKENDS = [ 'social_core.backends.google.GoogleOAuth2', # Google 'social_core.backends.facebook.FacebookOAuth2', # Facebook ... 'django.contrib.auth.backends.ModelBackend', # Django 기본 유저모델] django.contrib.auth.backends.ModelBackend가 있어야 createsuperuser로 만들어진 계정의 로그인이 가능해집니다. Social Login용 URL Namespace 지정최상위 프로젝트 urls.py에 지정할 social login의 namespace를 지정해줍니다. 또한, Login 후 어떤 URL로 장고가 유저를 Redirect시킬지 지정해 줍니다. 123SOCIAL_AUTH_URL_NAMESPACE = 'social'LOGIN_REDIRECT_URL='/' 꼭 namespace가 ‘social\u001d’일 필요는 없습니다. 하지만 가이드에서는 ‘social’을 사용하기에 아래 urls.py 설정에서도 동일하게 사용할 예정입니다. Social Login을 위한 API Key/Secret 설정하기우선 프로젝트 BASE_DIR(manage.py파일이 있는 폴더\u001d)에 envs.json이라는 이름의 환경변수를 담은 json 파일을 만들어 줍니다. 123456{ \"FACEBOOK_KEY\":\"숫자숫자숫자들\", \"FACEBOOK_SECRET\":\"숫자영어숫자영어들\", \"GOOGLE_KEY\":\"숫자-영어.apps.googleusercontent.com\", \"GOOGLE_SECRET\":\"숫자영어대문자들\"} 당연하게도 위 파일은 실제 동작하는 Key와 Secret이 아닙니다. Social Login을 사용하기 위해 Google에서는 Google+ API를 활성화 하고 OAuth 2.0 클라이언트 ID를 ‘웹 애플리케이션’으로 생성해 API Key/Secret을 발급받아야 합니다. Google Login은 Google+ API에 연결되어있기 때문에 다른 Login API는 없습니다. Facebook의 경우에는 Facebook for Developers에서 새 앱 추가 후 ‘Facebook 로그인’ 제품을 활성화 시킨 후 앱의 대시보드에서 앱 ID와 앱 시크릿 코드를 받아 이용하면 됩니다. 두 서비스 모두 지정된 url에서만 동작하기 때문에 Google의 경우에는 ‘OAuth 2.0 클라이언트 ID’에서 ‘승인된 리디렉션 URI\u001d’에 http://localhost:8000/complete/google-oauth2/을 추가해줘야 하며, Facebook의 경우에는 ‘Facebook 로그인’의 ‘클라이언트 OAuth 설정’에 있는 ‘유효한 OAuth 리디렉션 URI’에 http://localhost:8000/을 추가해주면 됩니다. 위 설정을 모두 하지 않을 경우 40x번대 에러가 발생합니다. 이제 위에서 만든 envs.json파일을 환경변수로 사용해야 합니다. settings.py파일 최상위에 이와 같은 코드를 적용해 줄 경우, 개발용 envs_dev.json와 배포용 envs.json, 그리고 환경변수로 관리되는 경우 모두 커버가 가능합니다. 12345678910111213141516171819202122232425262728293031323334353637383940import osimport jsonfrom django.core.exceptions import ImproperlyConfigured# Build paths inside the project like this: os.path.join(BASE_DIR, ...)BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))# Env for dev / deploydef get_env(setting, envs): try: return envs[setting] except KeyError: error_msg = \"You SHOULD set {} environ\".format(setting) raise ImproperlyConfigured(error_msg)DEV_ENVS = os.path.join(BASE_DIR, \"envs_dev.json\")DEPLOY_ENVS = os.path.join(BASE_DIR, \"envs.json\")if os.path.exists(DEV_ENVS): # Develop Env env_file = open(DEV_ENVS)elif os.path.exists(DEPLOY_ENVS): # Deploy Env env_file = open(DEPLOY_ENVS)else: env_file = Noneif env_file is None: # System environ try: FACEBOOK_KEY = os.environ['FACEBOOK_KEY'] FACEBOOK_SECRET = os.environ['FACEBOOK_SECRET'] GOOGLE_KEY = os.environ['GOOGLE_KEY'] GOOGLE_SECRET = os.environ['GOOGLE_SECRET'] except KeyError as error_msg: raise ImproperlyConfigured(error_msg)else: # JSON env envs = json.loads(env_file.read()) FACEBOOK_KEY = get_env('FACEBOOK_KEY', envs) FACEBOOK_SECRET = get_env('FACEBOOK_SECRET', envs) GOOGLE_KEY = get_env('GOOGLE_KEY', envs) GOOGLE_SECRET = get_env('GOOGLE_SECRET', envs) 이와 같이 사용할 경우, APACHE웹서버 등에서 시스템 환경변수를 불러오지 못하는 상황이거나, HEROKU나 PythonAnywhere와 같은 PaaS에서도 Django코드와 API키들을 완전히 분리해 사용할 수 있습니다. 위에서 지정한 FACEBOOK_KEY들을 SocialLogin에 할당해 줍니다. 123456789101112# SocialLogin: FacebookSOCIAL_AUTH_FACEBOOK_KEY = FACEBOOK_KEYSOCIAL_AUTH_FACEBOOK_SECRET = FACEBOOK_SECRETSOCIAL_AUTH_FACEBOOK_SCOPE = ['email']SOCIAL_AUTH_FACEBOOK_PROFILE_EXTRA_PARAMS = { 'fields': 'id, name, email'}# SocialLogin: GoogleSOCIAL_AUTH_GOOGLE_OAUTH2_KEY = GOOGLE_KEYSOCIAL_AUTH_GOOGLE_OAUTH2_SECRET = GOOGLE_SECRETSOCIAL_AUTH_GOOGLE_OAUTH2_SCOPE = ['email'] 위 코드는 가장 기본적인 ‘email’을 유저 식별도구로 받아옵니다. _PROFILE_EXTRA_PARAMS를 이용해 다른 Field를 받아올 수도 있습니다. (필수 아님) project폴더의 urls.py 설정하기(최상위 urls.py)widgets:이제 프로젝트 폴더의 urls.py에 Social Login이 사용할 url들을 등록하고 namespace를 지정해 Template에서 사용할 수 있도록 설정해야 합니다. 12345678from django.conf.urls import url, include # url뿐 아니라 include를 import해야 합니다.from django.conf import settingsfrom django.contrib import adminurlpatterns = [ url(r'^admin/', admin.site.urls), url('', include('social_django.urls', namespace='social')), # 이 줄을 등록해주면 됩니다.] 이와 같이 social_django.urls를 include하고 ‘social’ namespace를 등록해 줍니다. Template에서 Social Login url 호출하기widgets:위 코드들을 추가해주는 것 만으로도 기본적인 Social Login기능은 완성되었습니다. 이제 Template에서 호출을 해봅시다. 12{% raw %}&lt;a href=\"{% url \"social:begin\" \"google-oauth2\" %}\"&gt;&lt;button class=\"btn btn-danger\" style=\"width: 40%\"&gt;G+ Login&lt;/button&gt;&lt;/a&gt;&lt;a href=\"{% url \"social:begin\" \"facebook\" %}\"&gt;&lt;button class=\"btn btn-primary\" style=\"width: 40%\"&gt;FB Login&lt;/button&gt;&lt;/a&gt;{% endraw %} 이와 같이 button을 등록해 호출할 수 있습니다. 위 버튼을 누를 경우 각각 Google/Facebook의 Social Login페이지로 넘어갑니다. 수고하셨습니다!widgets:위 코드만으로도 약간의 조작을 통해 더 멋진 Social Login기능을 구현하실 수 있으리라 생각합니다. Happy Coding!","link":"/2017/02/08/Setup-SocialAuth-for-Django/"},{"title":"나만의 웹 크롤러 만들기(4): Django로 크롤링한 데이터 저장하기","text":"좀 더 보기 편한 깃북 버전의 나만의 웹 크롤러 만들기가 나왔습니다! (@2017.03.18) 본 블로그 테마가 업데이트되면서 구 블로그의 URL은 https://beomi.github.io/beomi.github.io_old/로 변경되었습니다. 예제 코드에서는 변경을 완료하였지만 캡쳐 화면은 변경하지 않았으니 유의 바랍니다. 이전게시글: 나만의 웹 크롤러 만들기(3): Selenium으로 무적 크롤러 만들기 Python을 이용해 requests와 selenium을 이용해 웹 사이트에서 데이터를 크롤링해 보았습니다. 하지만 이러한 데이터를 체계적으로 관리하려면 DB가 필요하고, 이러한 DB를 만들고 관리하는 방법이 여러가지가 있지만 이번 가이드에서는 Python 웹 프레임워크인 django의 Database ORM을 이용해 DB를 만들고 데이터를 저장해 보려 합니다. 이번 가이드에서는 1회차 가이드였던 이 블로그를 크롤링해서 나온 결과물을 Django ORM으로 Sqlite DB에 저장해보는 것까지를 다룹니다. 이번 가이드는 기본적으로 Django의 Model에 대해 이해하고 있는 분들에게 추천합니다. 만약 django가 처음이시라면 DjangoGirls Tutorial: DjangoTube를 따라해보시면 기본적인 이해에 도움이 되시리라 생각합니다. 30분 내외로 따라가실 수 있습니다. Django 프로젝트 만들기widgets:우선 크롤링 한 데이터를 저장할 Django 프로젝트와 앱을 만들고, Model을 통해 DB를 만들어야 합니다. Django 설치하기Django는 pip로 간편하게 설치할 수 있습니다. 가상환경을 이용해 설치하는 것을 추천합니다. 가상환경은 python3.4이후부터는 python3 -m venv 가상환경이름으로 만드실 수 있습니다. 1pip install django 글 작성 시점인 2017.02.28 기준 1.10.5 버전이 최신버전입니다. Django Start Project | 프로젝트 만들기django가 성공적으로 설치되면 django-admin이라는 명령어로 장고 프로젝트를 생성할 수 있습니다. 이번 가이드에서는 websaver라는 이름의 프로젝트를 만들어보겠습니다. 1django-admin startproject websaver 성공적으로 생길 경우 어떠한 반응도 나타나지 않습니다. 위 명령어를 치면 명령어를 친 위치에 websaver라는 폴더가 생기고, 그 안의 구조는 아래와 같습니다. cd websaver로 websaver폴더 안으로 진입한 상태입니다. tree 명령어는 mac에서 brew install tree로 설치한 명령어입니다. 기본적으로는 깔려있지 않습니다. 이와 같이 manage.py파일과 프로젝트 이름인 websaver라는 이름의 폴더가 함께 생성됩니다. \u0004Django Start App | 장고 앱 만들기Django는 프로젝트와 그 안의 앱으로 관리됩니다. 이 앱은 하나의 기능을 담당하는 단위로 보시면 됩니다. 앱은 manage.py파일을 통해 startapp이라는 명령어로 생성 가능합니다. parsed_data라는 이름의 앱을 만들어보겠습니다. 1python manage.py startapp parsed_data manage.py 파일이 있는 곳에서 실행합니다. django가 설치된 가상환경에 진입해 있는지 꼭 확인하세요! 이제 아래와 같은 구조로 앱이 생겼을 것인데, 이 앱을 Django가 관리하도록 websaver폴더 안의 settings.py파일의 INSTALLED_APPS에 추가해줘야 합니다. 유의: .pyc파일은 python실행시 생기는 캐싱 파일입니다. 없으셔도 전혀 문제는 발생하지 않습니다. 123456789101112# websaver/settings.py...INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'parsed_data', # 앱을 추가해 줍시다.]... Django First Migration | 첫 마이그레이션​장고는 python manage.py migrate이라는 명령어로 DB를 migrate합니다. 1python manage.py migrate 위 명령어를 입력하면 아래와 같이 Django에서 사용하는 기본적인 DB가 생성됩니다. parsed_data App Model | parsed_data 앱 모델 만들기이제 DB구조를 관리해주는 Model을 만들어 줘야 합니다. Django에서 모델은 앱 단위로 만들어지고 구성됩니다. 따라서 앞서 만들어준 parsed_data앱 안의 models.py파일을 수정해줘야 합니다. 이 모델 파일은 크롤링해온 데이터를 필드별로 저장하는 것이 목적입니다. 따라서 크롤링한 데이터를 파이썬이 관리할 수 있는 객체로 만들어두는 것이 중요합니다. 이번 가이드에서는 나만의 웹 크롤러 만들기 With Requests/BeautifulSoup에서 만든 parser.py파일을 수정해 게시글의 title와 link를 DB에 저장해보겠습니다. 따라서 이번 앱의 모델에서는 title와 link라는 column을 가진 BlogData라는 이름의 Table을 DB에 만들면 됩니다. django models의 class는 DB의 Table이 됩니다. 1234567# parsed_data/models.pyfrom django.db import modelsclass BlogData(models.Model): title = models.CharField(max_length=200) link = models.URLField() 이와 같이 만들어주면 title은 200글자 제한의 CharField로, link는 URLField로 지정됩니다. parsed_data App Makemigrations &amp; Migrate | 앱 DB 반영하기이제 해야 할 일은 Django가 모델을 관리하도록 하려면 makemigrations를 통해 DB의 변경 정보를 정리하고, migrate를 통해 실제 DB에 반영하는 과정을 진행해야 합니다. django가 설치된 가상환경에서 실행하도록 합시다. 명령어의 실행 위치는 manage.py파일이 있는 곳입니다. 12python manage.py makemigrations parsed_datapython manage.py migrate parsed_data 각 명령어 입력시 아래와 같이 결과가 나타난다면 성공적으로 DB에 반영된 것입니다. 크롤링 함수 만들기widgets:나만의 웹 크롤러 만들기 With Requests/BeautifulSoup에서 만든 parser.py파일을 수정해보겠습니다. 이번 파일은 manage.py가 있는 위치에 parser.py라는 이름으로 저장해보겠습니다. 만약 requests와 bs4가 설치되어있지 않다면 pip로 설치해주세요! 1234567891011121314151617181920212223# parser.pyimport requestsfrom bs4 import BeautifulSoupimport jsonimport os# python파일의 위치BASE_DIR = os.path.dirname(os.path.abspath(__file__))req = requests.get('https://beomi.github.io/beomi.github.io_old/')html = req.textsoup = BeautifulSoup(html, 'html.parser')my_titles = soup.select( 'h3 &gt; a' )data = {}for title in my_titles: data[title.text] = title.get('href')with open(os.path.join(BASE_DIR, 'result.json'), 'w+') as json_file: json.dump(data, json_file) 이전의 parser.py\u001d파일은 위와 같습니다. 이제 이 파일을 parse_blog라는 함수로 만들고, {‘블로그 글 타이틀’: ‘블로그 글 링크’}로 이루어진 딕셔너리를 반환하도록 만들어 봅시다. 123456789101112131415# parser.pyimport requestsfrom bs4 import BeautifulSoupdef parse_blog(): req = requests.get('https://beomi.github.io/beomi.github.io_old/') html = req.text soup = BeautifulSoup(html, 'html.parser') my_titles = soup.select( 'h3 &gt; a' ) data = {} for title in my_titles: data[title.text] = title.get('href') return data 이제 parse_blog라는 함수를 다른 파일에서 import해 사용할 수 있습니다. 또한, 현재 프로젝트 폴더의 구조는 아래와 같습니다. 하지만 현재 parse_blog\u001d함수는 Django에 저장하는 기능을 갖고 있지 않습니다. 따라서 약간 더 추가를 해줘야 합니다. Django 환경 불러오기12345678910111213141516171819202122# parser.pyimport requestsfrom bs4 import BeautifulSoup# 아래 4줄을 추가해 줍니다.import os# Python이 실행될 때 DJANGO_SETTINGS_MODULE이라는 환경 변수에 현재 프로젝트의 settings.py파일 경로를 등록합니다.os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"websaver.settings\")# 이제 장고를 가져와 장고 프로젝트를 사용할 수 있도록 환경을 만듭니다.import djangodjango.setup()def parse_blog(): req = requests.get('https://beomi.github.io/beomi.github.io_old/') html = req.text soup = BeautifulSoup(html, 'html.parser') my_titles = soup.select( 'h3 &gt; a' ) data = {} for title in my_titles: data[title.text] = title.get('href') return data 위 코드에서 아래 4줄을 추가해 줄 경우, 이 파일을 단독으로 실행하더라도 마치 manage.py\u001d을 통해 django를 구동한 것과 같이 django환경을 사용할 수 있게 됩니다. Django ORM으로 데이터 저장하기1234import osos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"websaver.settings\")import djangodjango.setup() python manage.py shell을 실행하는 것과 비슷한 방법입니다. 이제 models에서 우리가 만든 BlogData를 import해 봅시다. 123456789101112131415161718192021222324252627# parser.pyimport requestsfrom bs4 import BeautifulSoupimport osos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"websaver.settings\")import djangodjango.setup()# BlogData를 import해옵니다from parsed_data.models import BlogDatadef parse_blog(): req = requests.get('https://beomi.github.io/beomi.github.io_old/') html = req.text soup = BeautifulSoup(html, 'html.parser') my_titles = soup.select( 'h3 &gt; a' ) data = {} for title in my_titles: data[title.text] = title.get('href') return data# 이 명령어는 이 파일이 import가 아닌 python에서 직접 실행할 경우에만 아래 코드가 동작하도록 합니다.if __name__=='__main__': blog_data_dict = parse_blog() for t, l in blog_data_dict.items(): BlogData(title=t, link=l).save() 위와 같이 parser.py를 수정한 후 터미널에서 parser.py파일을 실행해 봅시다. 1python parser.py 아무런 에러가 나지 않는다면 성공적으로 저장된 것입니다. 저장된 데이터 Django Admin에서 확인하기SuperUser | 관리자계정 만들기Django는 Django Admin이라는 강력한 기능을 제공합니다. 우선 Admin 계정을 만들어야 합니다. createsuperuser 명령어로 만들 수 있습니다. 기본적으로 유저이름, 이메일, 비밀번호를 받습니다. 이메일은 입력하지 않아도 됩니다. 앱에 Admin 등록하기Django가 어떤 앱을 admin에서 관리하도록 하려면 앱 폴더(parsed_data) 안의 admin.py파일을 수정해줘야 합니다. 1234567# parsed_data/admin.pyfrom django.contrib import admin# models에서 BlogData를 import 해옵니다.from .models import BlogData# 아래의 코드를 입력하면 BlogData를 admin 페이지에서 관리할 수 있습니다.admin.site.register(BlogData) Django Runserver | 장고 서버 실행하기이제 manage.py가 있는 위치에서 runserver명령어로 장고 개발 서버를 실행해 봅시다. 1python manage.py runserver 아래와 같이 나타난다면 성공적으로 서버가 실행된 것입니다. 이제 http://localhost:8000/admin/로 들어가 봅시다. 아까 createsuperuser로 만든 계정으로 로그인 해 봅시다. 우리가 만든 parsed_data앱 안에 BlogData라는 항목이 나와있는 것을 볼 수 있습니다. BlogData object라는 이름으로 데이터들이 들어와 있는 것을 확인할 수 있습니다. 하나를 클릭해 들어가 보면 아래와 같이 title와 link가 성공적으로 들어와 있는 것을 볼 수 있습니다. 약간 더 나아가기widgets:위에서 Admin페이지에 들어갈 때 모든 데이터들의 이름이 BlogData object로 나와있는 것을 볼 수 있습니다. 우리가 만들어 준 parsed_data/models.py파일의 BlogData Class를 살펴보면 models.Model클래스를 상속받아 만들었고, 이 클래스는 기본적으로 ClassName + object라는 값을 반환하는 __str__함수를 내장하고 있습니다. 따라서 models.Model을 상속받은 BlogData의 __str__\u001d함수에서는 BlogData object라는 값을 반환합니다. 이 __str__함수를 오버라이딩해 사용하면 Admin에서 데이터의 이름을 좀 더 직관적으로 알 수 있습니다. str 함수 오버라이딩하기parsed_data앱 폴더 안의 models.py파일을 아래와 같이 수정해 봅시다. 12345678910# parsed_data/models.pyfrom django.db import modelsclass BlogData(models.Model): title = models.CharField(max_length=200) link = models.URLField() def __str__(self): return self.title 위 코드는 BlogData 데이터 객체의 title 값을 반환합니다. 이제 장고 서버를 다시 켜주고 BlogData admin page로 들어가면 타이틀 이름으로 된 데이터들을 볼 수 있습니다. 현재 models.py파일을 수정했지만 DB에 반영되는 사항이 아니기 때문에 makemigrations\u001d나 migrate를 해줄 필요가 없습니다. 다음 가이드에서는..widgets:다음 가이드는 주기적으로 데이터를 크롤링 해, 새로운 데이터가 생기는 경우 텔레그램 봇으로 메시지 알림을 보내주는 과정을 다룰 예정입니다. 다음 가이드: 나만의 웹 크롤러 만들기(5): 웹페이지 업데이트를 알려주는 Telegram 봇","link":"/2017/03/01/HowToMakeWebCrawler-Save-with-Django/"},{"title":"[DjangoTDDStudy] #01: 개발환경 세팅하기(Selenium / ChromeDriver)","text":"Web을 직접 테스트한다고?웹 서비스를 개발하는 과정에서 꼭 필요한 것이 있다. 바로 실제로 기능이 동작하는지 테스트 하는 것.이 테스트를 개발자가 직접 할 수도 있고, 혹은 전문적으로 테스트만 진행하는 QA팀에서 진행할 수도 있다.하지만 위의 두 방법은 ‘사람이 직접 해야한다’는 공통점이 있다. 이걸 자동화 할 수 있다면 어떨까? Selenium 설치하기Selenium은 위의 질문에 대한 답변을 준다. 사람이 하기 귀찮은 부분을 자동화! 우선 Selenium을 설치해주자.(단, Python3가 설치되어있다는 상황을 가정하며, Virtualenv / Pyvenv등의 가상환경 사용을 권장한다. 이 게시글에서는 tdd_study라는 이름의 가상환경을 이용한다.) 1$ pip install selenium PIP가 설치되어있다면 위 명령어 한줄만으로 Selenium이 설치된다.Selenium의 설치가 완료되었다면, 우선 ChromeDriver를 받아준다. ChromeDriver 설치하기Selenium은 기본적으로 Firefox 드라이버를 내장하고있다. 이 ‘Driver’들은 시스템에 설치된 브라우저들을 자동으로 동작하게 하는 API를 내장하고 있고, 우리는 각 브라우저별 드라이버를 다운받아 쉽게 이용할 수 있다. 현재 Selenium은 대다수의 모던 웹브라우저들(Chrome, Firefox, IE, Edge, Phantomjs, etc.)을 지원하고 있기 때문에, 일상적으로 사용하는 크롬드라이버를 사용하기로 했다.(만약 Headless Browser를 이용해야 한다면 Phantomjs를 이용해보자.) 크롬드라이버는 크로미움의 ChromeDriver에서 최신 버전으로 받을 수 있고, 이번 스터디에서는 Chrome v54에서 v56까지를 지원하는 ChromeDriver 2.27버전을 이용하려 한다. 크롬드라이버는 어떤 파일을 설치하는 것이 아니라, Binary가 내장되어있는 하나의 실행파일이라고 보면 된다. 만약 MAC OS나 Linux계열을 사용한다면, 크롬드라이버를 받은 후 그 파일을 PATH에 등록해 주자. 예시)위 사이트에서 받은 파일의 이름이 chromedriver 이고 받은 경로가/Users/beomi/bin 이라면,사용하는 쉘(bash / zsh등)의 RC파일(유저 홈 디렉토리의 .bashrc / .zshrc)의 제일 아래에 1export PATH=${PATH}:~/bin 위의 코드를 적고 저장한 후, 쉘을 재실행해준다.(터미널을 껐다가 켜주자.) 이렇게 하고나면, 아래 실습시 크롬드라이버의 위치를 지정하지 않고 파일 이름만으로 이용 할 수 있다는 장점이 있다. Django 설치하기 Django는 앞으로 우리가 스터디에 사용할 WebFramework다. 1$ pip install django 위 명령어로 역시 쉽게 설치 가능하다.(2016.12.27기준 1.10.4가 최신버전이며, 1.10.x버전으로 스터디를 진행할 예정이다.) pip로 Django가 설치되고 나면 django-admin 이라는 명령어를 쉘에서 사용할 수 있다. 실습0. 설치 잘 되었는지 확인해 보기쉘에서 1$ pip list --format=columns 라는 명령어를 쳤을 때 아래 스샷과 같이 Django와 selenium이 보인다면 정상적으로 설치가 진행 된 것이다. 설치가 잘 되었다면 다음으로 진행해 보자. 1. Selenium 이용해보기12345678from selenium import webdriverbrowser = webdriver.Chrome('chromedriver')# chromedriver가 Python파일과 같은 위치에 있거나, 혹은 OS의 PATH에 등록되어 쉘에서 실행 가능한 경우 위와같이 한다.# 혹은 browser = webdriver.Chrome('/path/to/chromedriver')의 절대경로로 해도 된다.browser.get('http://localhost:8000')assert 'Django' in browser.title 위 코드는 Chrome 브라우저를 작동시키는 WebDriver를 이용해 새 크롬 창을 띄우고 http://localhost:8000이라는 url로 들어간 후 브라우저의 title에 ‘Django’라는 글자가 들어가 있는지를 확인(Assert)해준다. 현재 상황에서는 django웹서버를 실행하지 않았기 때문에 당연하게도 AssertionError가 난다. 2. Django 서버 띄우기이제 Django서버를 띄워보자. Django는 django-admin이라는 명령어를 통해 기본적인 뼈대가 구성된 프로젝트 폴더 하나를 만들어 준다. 1$ django-admin startproject tdd_study_proj 위 명령어를 치면 다음과 같은 폴더 구조를 가진 프로젝트 폴더가 생긴다. 12345678910(tdd_study) ➜ tdd_study_proj tree.├── manage.py└── tdd_study_proj ├── __init__.py ├── settings.py ├── urls.py └── wsgi.py1 directory, 5 files (유의: tree명령어는 Mac OS에서 HomeBrew를 통해 설치한 패키지다. 자신의 쉘에서 동작하지 않는다고 문제가 있는건 아니다.) 위 파일 구조를 보면 tdd_study_proj라는 큰 폴더(현재위치) 안에 manage.py파일과 현재위치 폴더이름과 같은 tdd_study_proj라는 프로젝트 폴더가 생겨있다. 이 상태에서 장고에 내장된 테스트 웹서버를 구동해 보자. 테스트용 웹서버는 runserver 라는 명령어로 실행할 수 있고, CTRL-C로 작동을 멈추게 할 수 있다.manage.py파일이 있는 곳에서 아래의 명령어를 쳐주자. 1$ python manage.py runserver 위 명령어를 치면 아래와 같이 테스트 서버가 http://127.0.0.1:8000 에서 실행되고 있다.(참고: 127.0.0.1 주소는 localhost와 동일합니다. 즉, 127.0.0.1:8000은 localhost:8000입니다.) 위 URL로 들어갔을 때 아래와 같은 화면이 나온다면 Django가 정상적으로 설치되었고, 테스트 웹서버도 정상적으로 구동중인 것이다. 3. 다시한번 테스트!Django서버가 켜져있는 상태로 둔 후, 새 쉘(혹은 cmd)창을 켜서 실습1. Selenium 이용해보기에서 만든 파일을 manage.py파일이 있는 폴더에 selenium_test.py라는 이름으로 만들어 주자. # selenium_test.py from selenium import webdriver browser = webdriver.Chrome('chromedriver') browser.get('http://localhost:8000') assert 'Django' in browser.title 이제는 에러가 나지 않고 테스트가 아무말(아무 에러)없이 끝나는걸 볼 수 있다 :)","link":"/2016/12/27/Django-TDD-Study-01-Setting-DevEnviron/"},{"title":"나만의 웹 크롤러 만들기(3): Selenium으로 무적 크롤러 만들기","text":"좀 더 보기 편한 깃북 버전의 나만의 웹 크롤러 만들기가 나왔습니다! Updated @ 2019.10.10. Typo/Layout fix, 네이버 로그인 Captcha관련 수정 추가 이전게시글: 나만의 웹 크롤러 만들기(2): Login with Session Selenium이란?Selenium은 주로 웹앱을 테스트하는데 이용하는 프레임워크다. webdriver라는 API를 통해 운영체제에 설치된 Chrome등의 브라우저를 제어하게 된다. 브라우저를 직접 동작시킨다는 것은 JavaScript를 이용해 비동기적으로 혹은 뒤늦게 불러와지는 컨텐츠들을 가져올 수 있다는 것이다. 즉, ‘눈에 보이는’ 컨텐츠라면 모두 가져올 수 있다는 뜻이다. 우리가 requests에서 사용했던 .text의 경우 브라우저에서 ‘소스보기’를 한 것과 같이 동작하여, JS등을 통해 동적으로 DOM이 변화한 이후의 HTML을 보여주지 않는다. 반면 Selenium은 실제 웹 브라우저가 동작하기 때문에 JS로 렌더링이 완료된 후의 DOM결과물에 접근이 가능하다. 어떻게 설치하나?pip selenium packageSelenium을 설치하는 것은 기본적으로 pip를 이용한다. 1pip install selenium 참고: Selenium의 버전은 자주 업데이트 되고, 브라우저의 업데이트 마다 새로운 Driver를 잡아주기 때문에 항상 최신버전을 깔아 주는 것이 좋다. 이번 튜토리얼에서는 BeautifulSoup이 설치되어있다고 가정합니다. BeautifulSoup은 pip install bs4로 설치 가능합니다. webdriverSelenium은 webdriver라는 것을 통해 디바이스에 설치된 브라우저들을 제어할 수 있다. 이번 가이드에서는 Chrome을 사용해 볼 예정이다. Chrome WebDriver크롬을 사용하려면 로컬에 크롬이 설치되어있어야 한다. 그리고 크롬 드라이버를 다운로드 받아주자. https://sites.google.com/a/chromium.org/chromedriver/downloads 글 수정일자인 2019년 10월 10일에는 크롬 77버전이 최신이며, 해당하는 크롬 드라이버를 받아야 한다. Update @ 2019.10.10 크롬에서는 현재 버전별 지정된 chromedriver를 받도록 안내하며, 버전에 일치하지 않는 드라이버를 사용하면 에러가 납니다. 현재 사용하는 크롬의 버전은 크롬 창에 👉 chrome://version 👈 이 URL을 주소창에 그대로 입력하면(http없이) 버전을 확인할 수 있습니다. 버전을 클릭하면 아래와 같은 OS별 Driver파일이 나열되어있다. 사용하는 OS에 따른 driver를 받아주자. zip파일을 받고 풀어주면 chromedriver라는 파일이 저장된다. 위 폴더를 기준으로 할 경우 /Users/beomi/Downloads/chromedriver가 크롬드라이버 파일의 위치가 된다. 이 경로를 나중에 Selenium 객체를 생성할 때 지정해 주어야 한다. (그래야 python이 chromedriver를 통해 크롬 브라우저를 조작할 수 있다!) PhantomJS webdriver 단, 2018년+ 기준 PhantomJS는 더이상 개발되지 않고 있기 때문에 앞으로는 크롬의 headless모드를 사용하는 것을 추천합니다. PhantomJS는 기본적으로 WebTesting을 위해 나온 Headless Browser다.(즉, 화면이 존재하지 않는다) 하지만 JS등의 처리를 온전하게 해주며 CLI환경에서도 사용이 가능하기 때문에, 만약 CLI서버 환경에서 돌아가는 크롤러라면 PhantomJS를 사용하는 것도 방법이다. PhantomJS는 공식 프로젝트의 PhantomJS Download Page에서 받을 수 있다. Binary 자체로 제공되기 때문에, Linux를 제외한 OS에서는 외부 dependency없이 바로 실행할 수 있다. 압축을 풀어주면 아래와 같은 많은 파일들이 있지만, 우리가 사용하는 것은 bin폴더 안의 phantomjs파일이다. 위 폴더 기준으로 할 경우 /Users/beomi/Downloads/phantomjs-2.1.1-macosx/bin/phantomjs가 PhantomJS드라이버의 위치다. Selenium으로 사이트 브라우징Selenium은 webdriver api를 통해 브라우저를 제어한다. 우선 webdriver를 import해주자. 1from selenium import webdriver 이제 driver라는 이름의 webdriver 객체를 만들어 주자. 이름이 꼭 driver일 필요는 없다. 이번 가이드에서는 크롬을 기본적으로 이용할 예정이다. 123456from selenium import webdriver# Chrome의 경우 | 아까 받은 chromedriver의 위치를 지정해준다.driver = webdriver.Chrome('/Users/beomi/Downloads/chromedriver')# PhantomJS의 경우 | 아까 받은 PhantomJS의 위치를 지정해준다.# driver = webdriver.PhantomJS('/Users/beomi/Downloads/phantomjs-2.1.1-macosx/bin/phantomjs') Selenium은 기본적으로 웹 자원들이 모두 로드될때까지 기다려주지만, 암묵적으로 모든 자원이 로드될때 까지 기다리게 하는 시간을 직접 implicitly_wait을 통해 지정할 수 있다. 12345from selenium import webdriverdriver = webdriver.Chrome('/Users/beomi/Downloads/chromedriver')# 암묵적으로 웹 자원 로드를 위해 3초까지 기다려 준다.driver.implicitly_wait(3) 이제 특정 url로 브라우저를 켜 보자. 123456from selenium import webdriverdriver = webdriver.Chrome('/Users/beomi/Downloads/chromedriver')driver.implicitly_wait(3)# url에 접근한다.driver.get('https://google.com') 만약 chromedriver의 위치가 정확하다면 새 크롬 화면이 뜨고 구글 첫 화면으로 들어가질 것이다. Selenium은 driver객체를 통해 여러가지 메소드를 제공한다. 아래의 메소드들은 보통 driver.~~~ 방식으로 사용합니다. URL에 접근하는 메소드, get('http://url.com') 페이지의 단일 element에 접근하는 메소드, find_element_by_name('HTML_name') find_element_by_id('HTML_id') find_element_by_xpath('/html/body/some/xpath') find_element_by_css_selector('#css &gt; div.selector') find_element_by_class_name('some_class_name') find_element_by_tag_name('h1') 페이지의 여러 elements에 접근하는 메소드 등이 있다. (대부분 element 를 elements 로 바꾸기만 하면 된다.) find_elements_by_css_selector('#css &gt; div.selector') 위 메소드들을 활용시 HTML을 브라우저에서 파싱해주기 때문에 굳이 Python와 BeautifulSoup을 사용하지 않아도 된다. 하지만 Selenium에 내장된 함수만 사용가능하기 때문에 좀더 사용이 편리한 soup객체를 이용하려면 driver.page_source API를 이용해 현재 렌더링 된 페이지의 Elements를 모두 가져올 수 있다. driver.page_source: 브라우저에 보이는 그대로의 HTML, 크롬 개발자 도구의 Element 탭 내용과 동일. requests 통해 가져온 req.text: HTTP요청 결과로 받아온 HTML, 크롬 개발자 도구의 페이지 소스 내용과 동일. 위 2개는 사이트에 따라 같을수도 다를수도 있습니다. 네이버 로그인 하기네이버는 requests를 이용해 로그인하는 것이 어렵다. 프론트 단에서 JS처리를 통해 로그인 처리를 하기 때문인데, Selenium을 이용하면 보다 쉽게 로그인을 할 수 있다. 123456from selenium import webdriverdriver = webdriver.Chrome('/Users/beomi/Downloads/chromedriver')driver.implicitly_wait(3)# url에 접근한다.driver.get('https://nid.naver.com/nidlogin.login') 네이버 로그인 화면을 확인 해 보면 아이디를 입력받는 부분의 name이 id, ​비밀번호를 입력받는 부분의 name이 pw인 것을 알 수 있다. find_element_by_name을 통해 아이디/비밀번호 input 태그를 잡아주고, 값을 입력해 보자. 12345678from selenium import webdriverdriver = webdriver.Chrome('/Users/beomi/Downloads/chromedriver')driver.implicitly_wait(3)driver.get('https://nid.naver.com/nidlogin.login')# 아이디/비밀번호를 입력해준다.driver.find_element_by_name('id').send_keys('naver_id')driver.find_element_by_name('pw').send_keys('mypassword1234') 성공적으로 값이 입력된 것을 확인할 수 있다. 이제 Login버튼을 눌러 실제로 로그인이 되는지 확인해 보자. 123456789from selenium import webdriverdriver = webdriver.Chrome('/Users/beomi/Downloads/chromedriver')driver.implicitly_wait(3)driver.get('https://nid.naver.com/nidlogin.login')driver.find_element_by_name('id').send_keys('naver_id')driver.find_element_by_name('pw').send_keys('mypassword1234')# 로그인 버튼을 눌러주자.driver.find_element_by_xpath('//*[@id=\"frmNIDLogin\"]/fieldset/input').click() Update @ 2019.10.10. 하지만 로그인이 이전처럼 잘 되지 않고 Captcha를 요구하는 창이 뜰 수 있다. 네이버에서 성공적으로 로그인이 되는 것을 확인할 수 있다. 로그인이 필요한 페이지인 네이버 페이의 주문내역 페이지를 가져와보자. 네이버 페이의 Url은 https://order.pay.naver.com/home 이다. 위 페이지의 알림 텍스트를 가져와 보자. 123456789101112131415161718from selenium import webdriverfrom bs4 import BeautifulSoupdriver = webdriver.Chrome('/Users/beomi/Downloads/chromedriver')driver.implicitly_wait(3)driver.get('https://nid.naver.com/nidlogin.login')driver.find_element_by_name('id').send_keys('naver_id')driver.find_element_by_name('pw').send_keys('mypassword1234')driver.find_element_by_xpath('//*[@id=\"frmNIDLogin\"]/fieldset/input').click()# Naver 페이 들어가기driver.get('https://order.pay.naver.com/home')html = driver.page_sourcesoup = BeautifulSoup(html, 'html.parser')notices = soup.select('div.p_inr &gt; div.p_info &gt; a &gt; span')for n in notices: print(n.text.strip()) 로그인이 잘 되고, 성공적으로 리스트를 받아오는 것을 확인해 볼 수 있다. 정리하기Selenium은 웹 테스트 자동화 도구이지만, 멋진 크롤링 도구로 사용할 수 있다. 또한, BeautifulSoup와 함께 사용도 가능하기 때문에 크롤링을 하는데 제약도 줄어 훨씬 쉽게 크롤링을 할 수 있다. 1234567891011121314151617181920from selenium import webdriverfrom bs4 import BeautifulSoup# setup Driver|Chrome : 크롬드라이버를 사용하는 driver 생성driver = webdriver.Chrome('/Users/beomi/Downloads/chromedriver')driver.implicitly_wait(3) # 암묵적으로 웹 자원을 (최대) 3초 기다리기# Logindriver.get('https://nid.naver.com/nidlogin.login') # 네이버 로그인 URL로 이동하기driver.find_element_by_name('id').send_keys('naver_id') # 값 입력driver.find_element_by_name('pw').send_keys('mypassword1234')driver.find_element_by_xpath( '//*[@id=\"frmNIDLogin\"]/fieldset/input').click() # 버튼클릭하기driver.get('https://order.pay.naver.com/home') # Naver 페이 들어가기html = driver.page_source # 페이지의 elements모두 가져오기soup = BeautifulSoup(html, 'html.parser') # BeautifulSoup사용하기notices = soup.select('div.p_inr &gt; div.p_info &gt; a &gt; span')for n in notices: print(n.text.strip()) 다음 가이드Selenium으로 많은 사이트에서 여러 정보를 가져와 볼 수 있게 되었습니다. 하지만 가져온 데이터를 DB에 저장하려면 약간의 어려움이 따르게 됩니다. 다음 시간에는 Django의 ORM을 이용해 sqlite3 DB에 데이터를 저장해보는 방법에 대해 알아보겠습니다. 다음 가이드: 나만의 웹 크롤러 만들기(4): Django로 크롤링한 데이터 저장하기","link":"/2017/02/27/HowToMakeWebCrawler-With-Selenium/"},{"title":"[번역]셀러리: 시작하기","text":"글 작성 시점 최신 버전 v4.0.2의 문서입니다. 원문: http://docs.celeryproject.org/en/latest/getting-started/index.html 셀러리: 시작하기출시버전: v4.0.2 출시일: 2017. 03. 15. 번역일: 2017. 03. 19. ~ 셀러리 입문하기 태스크 큐란 무엇인가? (What’s a Task Queue?) 뭐가 필요한가요? (What do I need?) 시작하기 (Get Started) 셀러리는.. (Celery is..) 셀러리의 기능들 (Features) 프레임워크와 함께 이용하기 (Framework Integration) 빠르게 찾아보기 (Quick Jump) 셀러리 설치하기 (Installation) 브로커 (Brokers) 브로커 가이드 (Broker Instructions) RabbitMQ 사용하기 (Using RabbitMQ) Redis 사용하기 (Using Redis) Amazon SQS 사용하기 (Using Amazon SQS) 브로커간 기능 (Broker Overview) 셀러리 한 발자국 내밀기 (First Steps with Celery) 브로커 선택하기 (Choosing a Broker) 셀러리 설치하기 (Installing Celery) 앱 (Application) 셀러리 워커 서버 띄우기 (Running the Celery worker server) 태스크 호출하기 (Calling the task) 결과 유지하기 (Keeping Results) 설정하기 (Configuration) 더 나아가기 (Where to go from here) 문제 해결하기 (Troubleshooting) 더 알아보기 (Next Steps) 셀러리를 기존 앱에서 사용하기 (Using Celery in your Application) 태스크 호출하기 (Calling Tasks) Canvas: 워크플로 디자인하기 (Canvas: Designing Work-flows) 라우팅 (Routing) 원격 제어 (Remote Control) 타임존 (Timezone) 최적화 (Optimization) 더, 더, 더. (What to do now?)","link":"/2017/03/19/Celery-Getting-Started/"},{"title":"[번역]셀러리 입문하기","text":"글 작성 시점 최신 버전 v4.0.2의 문서입니다. 원문: http://docs.celeryproject.org/en/latest/getting-started/introduction.html 셀러리 입문하기widgets: 태스크 큐란 무엇인가? (What’s a Task Queue?) 뭐가 필요한가요? (What do I need?) 시작하기 (Get Started) 셀러리는.. (Celery is..) 셀러리의 기능들 (Features) 프레임워크와 함께 이용하기 (Framework Integration) 셀러리 설치하기 (Installation) 태스크 큐란 무엇인가?태스크 큐는 스레드간 혹은 기계 간 업무를 분산하는 목적으로 만들어진 메커니즘입니다. 태스크 큐에 들어가는 일거리들은 태스크(Task)라고 불리고 각각 독립된 워커(Worker)프로세스들은 새로운 일거리(Task)가 없는지 지속적으로 태스크 큐를 감시합니다. 셀러리는 메시지(message)를 통해 통신하는데요, 보통 브로커(Broker)가 클라이언트와 워커 사이에서 메시지를 중계해줍니다. 브로커는 클라이언트가 큐에 새로 추가한 태스크를 메시지로 워커에 전달해줍니다. 셀러리 시스템에서는 여러개의 워커와 브로커를 함께 사용할 수 있는데요, 이 덕분에 높은 가용성과 Scaling이 가능합니다. 셀러리는 Python으로 짜여져 있습니다. 하지만 어떤 언어를 통해서도 프로토콜을 통해 셀러리를 사용할 수 있습니다. 예를들어, Node나 PHP를 위한 node-celery나 PHP client도 있답니다. 또, HTTP endpoint를 통해 webhook으로 태스크를 요청하는 것도 가능하답니다. 뭐가 필요한가요?셀러리를 사용하려면 메시지를 주고받아주는 브로커 등이 필요합니다. RabbitMQ나 Redis같은 브로커가 가장 좋은 선택이지만, 개발환경에서는 Sqlite와 같이 수많은 실험적인 대안도 있습니다. 셀러리는 단일 머신, 복수 머신, 혹은 데이터센터간에서도 사용 가능합니다. 시작하기만약 여러분이 셀러리를 처음 이용하시거나 3.1버전 같은 이전 버전을 이용했다면, 셀러리 한 발자국 내밀기나 더 알아보기 튜토리얼을 먼저 해보세요. 버전 확인 셀러리 4.0은 Python2.7/3.4/3.5 PyPy5.4/5.5에서 정상적으로 동작합니다. 셀러리는.. 단순해요! 셀러리는 사용하기도 쉽고 관리하기도 쉽습니다. 설정 파일도 필요하지 않아요! 가장 단순한 셀러리 앱은 아래와 같이 만들 수 있어요. 1234567from celery import Celeryapp = Celery('hello', broker='amqp://guest@localhost//')@app.taskdef hello(): return 'hello world' 높은 가용성 워커와 클라이언트는 연결이 끊어지거나 실패하면 자동으로 다시 연결을 시도합니다. 그리고 몇몇 브로커들은 Primary/Primary나 Primary/Replica 의 복제방식을 통해 고가용성을 제공합니다. 빨라요! 하나의 셀러리 프로세스는 1분에 수십만개의 태스크를 처리할 수 있고, ms초 이하의 RTT(왕복지연시간)로 태스크를 처리 가능하답니다. (RabbitMQ, librabbitmq와 최적화된 설정을 할 경우) 유연해요! 셀러리의 대부분 파트는 그 자체로 이용할 수도 있고 원하는 만큼 확장도 가능합니다. Custom pool implementations, serializers, compression schemes, logging, schedulers, consumers, producers, broker transports을 포함해 더 많이요. 셀러리의 기능들 모니터링 모니터링 이벤트 스트림은 각 워커에서 나오고, 클러스터에서 수행하는 작업을 실시간으로 알려줍니다. 더 알아보기.. 워크 플로우 간단하거나 복잡한 워크플로우를 “캔버스”라는 도구를 통해 그룹핑, 체이닝, 청킹등을 할 수 있습니다. 더 알아보기.. 시간 / 비율 제한 태스크가 시간당 얼마나 실행 될지 제어할 수 있고, 한 태스크가 얼마나 오랫동안 실행될지 허용할 수 있습니다. 각각의 워커마다 다르게 설정하거나 각각의 태스크마다도 다르게 설정할 수 있답니다. 더 알아보기.. 스케쥴링 어떤 태스크를 정해진 시간에 실행할 수 있습니다. 또, 정해진 주기로 태스크를 실행 할 수도 있습니다. Crontab에서 사용하는 방식(분/시간/요일등등)을 이용할 수도 있습니다. 더 알아보기.. 사용자 컴포넌트 각각의 워커 컴포넌트들을 커스터마이징해 사용할 수 있습니다. 그리고 추가적인 컴포넌트도 커스터마이징해 사용할 수 있습니다. 워커는 내부 구조를 세밀하게 제어할수있는 종속성 그래프인 “bootsteps”를 사용하여 빌드됩니다. 프레임워크와 함께 이용하기셀러리는 웹 프레임 워크와 쉽게 함께 사용할 수 있고, 일부는 합쳐진 패키지도 있습니다. Pyramid: pyramid_celery Pylons: celery-pylons Flask: 필요없음 Web2Py: web2py-celery Tornado: tornado-celery Django의 경우에는 장고와 함께하는 셀러리 첫걸음을 참고하세요. 통합 패키지가 굳이 필요하지는 않지만 개발을 더 쉽게 해주고 DB 커넥션 등에서 중요한 hook를 추가하기도 하기때문에 이용하는 편이 낫습니다. 설치하기셀러리는 PyPI를 통해 쉽게 설치할 수 있습니다. 1pip install -U pip","link":"/2017/03/19/Introduction-to-Celery/"},{"title":"React+JSX(ES6)를 빌드 없이 사용하기: browser.js","text":"Babel: ES6를 ES5로바벨(Babel)은 ES6(ECMAScript6)을 ES5 문법으로 변환시켜 오래된 브라우저들에서도 ES6의 기능을 이용할 수 있도록 도와주는 자바스크립트 모듈이다. React는 개발시 ES6 문법을 주로 이용하기 때문에 이러한 Babel은 필수적이라고 말할 수 있다. 그러나…React에서 공식적인 이용 방법 중 하나인 CDN을 이용할 경우 (아래 사진처럼) 실제 첫 튜토리얼을 할 경우 JavaScript에서 JSX를 공식적으로 지원하지 않기 때문에 JS문법 에러가 난다. 아래 코드는 작동하지 않는다. 1234ReactDOM.render( &lt;h1&gt;Hello, world!&lt;/h1&gt;, document.getElementById('root')); 물론 실제 배포시에는 빌드 과정을 거쳐 나온 파일을 관리해야 한다. 하지만 React를 처음 배우는 과정에서는 로컬의 한 HTML파일 안에서 모든 과정이 작동하기를 원하게 된다. 따라서 클라이언트 렌더링을 고려할 수 있다. 물론 클라이언트 렌더링은 성능 이슈가 있기 때문에 실 배포시에는 사용하지 않아야 한다. Browser.js 사용하기Babel은 6버전부터 Browser.js를 업데이트 하지 않았다. 하지만 정상적으로 동작하는 파일이 CDN에 존재하기 때문에, HTML문서에 다음 세 줄을 추가해 주면 script태그에 type=&quot;text/babel&quot;이라는 타입을 가진 코드들을 ES6로 간주하고 ES5로 변환해 준다. 123&lt;script src=\"https://unpkg.com/react@15/dist/react.js\"&gt;&lt;/script&gt;&lt;script src=\"https://unpkg.com/react-dom@15/dist/react-dom.js\"&gt;&lt;/script&gt;&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/babel-core/5.8.34/browser.js\"&gt;&lt;/script&gt; Browser.js파일을 추가해서 우리는 위 코드를 아래와 같이 쓸 수 있게 된다. 123456789101112&lt;script src=\"https://unpkg.com/react@15/dist/react.js\"&gt;&lt;/script&gt;&lt;script src=\"https://unpkg.com/react-dom@15/dist/react-dom.js\"&gt;&lt;/script&gt;&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/babel-core/5.8.34/browser.js\"&gt;&lt;/script&gt;&lt;div id=\"root\"&gt;&lt;/div&gt;&lt;script type=\"text/babel\"&gt; ReactDOM.render( &lt;h1&gt;Hello, world!&lt;/h1&gt;, document.getElementById('root') );&lt;/script&gt; 위 코드를 살펴보면 정상적으로 동작한다는 것을 알 수 있다. 입문/개발 전용!!단, 이 방법은 React에 입문하는 사람이 HTML 문서 하나만으로, 그리고 NPM을 사용하지 않고 작업할 경우에 사용할 수 있는 방법이며, 실 프로젝트에서 이와 같이 사용하는 것은 여러 문제를 일으킬 수 있다. 그러니 입문/개발때에만 사용하자 :)","link":"/2017/03/20/Using-ES6-JSX-on-Client/"},{"title":"Fabric으로 Django 배포하기","text":"이번 가이드는 완성된 상태의 Django 프로젝트가 있다고 가정합니다. 예제로 https://github.com/Beomi/irkshop 을 배포해 봅니다. https://gist.github.com/Beomi/945cd905175c3b21370f8f04abd57404의 예제를 설명합니다. Fabric으로 Django 배포하기Django는 내장된 runserver라는 개발용 웹 서버가 있습니다. 하지만 개발용 웹 서버를 상용 환경에서 사용하는 것은 여러가지 문제를 가져옵니다. 메모리 문제등의 성능 이슈부터 Static file서빙의 보안 문제까지 다양한데요, 이 때문에 Django는 웹 서버(ex: Apache2 NginX등)를 통해 배포하게 됩니다. 하지만 이러한 배포작업은 아마존 EC2등의 VPS나 리얼 서버에서 Apache2를 깔고, python3와 mod_wsgi등을 깔아야만 동작하기 때문에 배포 자체가 어려움을 갖게 됩니다. 또한 SSH에 접속히 직접 명령어를 치는 경우 오타나 실수등으로 인해 정상적으로 작동하지 않는 경우도 부지기수입니다. 따라서 이러한 작업을 자동화해주는 도구가 바로 Fabric이고, 이번 가이드에서는 Django 프로젝트를 Vultr VPS, Ubuntu에 올리는 방법을 다룹니다. Vultr VPS 생성하기Vultr는 VPS(가상서버) 제공 회사입니다. 최근 가격 인하로 유사 서비스 대비 절반 가격에 이용할 수 있어 가성비가 좋습니다. 사용자가 많지 않은 (혹은 혼자 사용하는..) 서비스라면 최소 가격인 1cpu 512MB의 월 2.5달러짜리를 이용하시면 됩니다. Vultr는 일본 Region에 서버가 있어 한국에서 사용하기에도 핑이 25ms정도로 양호합니다. VPS하나를 만든 후 root로 접속해 장고를 구동할 사용자를 만들어 봅시다. django 유저 만들기(sudo권한 가진 유저 만들기)Fabric을 사용할 때 초기에 apt를 이용해 패키지를 설치해야 할 필요가 있습니다. 하지만 처음에 제공되는 root계정은 사용하지 않는 것을 보안상 추천합니다. 따라서 우리는 sudo권한을 가진 django라는 유저를 생성하고 Fabric으로 진행해 보겠습니다. 12adduser django # `django`라는 유저를 만듭니다.adduser django sudo # django유저를 `sudo`그룹에 추가합니다. 비밀번호를 만드는 것을 제외하면 나머지는 빈칸으로 만들어 두어도 무방합니다. Fabric 설치하기Fabric은 기본적으로 서버가 아닌 클라이언트에 설치합니다. 개념상 로컬에서 SSH로 서버에 접속해 명령을 처리하는 것이기 때문에 당연히 SSH 명령을 입려하는 로컬에 설치되어야 합니다. Fabric은 공식적으로는 Python2.7만을 지원합니다. 하지만 이 프로젝트를 Fork해서 Python3을 지원하는 프로젝트인 Fabric3이 있습니다. 이번 가이드에서는 이 Fabric3을 설치합니다. 123pip3 install fabric3# 혹은python3 -m pip install fabric3 fabfile 만들기Fabric import하기Fabric을 설치하시면 fab이라는 명령어를 사용할 수 있습니다. 이 명령어는 fab some_func라는 방식을 통해 fabfile.py파일 안의 함수를 실행할 수 있습니다. fabfile은 기본적으로 manage.py파일와 같은 위치인 프로젝트 폴더에 두시는 것을 권장합니다. 12from fabric.contrib.files import append, exists, sed, putfrom fabric.api import env, local, run, sudo 우선 fabric에서 사용하는 API들을 import해줍니다. fabric.contrib.files에서는 원격(혹은 로컬)의 파일을 관리하는 API입니다. fabric.api는 Fabric에서 사용하는 환경이나, SSH로 연결한 원격 서버에서 명령어를 실행하는 API입니다. PROJECT_DIR, BASE_DIR 지정하기장고의 settings.py파일에 기본적으로 지정된 BASE_DIR와 같은 장고 프로젝트의 폴더 위치를 지정하는 PROJECT_DIR와 BASE_DIR을 지정해 줍니다. PROJECT_DIR은 settings.py가 있는 폴더의 위치이고, BASE_DIR은 manage.py가 있는 폴더입니다. 12345678from fabric.contrib.files import append, exists, sed, putfrom fabric.api import env, local, run, sudoimport randomimport osimport jsonPROJECT_DIR = os.path.dirname(os.path.abspath(__file__))BASE_DIR = os.path.dirname(PROJECT_DIR) 배포용 변수 불러오기서버에 배포를 하기 위해 git을 이용합니다. 따라서 소스가 올라가 있는 깃헙(혹은 gitlab, bitbucket 등)의 주소(REPO_URL)가 필요합니다. 그리고 원격으로 SSH접속을 하기 때문에 원격 서버에 접속할 수 있는 SSH용 주소(REMOTE_HOST_SSH), 원격 계정 ID(REMOTE_USER), 원격 계정 비밀번호(REMOTE_PASSWORD)가 필요합니다. 또한, 장고 settings.py의 ALLOWED_HOSTS에 추가할 도메인(REMOTE_HOST)이 필요합니다. 이러한 변수들은 보통 json파일에 저장하고 .gitignore에 이 json파일을 지정해 git에 올라가지 않도록 관리합니다. 이번 가이드에서는 deploy.json이라는 파일에 아래 변수들을 저장하고 관리해 보겠습니다. deploy.json파일을 fabfile.py파일이 있는 곳에 아래 내용을 담고 저장해주세요. REPO_URL와 PROJECT_NAME을 제외한 설정은 위 Vultr에서 만들어준 대로 진행해주세요. 단, REMOTE_USER는 root이면 안됩니다! 12345678{ \"REPO_URL\":\"https://github.com/Beomi/irkshop.git\", \"PROJECT_NAME\":\"irkshop\", \"REMOTE_HOST_SSH\":\"45.77.20.73\", \"REMOTE_HOST\":\"45.77.20.73\", \"REMOTE_USER\":\"django\", \"REMOTE_PASSWORD\":\"django_pwd123\"} 만약 SSH 포트가 다르다면 REMOTE_HOST_SSH 뒤 포트를 :으로 붙여주면 됩니다. (ex: 45.77.20.73:22) REMOTE_HOST는 도메인 주소(ex: irkshop.testi.kr)일 수 있습니다. 하지만 이번 배포에서는 도메인을 다루지 않으므로 IP주소로 대신합니다. json파일을 만들었다면 이 파일을 이제 fabfile.py에서 불러와 사용해 봅시다. 1234567891011121314151617181920212223from fabric.contrib.files import append, exists, sed, putfrom fabric.api import env, local, run, sudoimport randomimport osimport jsonPROJECT_DIR = os.path.dirname(os.path.abspath(__file__))BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))# deploy.json파일을 불러와 envs변수에 저장합니다.with open(os.path.join(PROJECT_DIR, \"deploy.json\")) as f: envs = json.loads(f.read())REPO_URL = envs['REPO_URL']PROJECT_NAME = envs['PROJECT_NAME']REMOTE_HOST_SSH = envs['REMOTE_HOST_SSH']REMOTE_HOST = envs['REMOTE_HOST']REMOTE_USER = envs['REMOTE_USER']REMOTE_PASSWORD = envs['REMOTE_PASSWORD']# 아래 부분은 Django의 settings.py에서 지정한 STATIC_ROOT 폴더 이름, STATID_URL, MEDIA_ROOT 폴더 이름을 입력해주시면 됩니다.STATIC_ROOT_NAME = 'static_deploy'STATIC_URL_NAME = 'static'MEDIA_ROOT = 'uploads' Fabric 환경 설정하기이제 Fabric이 사용할 env를 설정해 줍시다. 대표적으로 env.user와 env.hosts, env.password가 있습니다. 12345678910111213141516171819202122232425262728293031from fabric.contrib.files import append, exists, sed, putfrom fabric.api import env, local, run, sudoimport randomimport osimport jsonPROJECT_DIR = os.path.dirname(os.path.abspath(__file__))BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))with open(os.path.join(PROJECT_DIR, \"deploy.json\")) as f: envs = json.loads(f.read())REPO_URL = envs['REPO_URL']PROJECT_NAME = envs['PROJECT_NAME']REMOTE_HOST_SSH = envs['REMOTE_HOST_SSH']REMOTE_HOST = envs['REMOTE_HOST']REMOTE_USER = envs['REMOTE_USER']REMOTE_PASSWORD = envs['REMOTE_PASSWORD']STATIC_ROOT_NAME = 'static_deploy'STATIC_URL_NAME = 'static'MEDIA_ROOT = 'uploads'# Fabric이 사용하는 env에 값들을 저장합니다.env.user = REMOTE_USERusername = env.userenv.hosts = [ REMOTE_HOST_SSH, # 리스트로 만들어야 합니다. ]env.password = REMOTE_PASSWORD# 원격 서버에서 장고 프로젝트가 있는 위치를 정해줍니다.project_folder = '/home/{}/{}'.format(env.user, PROJECT_NAME) 이와 같이 설정시 fab명령어를 실행할 경우에 추가적인 값을 입력할 필요가 없어집니다. APT 설치 목록 지정하기VPS에 따라 설치되어있는 리눅스 패키지가 다릅니다. 이번 가이드에서는 Apache2와 mod-wsgi-py3을 사용하기 때문에 이 패키지와 파이썬 의존 패키지들을 설치해 줘야 합니다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445from fabric.contrib.files import append, exists, sed, putfrom fabric.api import env, local, run, sudoimport randomimport osimport jsonPROJECT_DIR = os.path.dirname(os.path.abspath(__file__))BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))with open(os.path.join(PROJECT_DIR, \"deploy.json\")) as f: envs = json.loads(f.read())REPO_URL = envs['REPO_URL']PROJECT_NAME = envs['PROJECT_NAME']REMOTE_HOST_SSH = envs['REMOTE_HOST_SSH']REMOTE_HOST = envs['REMOTE_HOST']REMOTE_USER = envs['REMOTE_USER']REMOTE_PASSWORD = envs['REMOTE_PASSWORD']STATIC_ROOT_NAME = 'static_deploy'STATIC_URL_NAME = 'static'MEDIA_ROOT = 'uploads'env.user = REMOTE_USERusername = env.userenv.hosts = [REMOTE_HOST_SSH,]env.password = REMOTE_PASSWORDproject_folder = '/home/{}/{}'.format(env.user, PROJECT_NAME)# APT로 설치할 목록을 정해줍니다.apt_requirements = [ 'ufw', # 방화벽 'curl', 'git', # 깃 'python3-dev', # Python 의존성 'python3-pip', # PIP 'build-essential', # C컴파일 패키지 'python3-setuptools', # PIP 'apache2', # 웹서버 Apache2 'libapache2-mod-wsgi-py3', # 웹서버~Python3 연결 # 'libmysqlclient-dev', # MySql 'libssl-dev', # SSL 'libxml2-dev', # XML 'libjpeg8-dev', # Pillow 의존성 패키지(ImageField) 'zlib1g-dev', # Pillow 의존성 패키지] Fab 함수 만들기Fabric은 fabfile있는 곳에서 fab 함수이름의 명령어로 실행 가능합니다. 단, _로 시작하는 함수는 Fabric이 관리하지 않습니다. 이제 서버에서 실행할 SSH를 캡슐화한다고 보면 됩니다. 크게 setup와 deploy로 나눌 수 있다고 봅니다. Setup은 장고 코드와 무관한 OS의 패키지 관리와 VirtualEnv관리, Deploy는 장고 소스가 변화할 경우 업데이트 되어야 하는 코드입니다. Setup에는 APT 최신 패키지 설치와 apt_requirements설치, 그리고 virtualenv를 만드는 것까지를 다룹니다. Deploy에서는 Git에서 최신 소스코드를 가져오고, Git에서 관리되지 않는 환경변수 파일을 서버에 업로드하고, 장고 settings.py파일을 상용 환경으로 바꿔주고, virtualenv로 만든 가상환경에 pip 패키지를 설치하고, StaticFile들을 collect하고, DB를 migrate해주고, Apache2의 VirtualHost에 장고 웹 서비스를 등록하고, 폴더 권한을 잡아주고, 마지막으로 Apache2 웹서버를 재부팅하는 과정까지를 다룹니다. 여기서부터는 코드가 너무 길어지는 관계로 apt_requirements 포함한 윗부분을 생략합니다. 123456789101112131415161718192021# 앞부분 생략def new_server(): setup() deploy()def setup(): _get_latest_apt() # APT update/upgrade _install_apt_requirements(apt_requirements) # APT install _make_virtualenv() # Virtualenvdef deploy(): _get_latest_source() # Git에서 최신 소스 가져오기 _put_envs() # 환경변수 json파일 업로드 _update_settings() # settings.py파일 변경 _update_virtualenv() # pip 설치 _update_static_files() # collectstatics _update_database() # migrate _make_virtualhost() # Apache2 VirtualHost _grant_apache2() # chmod _grant_sqlite3() # chmod _restart_apache2() # 웹서버 재시작 이와 같이 함수를 등록해주면 fab new_server, fab setup, fab deploy를 통해 바로바로 배포를 할 수 있습니다. 이제 _로 시작하는, 진짜 Fabric함수들을 만들어 보겠습니다. _ 로 시작하는 함수들을 설명할 때는 함수만 각각 설명합니다. 모두 모인 코드는 글 상단의 GIST를 참고해주세요. _get_latest_apt: APT 업데이트 &amp; 업그레이드 1234def _get_latest_apt(): update_or_not = input('would you update?: [y/n]') if update_or_not=='y': sudo('sudo apt-get update &amp;&amp; sudo apt-get -y upgrade') _install_apt_requirements: apt_requirements에 적은 패키지들을 설치합니다. 12345def _install_apt_requirements(apt_requirements): reqs = '' for req in apt_requirements: reqs += (' ' + req) sudo('sudo apt-get -y install {}'.format(reqs)) _make_virtualenv: 원격 서버에 ~/.virtualenvs폴더가 없는 경우 virtualenv와 virtualenvwrapper를 설치하고 .bashrc파일에 virtualenvwrapper를 등록해 줍니다. 123456789def _make_virtualenv(): if not exists('~/.virtualenvs'): script = '''\"# python virtualenv settings export WORKON_HOME=~/.virtualenvs export VIRTUALENVWRAPPER_PYTHON=\"$(command \\which python3)\" # location of python3 source /usr/local/bin/virtualenvwrapper.sh\"''' run('mkdir ~/.virtualenvs') sudo('sudo pip3 install virtualenv virtualenvwrapper') run('echo {} &gt;&gt; ~/.bashrc'.format(script)) _get_latest_source: 만약 .git폴더가 없다면 원격 git repo에서 clone해오고, 있다면 fetch해온 후 최신 커밋으로 reset합니다. 1234567def _get_latest_source(): if exists(project_folder + '/.git'): run('cd %s &amp;&amp; git fetch' % (project_folder,)) else: run('git clone %s %s' % (REPO_URL, project_folder)) current_commit = local(\"git log -n 1 --format=%H\", capture=True) run('cd %s &amp;&amp; git reset --hard %s' % (project_folder, current_commit)) _put_envs: 로컬의 envs.json이름의 환경변수를 서버에 업로드 합니다. Apache2는 웹서버가 OS의 환경변수를 사용하지 않기 때문에 json와 같은 파일을 통해 환경변수를 관리해 줘야 합니다. 저는 envs.json이라는 파일을 manage.py파일이 있는 프로젝트 폴더에 만든 후 환경변수를 장고의 settings.py에서 불러와 사용합니다. 12def _put_envs(): put(os.path.join(PROJECT_DIR, 'envs.json'), '~/{}/envs.json'.format(PROJECT_NAME)) _update_settings: settings.py파일을 바꿔줍니다. DEBUG를 False로 바꾸고, ALLOWED_HOSTS에 호스트 이름을 등록하고, 장고에서 만들어준 키 파일이 아니라 서버에서 랜덤으로 만든 Secret KEY를 사용하게 합니다. 12345678910111213def _update_settings(): settings_path = project_folder + '/{}/settings.py'.format(PROJECT_NAME) sed(settings_path, \"DEBUG = True\", \"DEBUG = False\") sed(settings_path, 'ALLOWED_HOSTS = .+$', 'ALLOWED_HOSTS = [\"%s\"]' % (REMOTE_HOST,) ) secret_key_file = project_folder + '/{}/secret_key.py'.format(PROJECT_NAME) if not exists(secret_key_file): chars = 'abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&amp;*(-_=+)' key = ''.join(random.SystemRandom().choice(chars) for _ in range(50)) append(secret_key_file, \"SECRET_KEY = '%s'\" % (key,)) append(settings_path, '\\nfrom .secret_key import SECRET_KEY') _update_virtualenv: virtualenv에 requirements.txt파일로 지정된 pip 패키지들을 설치합니다. 1234567def _update_virtualenv(): virtualenv_folder = project_folder + '/../.virtualenvs/{}'.format(PROJECT_NAME) if not exists(virtualenv_folder + '/bin/pip'): run('cd /home/%s/.virtualenvs &amp;&amp; virtualenv %s' % (env.user, PROJECT_NAME)) run('%s/bin/pip install -r %s/requirements.txt' % ( virtualenv_folder, project_folder )) _update_static_files: CollectStatic을 해줍니다. 12345def _update_static_files(): virtualenv_folder = project_folder + '/../.virtualenvs/{}'.format(PROJECT_NAME) run('cd %s &amp;&amp; %s/bin/python3 manage.py collectstatic --noinput' % ( project_folder, virtualenv_folder )) _update_database: DB migrate를 해줍니다. 12345def _update_database(): virtualenv_folder = project_folder + '/../.virtualenvs/{}'.format(PROJECT_NAME) run('cd %s &amp;&amp; %s/bin/python3 manage.py migrate --noinput' % ( project_folder, virtualenv_folder )) _make_virtualhost: Apache2가 관리하는 VirtualHost를 만들어줍니다. 80포트로 지정하고 Static파일을 Apache2가 서빙합니다. 만약 SSL을 사용하고 싶으시다면 *:443으로 관리되는 파일을 추가적으로 만드셔야 합니다. 이번 가이드에서는 다루지 않습니다. 123456789101112131415161718192021222324252627282930313233def _make_virtualhost(): script = \"\"\"'&lt;VirtualHost *:80&gt; ServerName {servername} Alias /{static_url} /home/{username}/{project_name}/{static_root} Alias /{media_url} /home/{username}/{project_name}/{media_url} &lt;Directory /home/{username}/{project_name}/{media_url}&gt; Require all granted &lt;/Directory&gt; &lt;Directory /home/{username}/{project_name}/{static_root}&gt; Require all granted &lt;/Directory&gt; &lt;Directory /home/{username}/{project_name}/{project_name}&gt; &lt;Files wsgi.py&gt; Require all granted &lt;/Files&gt; &lt;/Directory&gt; WSGIDaemonProcess {project_name} python-home=/home/{username}/.virtualenvs/{project_name} python-path=/home/{username}/{project_name} WSGIProcessGroup {project_name} WSGIScriptAlias / /home/{username}/{project_name}/{project_name}/wsgi.py {% raw %} ErrorLog ${{APACHE_LOG_DIR}}/error.log CustomLog ${{APACHE_LOG_DIR}}/access.log combined {% endraw %} &lt;/VirtualHost&gt;'\"\"\".format( static_root=STATIC_ROOT_NAME, username=env.user, project_name=PROJECT_NAME, static_url=STATIC_URL_NAME, servername=REMOTE_HOST, media_url=MEDIA_ROOT ) sudo('echo {} &gt; /etc/apache2/sites-available/{}.conf'.format(script, PROJECT_NAME)) sudo('a2ensite {}.conf'.format(PROJECT_NAME)) _grant_apache2: 프로젝트 폴더내 파일을 www-data그룹(Apache2)이 관리할 수 있도록 소유권을 변경합니다. 12def _grant_apache2(): sudo('sudo chown -R :www-data ~/{}'.format(PROJECT_NAME)) _grant_sqlite3: 만약 Sqlite3을 그대로 이용할 경우 www-data가 쓰기 권한을 가져야 합니다. 12def _grant_sqlite3(): sudo('sudo chmod 775 ~/{}/db.sqlite3'.format(PROJECT_NAME)) _restart_apache2: 모든 설정을 마친 후 Apache2 웹서버를 재실행해 설정을 적용해줍니다. 12def _restart_apache2(): sudo('sudo service apache2 restart') 배포해보기이제 manage.py파일이 있는 곳에서 아래 명령어를 입력해 봅시다. 1fab new_server 이 명령어를 치면 자동으로 모든 과정이 완료되고 서버가 뜹니다. 만약 파일을 수정하고 커밋했다면, Github Repo에 올린 후 deploy 명령어를 통해 새 코드를 서버에 배포할 수 있습니다. 1fab deploy 짜잔!프로젝트 하나가 배포가 완료되었습니다! 아무것도 없어보이지만, DB에 자료를 추가하면 IRKSHOP 예제처럼 예쁘게 생성됩니다.","link":"/2017/03/20/Deploy-Django-with-Fabric/"},{"title":"편리한 깃헙페이지 블로깅을 위한 이미지서버, 구글드라이브: 앱으로 만들고 키보드 단축키 연결하기","text":"본 가이드는 MacOS에서 이용가능합니다. 이전 가이드: 편리한 깃헙페이지 블로깅을 위한 이미지서버, 구글드라이브: 업로드 ShellScript편 터미널에서 gdrive list라고 했을때 에러가 나지 않는 상태에서 아래 가이드를 진행해주세요. 들어가며이전 가이드에서 스크린샷을 찍고 구글드라이브에 올린 후 이미지의 공유 URL을 가져오는 스크립트를 작성했습니다. 하지만 키보드 Shortcuts를 이용한 편리성에는 따라가기가 어렵죠. .sh스크립트를 키보드로 연동하는 방법 중 여러가지 방법이 있지만, 이번에는 MacOS App으로 만든 후 앱을 실행하는 것을 서비스에 등록하고 Automator를 통해 키보드와 앱실행 서비스를 연동하는 과정을 다룹니다. 만약 잘 동작하는 맥용 앱을 바로 다운받으시려면 CaptureToGdrive.zip을 받아주신 후 압축을 푸신 후 앱을 Application폴더로 옮기신 후 [백투더맥 Q&amp;A] 키보드 단축키로 응용 프로그램을 실행하는 바로 가기 만들기 과정을 따라가시면 됩니다. SH파일을 앱으로 만들기우선 .sh파일로 된 스크립트를 맥용 앱으로 Wrapping해주는 작업이 필요합니다. 이번 가이드에서는 이 작업을 간소화해주는 platypus를 이용합니다. platypus는 platypus.zip을 받고 압축을 풀어 사용하시면 됩니다. 앱을 실행하면 아래와 같은 화면이 뜹니다. App Name을 CaptureToGdrive로, Script Type을 bash로, Script Path는 아래의 +New를 눌러 아래와 같이 코드를 입력해 줍시다. 12345678#!/bin/bashscreencapture -tpng -i /tmp/temp_shot_gdrive.pngDATEFILENAME=`date +\"%Y%m%d%H%M\"`ID=`/usr/local/bin/gdrive upload /tmp/temp_shot_gdrive.png --name screenshot${DATEFILENAME}.png --share | egrep \"^Uploaded\" | awk '{print $2}'`URL=\"https://drive.google.com/uc?id=${ID}\"echo ${URL} | pbcopy 위 코드는 이전 가이드에서 다뤘던 것과 약간 다른데요, gdrive명령어의 위치를 명확히 /usr/local/bin/gdrive로 바꿔준점이 다릅니다. 쉘 스크립트를 사용할 때 명확히 하지 않으면 gdrive의 PATH를 잡지 못해 에러가 나기 때문입니다. 만약 다른 위치에 까셨다면 which gdrive명령을 통해 그 위치로 변경해주시면 됩니다. 스크립트를 입력하고 나면 AppName이 초기화되는 사소한 문제가 있으니 다시 AppName을 등록해 줍시다. 스크린샷 촬영은 인터페이스가 필요없기 때문에 Interface는 None으로, root권한이 필요없고 백그라운드일 필요도 없고 프로그램이 굳이 계속 떠 있을 필요가 없기때문에 모든 체크박스는 아래와 같이 체크해제 해두시면 됩니다. 이제 CreateApp을 클릭하고 아래와 같이 클릭한 후 Create를 누르면 앱이 만들어집니다 :) 만들어진 앱을 실행해 보시고 잘 되시는지 확인해보세요. 앱을 키보드로 연결하기이 부분은 좀 더 잘 정리되어있는 [백투더맥 Q&amp;A] 키보드 단축키로 응용 프로그램을 실행하는 바로 가기 만들기을 참고하시기 바랍니다. 마치며이번 가이드에서는 업로드 되는 폴더를 정확히 명시하지는 않았습니다. gdrive패키지에서 -p를 이용하면 폴더를 지정가능하다고 하지만 테스트 결과 제대로 업로드 되지 않는 것을 확인했기 때문에, 현재 주로 사용하지는 않는 다른 구글 아이디에 gdrive를 연결해 두었습니다. Imgur, Dropbox등 여러 이미지 Serving 업체들이 있지만, 구글이 가진 구글 Fiber망과 서비스의 안정성은 여타 서비스들이 따라가기 어려운 점이라고 생각합니다 :)","link":"/2017/03/28/Make-Capture-to-GDrive-App/"},{"title":"편리한 깃헙페이지 블로깅을 위한 이미지서버, 구글드라이브: 업로드 ShellScript편","text":"본 가이드는 MacOS에서 이용가능합니다. 들어가며깃헙 페이지를 Jekyll등을 이용해 Markdown파일을 이용하다보면 스크린샷을 저장하고 깃헙 레포 폴더에 옮긴 후 수동으로 url을 추가해 주는 작업이 상당히 귀찮고, 심지어 깃헙 레포당 저장공간은 1G로 제한됩니다. Dropbox의 경우에는 MacOS 내장 스크린샷(CMD+Shift+4)를 이용할 경우 파일을 자동으로 dropbox에 올린 후 공유 url이 나옵니다. 하지만 일반 유저는 용량 제한도 있고, 트래픽 제한도 있습니다. 따라서 무료로 15G의 용량과 명시적 트래픽 제한이 없는 구글드라이브를 이용하는 방안을 고려해보았습니다. 정확히는 Github은 레포당 용량을 명시적으로 제한하지는 않지만 1G가 넘어가는 경우 스토리지를 이용하도록 가이드합니다. Dropbox링크를 통한 트래픽은 무료 유저의 경우 일 20G, 유료플랜 유저의 경우 일 200G를 줍니다. GoogleDrive의 경우 무료 계정도 일 100G(추정치)의 트래픽을 제공하기 때문에 큰 무리는 따르지 않는다 생각합니다. Gdrive 설치하기이번 가이드에서는 Gdrive를 이용합니다. Homebrew를 통해 간단히 설치할 수 있습니다. 터미널에서 아래와 같이 입력해 주세요. 1brew install gdrive Gdrive AUTHgdrive를 설치하고 나서, gdrive가 구글드라이브에 액세스 할 수 있도록 권한을 부여해야 합니다. 아래 명령어는 구글드라이브의 최상위 디렉토리를 리스팅 하는 명령어인데, 이 과정에서 드라이브 액세스 권한을 요구하기 때문에 자연스럽게 권한 등록이 가능합니다. 1gdrive list 명령어를 입력시 아래와 같은 창이 뜹니다. 절대 창을 끄지 마시고 아래 안내되는 구글 링크로 들어가세요. 보안을 위해 키 일부를 지웠습니다. 원래는 회색 빈칸이 없습니다 :) 링크를 따라가시면 구글 로그인을 요구합니다. 로그인을 하시면 아래와 같은 권한 요구 창이 뜨는데요, ‘허용’을 눌러주시면 됩니다. 허용을 누르면 아래와 같은 코드가 나옵니다. 이 코드를 아까 터미널 창에 복사-붙여넣기를 해주세요. 만약 코드가 정상적이었다면 아래와 같이 최상위 디렉토리의 폴더/파일 리스트가 나타납니다. capture.sh파일 만들기Gdrive가 정상적으로 구글 계정과 연결되었다면, 이제 capture.sh파일을 만들어야 합니다. 파일의 코드는 아래와 같습니다. 복사 하신 후 원하시는 위치에 넣어주세요. (저는 ~/capture.sh로 두었습니다.) 123456789#!/bin/bash# ~/capture.shscreencapture -tpng -i /tmp/temp_shot_gdrive.pngDATEFILENAME=`date +\"%Y%m%d%H%M\"`# use -p id to upload to a specific folderID=`gdrive upload /tmp/temp_shot_gdrive.png --name screenshot${DATEFILENAME}.png --share | egrep \"^Uploaded\" | awk '{print $2}'`URL=\"https://drive.google.com/uc?id=${ID}\"echo ${URL} | pbcopy 우선 이 스크립트 파일에 실행권한을 줘야 합니다. 1chmod +x capture.sh 이제 ./capture.sh명령을 입력하면 캡쳐메뉴로 진입하고, 캡쳐를 진행하고 잠시 기다리면(업로드 시간) 클립보드에 구글드라이브로 공유된 파일의 URL이 복사됩니다. rc파일(.zshrc/.bashrc)에 alias걸기capture.sh파일을 둔 위치가 ~/capture.sh라고 가정하고, ~/.zshrc(혹은~/.bashrc) 파일을 수정해 주겠습니다. 항상 ./capture.sh라고 입력하는 것은 귀찮은 일이기 때문에, alias를 통해 cap라는 명령어를 캡쳐 명령어로 지정해 봅시다. .zshrc나 .bashrc파일 제일 아래에 아래 코드를 덧붙여주고 저장해줍시다. 1alias cap=\"~/capture.sh\" 터미널을 재실행한 후 cap라는 명령을 치면 캡쳐 도구가 뜹니다! 다음 가이드: 앱으로 만들어 단축키로 연결하기기본 스크린샷처럼 키보드 단축키 만으로 스크린샷 링크를 가져올 수 있다면 훨씬 편리하겠죠? 다음 가이드에서는 이번에 만든걸 앱으로 만들어 스크린샷 단축키로 연결하는 과정을 다룹니다. 다음가이드: 편리한 깃헙페이지 블로깅을 위한 이미지서버, 구글드라이브: 앱으로 만들고 키보드 단축키 연결하기 완성된 앱도 함께 제공합니다!","link":"/2017/03/28/Use-GoogleDrive-as-Image-Server/"},{"title":"Django에 Social Login와 Email유저 함께 이용하기","text":"django-custom-user와 social-auth-app-django(구 python-social-auth)를 이용해 이메일 기반 유저와 소셜 로그인으로 로그인 한 유저를 하나처럼 사용하는 방법입니다. 장고에 소셜 로그인을 붙이는 가이드는 Django에 Social Login 붙이기: Django세팅부터 Facebook/Google 개발 설정까지 포스팅에서 찾으실 수 있습니다. Django + SocialLogin + Email as User웹 서비스를 제공할 때 여러가지 로그인 방법을 구현할 수 있습니다. 아이디/패스워드 기반의 방식, 페이스북과 구글등의 OAuth를 이용한 소셜 로그인 방식 등이 있습니다. 장고 프로젝트를 만들 때 django-custom-user등의 패키지를 이용하면 이메일 주소를 Unique Key로 사용해 이메일 주소로 로그인을 할 수 있도록 만들어 줍니다. django-custom-user에 관한 문서는 Django Custom User GitHub에서 확인하실 수 있습니다. 하지만, social-auth-app-django를 통해 유저를 생성 할 경우 OAuth Provider에 따라 다른 User를 생성합니다. 즉, 같은 이메일 주소를 가지고 있는 유저라 하더라도 페이스북을 통해 로그인 한 유저와 구글을 통해 로그인 한 유저는 다르게 다뤄진다는 뜻입니다. 사실 이메일 주소를 신뢰하지 않고 Provier마다 다른 유저로 생성하는 것이 기본으로 되어있는 이유는 Oauth Provier의 신뢰 문제입니다. 모든 Oauth Provier가 가입한 유저의 Email의 실 소유권을 확인하지는 않기 때문입니다. 이를 해결하고 같은 이메일을 통해 로그인한 유저는 모두 같은 유저로 취급하기 위해서는 장고 프로젝트 폴더의 settings.py파일 안에서 social-auth-app-django의 Pipeline설정을 변경해 줘야 합니다. 123456789101112SOCIAL_AUTH_PIPELINE = ( 'social_core.pipeline.social_auth.social_details', 'social_core.pipeline.social_auth.social_uid', 'social_core.pipeline.social_auth.auth_allowed', 'social_core.pipeline.social_auth.social_user', 'social_core.pipeline.user.get_username', 'social_core.pipeline.social_auth.associate_by_email', # &lt;--- 이 줄이 핵심입니다. 'social_core.pipeline.user.create_user', 'social_core.pipeline.social_auth.associate_user', 'social_core.pipeline.social_auth.load_extra_data', 'social_core.pipeline.user.user_details',) SOCIAL_AUTH_PIPELINE은 settings.py내에는 기본적으로 지정이 해제되어있습니다. 따라서 변수가 없는 경우 위 코드 전체를 settings.py파일 끝에 덧붙이시면 됩니다. 참고: Python Social Auth: Associate users by Email","link":"/2017/03/22/Setup-SocialAuth-for-Django-Email-as-User/"},{"title":"나만의 웹 크롤러 만들기(5): 웹페이지 업데이트를 알려주는 Telegram 봇","text":"좀 더 보기 편한 깃북 버전의 나만의 웹 크롤러 만들기가 나왔습니다! 이전게시글: 나만의 웹 크롤러 만들기(4): Django로 크롤링한 데이터 저장하기 이번 가이드에서는 작업하는 컴퓨터가 아닌 원격 우분투16.04 서버(vps)에 올리는 부분까지 다룹니다. 테스트는 crontab -e 명령어를 사용할 수 있는 환경에서 가능하며, VISA/Master카드등 해외결제가 가능한 카드가 있다면 서비스 가입 후 실제로 배포도 가능합니다. 이 가이드에서는 Vultr VPS를 이용합니다. 앞서 Django를 이용해 크롤링한 데이터를 DB에 저장해 보았습니다. 하지만 크롤링을 할 때마다 동일한(중복된) 데이터를 DB에 저장하는 것은 바람직 하지 않은 일이죠. 또한, 크롤링을 자동으로 해 사이트에 변경사항이 생길 때 마다 내 텔레그램으로 알림을 받을 수 있다면 더 편리하지 않을까요? 이번 가이드에서는 클리앙 중고장터 등을 크롤링해 새 게시글이 올라올 경우 새글 알림을 텔레그램으로 보내는 것까지를 다룹니다. 다루는 내용: Telegram Bot API requests / BeautifulSoup Crontab 시작하며widgets:텔레그램은 REST API를 통해 봇을 제어하도록 안내합니다. 물론 직접 텔레그램 api를 사용할 수도 있지만, 이번 가이드에서는 좀 더 빠른 개발을 위해 python-telegram-bot 패키지를 사용합니다. python-telegram-bot은 Telegram Bot API를 python에서 쉽게 이용하기 위한 wrapper 패키지입니다. python-telegram-bot 설치하기widgets:python-telegram-bot은 pip로 설치 가능합니다. 1pip install python-telegram-bot requests, bs4 역시 설치되어있어야 합니다! 텔레그램 봇 만들기 &amp; API Key받기widgets:텔레그램 봇을 만들고 API키를 받아 이용하는 기본적인 방법은 python에서 telegram bot 사용하기에 차근차근 설명되어있습니다. 위 가이드에서 텔레그램 봇의 토큰을 받아오세요. 토큰은 aaaa:bbbbbbbbbbbbbb와 같이 생긴 문자열입니다. 이번 가이드는 텔레그램 봇을 다루는 내용보다는 Cron으로 크롤링을 하고 변화 발견시 텔레그램 메시지를 보내는 것에 초점을 맞췄습니다. 텔레그램 봇 API키를 받아왔다면 아래와 같이 크롤링을 하는 간단한 python파일을 작성해 봅시다. 클리앙에 새로운 글이 올라오면 “새 글이 올라왔어요!”라는 메시지를 보내는 봇을 만들어 보겠습니다. 클리앙 새글 탐지코드 만들기widgets: 우선 게시판의 글 제목중 첫번째 제목을 가져오고 txt파일로 저장하는 코드를 만들어 봅시다. 회원 장터 주소는 http://clien.net/cs2/bbs/board.php?bo_table=sold이고, 첫 게시글의 CSS Selector는 #content &gt; div.board_main &gt; table &gt; tbody &gt; tr:nth-child(3) &gt; td.post_subject &gt; a임을 알 수 있습니다. 따라서 아래와 같이 latest 변수에 담아 같은 폴더의 latest.txt 파일에 써 줍시다. 123456789101112131415161718# clien_market_parser.pyimport requestsfrom bs4 import BeautifulSoupimport os# 파일의 위치BASE_DIR = os.path.dirname(os.path.abspath(__file__))req = requests.get('http://clien.net/cs2/bbs/board.php?bo_table=sold')req.encoding = 'utf-8' # Clien에서 encoding 정보를 보내주지 않아 encoding옵션을 추가해줘야합니다.html = req.textsoup = BeautifulSoup(html, 'html.parser')posts = soup.select('td.post_subject')latest = posts[1].text # 0번은 회원중고장터 규칙입니다.with open(os.path.join(BASE_DIR, 'latest.txt'), 'w+') as f: f.write(latest) 위와 같이 코드를 구성하면 latest.txt파일에 가장 최신 글의 제목이 저장됩니다. 크롤링 이후 새로운 글이 생겼는지의 유무를 알아보려면 크롤링한 최신글의 제목과 파일에 저장된 제목이 같은지를 확인하면 됩니다. 만약 같다면 패스, 다르다면 텔레그램으로 메시지를 보내는거죠! 1234567891011121314151617181920212223242526# clien_market_parser.pyimport requestsfrom bs4 import BeautifulSoupimport os# 파일의 위치BASE_DIR = os.path.dirname(os.path.abspath(__file__))req = requests.get('http://clien.net/cs2/bbs/board.php?bo_table=sold')req.encoding = 'utf-8'html = req.textsoup = BeautifulSoup(html, 'html.parser')posts = soup.select('td.post_subject')latest = posts[1].textwith open(os.path.join(BASE_DIR, 'latest.txt'), 'r+') as f_read: before = f_read.readline() if before != latest: # 같은 경우는 에러 없이 넘기고, 다른 경우에만 # 메시지 보내는 로직을 넣으면 됩니다. f_read.close()with open(os.path.join(BASE_DIR, 'latest.txt'), 'w+') as f_write: f_write.write(latest) f_write.close() 새글이라면? 텔레그램으로 메시지 보내기!widgets:이제 메시지를 보내볼게요. telegram을 import하신 후 bot을 선언해주시면 됩니다. token은 위에서 받은 토큰입니다. 1234567891011121314151617181920212223242526272829303132333435# clien_market_parser.pyimport requestsfrom bs4 import BeautifulSoupimport osimport telegram# 토큰을 지정해서 bot을 선언해 줍시다! (물론 이 토큰은 dummy!)bot = telegram.Bot(token='123412345:ABCDEFgHiJKLmnopqr-0StUvwaBcDef0HI4jk')# 우선 테스트 봇이니까 가장 마지막으로 bot에게 말을 건 사람의 id를 지정해줄게요.# 만약 IndexError 에러가 난다면 봇에게 메시지를 아무거나 보내고 다시 테스트해보세요.chat_id = bot.getUpdates()[-1].message.chat.id# 파일의 위치BASE_DIR = os.path.dirname(os.path.abspath(__file__))req = requests.get('http://clien.net/cs2/bbs/board.php?bo_table=sold')req.encoding = 'utf-8'html = req.textsoup = BeautifulSoup(html, 'html.parser')posts = soup.select('td.post_subject')latest = posts[1].textwith open(os.path.join(BASE_DIR, 'latest.txt'), 'r+') as f_read: before = f_read.readline() if before != latest: bot.sendMessage(chat_id=chat_id, text='새 글이 올라왔어요!') else: # 원래는 이 메시지를 보낼 필요가 없지만, 테스트 할 때는 봇이 동작하는지 확인차 넣어봤어요. bot.sendMessage(chat_id=chat_id, text='새 글이 없어요 ㅠㅠ') f_read.close()with open(os.path.join(BASE_DIR, 'latest.txt'), 'w+') as f_write: f_write.write(latest) f_write.close() 이제 clien_market_parser.py파일을 실행할 때 새 글이 올라왔다면 “새 글이 올라왔어요!”라는 알림이, 새 글이 없다면 “새 글이 없어요 ㅠㅠ”라는 알림이 옵니다. 지금은 자동으로 실행되지 않기 때문에 python clien_market_parser.py명령어로 직접 실행해 주셔야 합니다. 자동으로 크롤링하고 메시지 보내기widgets:가장 쉬운방법: while + sleep가장 쉬운 방법은 python의 while문을 쓰는 방법입니다. 물론, 가장 나쁜 방법이에요. 안전하지도 않고 시스템의 메모리를 좀먹을 수도 있어요. 하지만 테스트에서는 가장 쉽게 쓸 수 있어요. 123456789101112131415161718192021222324252627282930313233343536# clien_market_parser.pyimport requestsfrom bs4 import BeautifulSoupimport osimport timeimport telegrambot = telegram.Bot(token='123412345:ABCDEFgHiJKLmnopqr-0StUvwaBcDef0HI4jk')chat_id = bot.getUpdates()[-1].message.chat.id# 파일의 위치BASE_DIR = os.path.dirname(os.path.abspath(__file__))while True: req = requests.get('http://clien.net/cs2/bbs/board.php?bo_table=sold') req.encoding = 'utf-8' html = req.text soup = BeautifulSoup(html, 'html.parser') posts = soup.select('td.post_subject') latest = posts[1].text with open(os.path.join(BASE_DIR, 'latest.txt'), 'r+') as f_read: before = f_read.readline() if before != latest: bot.sendMessage(chat_id=chat_id, text='새 글이 올라왔어요!') else: bot.sendMessage(chat_id=chat_id, text='새 글이 없어요 ㅠㅠ') f_read.close() with open(os.path.join(BASE_DIR, 'latest.txt'), 'w+') as f_write: f_write.write(latest) f_write.close() time.sleep(60) # 60초(1분)을 쉬어줍니다. 파이썬 동작중에는 CTRL+C로 빠져나올수 있습니다. 추천: 시스템의 cron/스케쥴러를 이용하기이번 가이드에서 핵심인 부분인데요, 이 부분은 이제 우분투 16.04 기준으로 진행할게요. 우선 Ubuntu 16.04가 설치된 시스템이 필요합니다. 이번 강의에서는 Vultr VPS를 이용합니다. Vultr는 가상 서버 회사인데, Tokyo리전의 VPS를 제공해줘 빠르게 이용이 가능합니다. 트래픽도 굉장히 많이주고요. 가이드를 만드는 지금은 월 2.5달러 VPS는 아쉽게도 없어서, 월5달러 VPS로 진행하지만 월2.5달러 VPS로도 충분합니다! 아래의 Deploy Now를 누르면 새 Cloud Instance(VPS)가 생성되는데요, 서버가 생성된 후 들어가 보면 다음과 같이 id와 pw가 나와있습니다. 패스워드는 눈 모양을 누르면 잠시 보입니다. 이 정보로 ssh에 접속해 봅시다. (윈도는 putty이나 Xshell등을 이용해주세요.) 우분투 16.04버전에는 이미 Python3.5버전이 설치되어있기 때문에 pip3, setuptools을 설치해 주고 Ubuntu의 Locale을 설정해줘야 합니다. 아래 명령어를 한줄씩 순차적으로 치시면 완료됩니다. 123sudo apt install python3-pip python3-setuptools build-essentialsudo locale-gen \"ko_KR.UTF-8\"pip3 install requests bs4 python-telegram-bot 설치를 하신 후 코드를 테스트 해보려면 위 파일을 vi등으로 열어 위 코드들을 입력하시면 됩니다. 이제 코드를 약간 바꿔볼게요. 새 글이 올라올때만 파일을 다시 쓰도록 해요. 1234567891011121314151617181920212223242526272829# clien_market_parser.pyimport requestsfrom bs4 import BeautifulSoupimport osimport telegrambot = telegram.Bot(token='123412345:ABCDEFgHiJKLmnopqr-0StUvwaBcDef0HI4jk')chat_id = bot.getUpdates()[-1].message.chat.id# 파일의 위치BASE_DIR = os.path.dirname(os.path.abspath(__file__))req = requests.get('http://clien.net/cs2/bbs/board.php?bo_table=sold')req.encoding = 'utf-8'html = req.textsoup = BeautifulSoup(html, 'html.parser')posts = soup.select('td.post_subject')latest = posts[1].textwith open(os.path.join(BASE_DIR, 'latest.txt'), 'r') as f_read: before = f_read.readline() f_read.close() if before != latest: bot.sendMessage(chat_id=chat_id, text='새 글이 올라왔어요!') with open(os.path.join(BASE_DIR, 'latest.txt'), 'w+') as f_write: f_write.write(latest) f_write.close() 이제 이 clien_market_parser.py 파일을 python3으로 실행해야 하기 때문에, python3이 어디 설치되어있는지 확인 해 봅시다.(아마 /usr/bin/python3일거에요!) 12root@vultr:~# which python3/usr/bin/python3 이제 Crontab에 이 파이썬으로 우리 파일을 매 1분마다 실행하도록 만들어 봅시다. crontab 수정은 crontab -e명령어로 사용 가능합니다. 만약 에디터를 선택하라고 한다면 초보자는 Nano를, vi를 쓰실수 있으시다면 vi를 이용하세요. 이 한줄을 crontab 마지막에 추가해 주세요. 1* * * * * /usr/bin/python3 /root/clien_market_parser.py 힌트: 매 12분마다로 하시려면 */12 * * * * /usr/bin/python3 /root/clien_market_parser.py로 하시면 됩니다. 이제 여러분의 휴대폰으로 새 글이 올라올 때 마다 알람이 올라올 거랍니다 :) 마무리widgets:이번편 가이드는 DB를 이용하지 않고 단순하게 새로운 글이 왔다는 사실만을 메시지로 알려주는 봇을 만들어서 뭔가 아쉬움이 있을겁니다. 다음편 가이드는 multiprocessing을 이용한 N배 빠른 크롤러을 만들어 봅니다. 다음 가이드: 나만의 웹 크롤러 만들기(6): N배빠른 크롤링, multiprocessing!","link":"/2017/04/20/HowToMakeWebCrawler-Notice-with-Telegram/"},{"title":"배포한 Django 서비스 Exception Sentry로 받아보기","text":"이번 포스팅은 wsgi기반으로 동작하는 django서비스를 대상으로 합니다. Django 서비스를 실제 서버에 배포하면 보안을 위해 프로젝트 폴더의 settings.py파일 안의 DEBUG항목을 False로 두고 배포합니다. 이렇게 디버그모드를 끌 경우 장고에서 기본적인 보안을 제공해 줍니다. 그러나 만약 View나 Model에서 Exception이 발생했을 경우 클라이언트에 흰색의 500에러 화면만을 띄워줍니다. 이 경우 개발자에게도 장고의 에러 화면을 보여주지 않습니다. 따라서 Exception이 발생할 경우 개발자(혹은 운영자)에게 에러를 전송할 필요가 있습니다. wsgi 기반으로 서버를 구동할 경우 에러로그는 Apache2나 NginX등의 웹서버의 접근/에러로그가 있으며 Wsgi의 에러로그로 두가지가 있습니다. 장고서버의 경우에는 Wsgi의 에러로그에 로그를 쌓습니다. 그러나, Django 프로젝트에 LOGGERS 설정값을 추가해줘야 하며 에러 트래킹을 따로 설정해줘야 합니다. 이때 사용할 수 있는 것이 Sentry와 같은 서비스입니다. 이번 가이드는 Sentry for Django를 기반으로 진행합니다. Sentry 설치하기우선 raven을 설치해 줍니다. raven은 Sentry를 위한 Python패키지입니다. 1pip install raven --upgrade \u0013Sentry settings.py에 설정하기장고 프로젝트의 settings.py파일 안 INSTALLED_APPS에 아래 줄을 추가해줍니다. 1234INSTALLED_APPS = [ # 기존 앱 가장 아래에 추가해주세요. 'raven.contrib.django.raven_compat',] 이제 Sentry용 환경변수를 추가해 줍시다. 아래 DSN_URL은 Sentry에 로그인 하신 후 Sentry for Django의 코드 부분을 복사하면 알 수 있습니다. 1234567891011121314# settings.py 파일 import문 아래에 raven을 import해주세요.import osimport raven# import아래 환경변수를 설정해주세요. 이 URL은 위 Sentry for Django에서 바로 찾을 수 있습니다.DSN_URL = 'https://sampleurl1234141534samplesample:somemoreurl12341235dfaetr@sentry.io/123456'# 기타 설정들(생략...)# settings.py 파일 가장 아래에 RAVEN_CONFIG를 추가해주세요.RAVEN_CONFIG = { 'dsn': '{}'.format(DSN_URL), # DSN_URL을 위에 적어주셔야 동작합니다. 'release': raven.fetch_git_sha(BASE_DIR), # Django가 Git으로 관리되는 경우 자동으로 커밋 버전에 따른 트래킹을 해줍니다.} Sentry wsgi.py에 설정하기이제 장고 프로젝트 폴더 안의 wsgi.py 파일을 아래와 같이 수정해봅시다. 12345678910import osfrom django.core.wsgi import get_wsgi_applicationfrom raven.contrib.django.raven_compat.middleware.wsgi import Sentry# 이 부분은 여러분의 장고 프로젝트 이름을 쓰세요..os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"장고프로젝트이름.settings\")# get_wsgi_application을 Sentry()로 감싸주세요.application = Sentry(get_wsgi_application()) 자, 이제 Sentry 사용을 위한 기본적인 설정이 끝났습니다. 여러분의 서비스를 다시 원래 서버에 배포해보세요. Exception이 발생하면..이제 여러분이 잡아주지 않은 Exception이 발생할 경우 아래와 같이 이메일이 옵니다. \b이메일의 Issue 링크를 클릭하면 아래와 같이 에러로그 페이지가 나옵니다. 조금 더 알아보기이번 포스팅에서는 장고의 Wsgi에서 발생하는 에러(Exception)을 Sentry 미들웨어를 통해 관리합니다. 그러나 유저에게 500페이지를 보여주는것은 여전합니다. 만약 여러분이 미리 잡아준 상황을 Sentry로 보내고 싶으시다면 Sentry를 logging와 함께 쓰기를 참고하세요.","link":"/2017/06/01/Add-Sentry-to-Django/"},{"title":"Django MS Azure에 Fabric으로 배포하기","text":"If you looking for English version, goto Deploy Django to MS Azure with Fabric3. 이번 가이드에서는 DjangoGirls Tutorial를 Fabric으로 Azure 가상 컴퓨터(Ubuntu16.04LTS)에 올리는 과정을 다룹니다. 지금 이 글을 읽고 있는 분들은 아마 장고걸즈 워크숍에 참가해 DjangoGirls Tutorial을 따라가다 이제 배포를 해볼 단계에 도착하셨을거에요. 오늘 우리가 만들어본(만들고 있는) Django 프로젝트를 MS가 서비스하는 Azure(애저)에 배포해보는 시간을 가져볼거에요. 만약 여러분이 AzurePass를 아직 계정에 등록하지 않았다면, Azure 가입하고 AzurePass 등록하기를 먼저 진행해 주세요. (윈도우의 경우) cmder 설치하기윈도우에서는 git와 ssh등의 명령어를 cmd에서 바로 사용할 수 없어요. 그래서 우리는 cmder라는 멋진 프로그램을 사용할 거에요. 우선 cmder.zip을 클릭해 cmder를 받아주세요. (84MB정도라 시간이 조금 걸릴거에요.) 다운받은 cmder.zip 파일의 압축을 풀어주세요.(이것도 시간이 조금 걸릴거에요.) 그러면 다음과 같은 내용이 보일거에요. 여기 있는 cmder.exe 파일을 실행해주세요. 실행하면 아래와 같은 Security Warning이 뜰 수 있어요. RUN을 눌러주세요. 만약 여러분이 cmder를 처음 실행하신다면 아래와 같은 워닝이 뜰거에요. 제일 첫번째 옵션인 “Unblock and continue”를 눌러주세요. 첫 실행시 아래 화면에서 약간 시간이 걸릴 수 있어요. 다음번 실행부터는 뜨지 않을테니 잠시만 기다려주세요! 아래 화면이 뜨면 여러분이 멋진 cmder를 쓸 준비를 마친거랍니다! 장고걸즈 튜토리얼을 따라오는 중이시라면 djangogirls라는 폴더를 만드셨을거에요. 폴더로 이동하는 명령어가 cd입니다! cd djangogirls라고 입력해주세요. 이제 배포를 진행해볼게요. Azure 가상컴퓨터 만들기Azure Portal에 들어가 로그인 하시면 아래와 같은 화면을 볼 수 있습니다. 이제 대시보드 왼쪽의 메뉴에서 가상 컴퓨터를 눌러보시면 아래와 같은 화면이 나옵니다. 이제 가상 컴퓨터를 추가해봅시다. + 추가 버튼을 눌러주시면 아래와 같은 수많은 선택지가 나오는데요, 우리는 그 중에서 Ubuntu Server(우분투 서버)를 사용할거랍니다. Ubuntu Server를 클릭해주세요. 우분투 서버를 클릭하면 아래와 같이 서버 버전들이 나옵니다. 우리는 오늘 Ubuntu Server 16.04 LTS를 사용할거에요. 우분투 서버 16.04를 선택하고 나면 아래와 같이 만들기 버튼이 나올거에요. 버튼을 눌러주세요. 만들기 버튼을 누르면 아래 사진처럼 기본 사항을 설정하는 창이 나올거에요. 내용을 화면 사진 그대로 채워주세요. 사용자 이름은 가이드 다음부분에서 이용할 django로 하셔야 합니다. 암호는 각자 사용하는 암호를 입력하시면 되는데요, 12자리 이상을 요구하기 때문에 약간 어려우실 수 있어요! 위치는 대한민국 중부/남부로 해주세요. (안되는 경우 아시아 남동부로 해주세요!) 이제 서버의 크기를 골라야 하는데요, 오늘 우리는 장고 서버만을 띄울 것이기 때문에 가장 왼쪽에 있는 DS1_V2를 이용할거에요. AzurePass를 등록하면 돈을 지불하지 않아도 되니 걱정하지 마세요! 이제 다음으로 넘어가면 저장소 설정을 해야 해요. 이부분에서는 ‘관리 디스크 사용’을 ‘예’로 클릭해주세요. 그 다음으로는 아래쪽의 네트워크 보안 그룹(방화벽)을 클릭해 주세요. 클릭하시면 아래와 같이 SSH (TCP/22)만 인바운드 규칙에 들어가 있는 것을 확인할 수 있어요. 우리는 여기서 HTTP (TCP/80)을 추가해 줄 거에요. + 인바운드 규칙 추가 버튼을 눌러주시고 사진과 같이 칸을 채워주시고 확인 버튼을 눌러주세요. 이제 설정이 모두 끝났어요! 아래쪽의 확인버튼을 눌러주세요. 다시 한번 확인 버튼을 눌러주세요. 이제 정말로 다음 확인버튼만 누르면 서버 설치가 끝나요! 이제 조금만 기다려주시면 서버 설치가 끝난답니다! 아이콘의 설명이 Creating에서 Running으로 바뀌면 설치가 끝난거에요. Azure 설정 확인하기Running으로 바뀐 아이콘을 클릭해주시면 아래 화면으로 들어올 수 있어요. 애저에서 가상컴퓨터가 생기면 공용 IP 주소라는걸 하나 갖게 된답니다. ip라는 것은 서버나 컴퓨터가 인터넷에 접속할 수 있게 해주는 일련의 번호인데요, 우리는 이 ip를 통해 우리 장고 프로젝트를 서버에 올리는 작업을 진행할 수 있어요. 지금 화면에 보이는 가상컴퓨터의 ip는 52.231.30.148인데요, 이렇게 숫자로만 되어있으면 우리는 기억하기가 어려워요. 그래서 djangogirls.com와 같이 사람이 이해하고 외우기도 쉬운 도메인을 저 ip주소에 붙여줄거에요. 자, 우선 여러분의 가상 컴퓨터의 ip를 복사(Ctrl+C / CMD+C)해두세요! 무료 도메인 얻어 가상컴퓨터에 연결하기.com, .net와 같이 유명한 도메인은 돈을 주고 사야한답니다.(1년에 만원정도 나가요) 하지만 우리는 오늘 무료 도메인을 연결해 볼 거에요. 우선 Dot.tk로 들어오세요. 이 Dot.tk에서는 .tk 도메인을 무료로 제공하고 있어요. 우선 djangogirls-seoul-tutorial이라는 이름으로 찾아볼게요. 값을 입력하고 Check Availability를 눌러주세요! 여러분은 여러분이 원하는 주소를 검색해보세요! (ex: myfirstdjango 등등) 오, 다행히 주소가 남아있어요. 이제 Get it now!버튼을 눌러 장바구니에 담아볼게요. 장바구니에 담은 도메인을 Checkout버튼을 눌러주면 아래 화면으로 넘어올거에요. Use DNS버튼을 눌러주신 후에 IP address 칸에 아까 애저 가상컴퓨터의 ip를 입력해주신 후 Continue를 눌러주세요. Continue를 누르면 로그인 화면이 나와요. 구글이나 페이스북의 소셜 로그인을 이용할 수 있어요! 페이스북은 가끔 오류가 나기도 해요. 그럴때는 구글이나 MS Live계정으로 다시한번 시도해주세요. 로그인이 완료되면 아래와 같이 자신의 정보를 입력하는 부분이 나와요. 꼭 다 채울필요는 없어요! 주문 거래 동의 체크상자를 클릭한 후 계속 버튼을 누르면 주문이 완료된답니다! 좋아요! Fabric3 설치하기이제 여러분의 서버는 여러분이입력한이름.tk라는 인터넷 주소로 연결되었어요. 하지만 아직 여러분의 서버에는 아무것도 설치되어있지 않아요. 물론 장고도 설치되어있지 않아요. 이제 Fabric3이라는 멋진 자동화 도구를 통해 명령어 한 줄로(마치 startapp처럼) 진짜 서버에 배포하는 멋진 작업을 해볼거에요! 우선 여러분의 가상환경에 fabric3을 설치해줘야 해요. fabric3은 아래 명령어를 통해 설치할 수 있어요. fabric이 아니라 fabric3입니다! 3을 빼먹지 마세요. 그냥 fabric은 파이썬2용이랍니다. 1pip install fabric3 deploy.json 수정하기이제 Fabfile for Django를 클릭해 압축파일을 받아 풀어주세요. 안에 deploy.json와 fabfile.py가 보일거에요. 이 두 파일을 여러분의 장고 폴더(manage.py파일이 있는 곳)안에 넣어주세요. deploy.json파일 안에는 우리 서버의 정보를 적어넣을 수 있어요. 123456789{ \"REPO_URL\":\"깃헙Repo주소\", \"PROJECT_NAME\":\"프로젝트폴더(settings.py가있는 폴더)의 이름(ex: mysite)\", \"REMOTE_HOST\":\"여러분이 만든 도메인주소(ex: djangogirls-seoul-tutorial.tk )\", \"REMOTE_USER\":\"django\", \"STATIC_ROOT\":\"static\", \"STATIC_URL\":\"static\", \"MEDIA_ROOT\":\"media\"} 파일에 있는 REPO_URL, PROJECT_NAME, REMOTE_HOST 부분을 채워주세요. 나머지 값은 우리가 따라한 튜토리얼에서 이미 설정되어있어요. 모든 값은 “큰 따옴표” 안에 들어가야 한다는 것을 주의하세요! Fabric으로 서버에 올리기Fabric을 사용하기 위한 명령어는 fab이라는 명령어입니다. 이 fab뒤에 new_server, deploy, createsuperuser등을 덧붙여 실제로 원격 서버에 명령을 내리는거에요. 서버에 처음 올릴 때는 fab new_server 명령어를 이용하세요. 파이썬3 설치부터 Apache2설치와 mod_wsgi설치까지 완료해준답니다. 1fab new_server 만약 여러분이 장고 소스를 수정(커밋&amp;푸시)후 서버에 올리고 싶으시다면 fab deploy 명령어를 이용하세요. 장고 앱을 새로 실행해주고 manage.py migrate, manage.py collectstaticfiles등의 명령을 서버에 실행해 준답니다. 1fab deploy 우리가 진행하는 튜토리얼에서는 슈퍼유저 만들기 항목이 있어요. 여러분의 컴퓨터에서는 manage.py createsuperuser를 통해 만들었지만 서버에 띄운 장고에 슈퍼유저를 만들어 주려면 fab create_superuser를 이용해주세요. 1fab create_superuser 짜잔!여러분은 이제 Azure에 올라간 진짜 웹 서비스 하나를 만들었어요! 이제 여러분은 장고로 웹 서비스를 만드는 것의 처음부터 끝까지를 모두 알게되었어요! 축하합니다 :D","link":"/2017/06/09/Deploy-Django-to-MS-Azure/"},{"title":"MS AzurePass 등록하기","text":"시작 전 MS계정이 모두 만들어져 있다고 가정합니다. MS AzurePass(애저패스)란?마이크로소프트에서는 자사 클라우드 서비스 Azure의 프로모 코드인 AzurePass를 행사등에 따라 발급해 줍니다. 무료 크레딧의 경우 30일의 제약이 있고 해외결제가 가능한 카드가 필요하지만 AzurePass를 통해 등록할 경우 카드가 필요하지 않습니다. AzurePass에서는 월 100달러(한국 12만원)의 크레딧을 3개월간 제공해 줍니다. AzurePass 등록하기 유의) 아래 과정은 MS계정 가입과는 별도입니다. 계정이 없다면 가입을 먼저 진행해주세요! 애저패스를 등록하기 위해서는 애저 공식 사이트가 아닌 애저패스의 사이트에서 진행해야 합니다. 애저패스 공식 사이트에 들어가 주세요. 공식 사이트에 들어가시면 아래와 같은 화면이 나옵니다. Start버튼을 눌러주세요. 스타트 버튼을 누르면 아래와 같은 화면이 나옵니다. 받으신 애저패스의 번호를 입력해 주신 후, 아래의 Claim Promo Code를 눌러주세요. Claim PromoCode버튼을 누르면 아래와 같은 화면이 나옵니다. 잠시 기다려 주세요. PromoCode의 등록이 완료되면 애저패스 활성화 버튼이 나옵니다. Activate버튼을 눌러주세요. Activate 버튼을 누르면 나오는 등록 화면입니다. 사용자 정보, 전화번호를 입력 후 구독계약에 동의후 ‘등록’버튼을 눌러주세요. 잠시 기다려주신 후 아래 화면을 보시면 성공적으로 등록이 완료된 것이랍니다. 이제 Azure Portal으로 이동해 작업을 계속하세요!","link":"/2017/06/21/Activate-MS-AzurePass/"},{"title":"Deploy Django to MS Azure with Fabric3","text":"한국어 버전은 Django MS Azure에 Fabric으로 배포하기에서 보실 수 있습니다. This guide covers about deploying DjangoGirls Tutorial to MS Azure Virtual machine(Ubuntu 16.04 LTS) with Fabric3. You’re probably participant in DjangoGirls Tutorial Workshop and you’ll be now on ‘deploy’ step on it. Today we’re going to deploy our django project to Azure which is provided with MS. If you didnt’ register your AzurePass yet, please precede this guide first: Register Azure and redeem AzurePass (If Windows) Using cmderYou can’t use linux commands like git or ssh on your cmd, so we’re going to use great shell program which named cmder. First, click this link:cmder.zip to download cmder. (It may take times.) Second, unzip downloaded cmder.zip file. (It’ll take some times too.) And then you got this!: Execute cmder.exe in this folder. If you execute cmder.exe as a first time you’ll be see Security Warning like this: just click RUN. And one time more, if you execute cmder for the first time, there will be another warning like this: click first option, “Unblock and continue”. It’ll take some times when you run cmder first time. This wouln’t appear next time, so please wait for a moment! If you see this, you’re ready to use cmder NOW! If you’re following DjangoGirls Tutorial, you probably made folder named djangogirls. Let’s get into it. cd is command to ender the folder! Let’s get into djangogirls folder with cd djangogirls. Let’s start deploy then. Deploy Azure Virtual machineYou’ll see this screen if you logged in to Azure Portal. Let’s make Virtual machine with clicking VirtualComputer(가상 컴퓨터) button. Now let’s add Virtual machine with ‘+Add’ button. If you click + Add button, you’ll see another options which provides many OS. But we’re going to use Ubuntu Server today. If you clicked Ubuntu Server there’ll be server lists like this: we’ll use Ubuntu Server 16.04 LTS. Then you’ll see Create button. Click it! You’ll see configure window when you click Create button. Fillout blanks like picture lower. Username should be django (surelly this is not critical but you may encounter issues. You may set your password on your own, but it shoud be longer/equal than 12. Please remember not to reset it later. Select locaiton on Korea Centeral or SouthEast Asia(which available one). Now we have to choose server size. We’ll setup just one django server so we’ll choose DS1_V2, the left one. Don’t worry, you won’t be charged :) Next step you have to setup storage settings. Just select Use managed disks to ‘Yes’. And then click Network Security Group(Firewall) settings. After click on it, you’ll see pre-configured setting SSH (TCP/22). We’re going to add HTTP (TCP/80) Click + Inbound Rule add Button, and fillout blanks like this and click OK button. Now default settings are finished! Just click OK button. And one more time, click confirm button. And lastly, click confirm button more! I know you’re tired with confirm button, but this is process of Azure :) If you see your azure dashboard again like this, your server deployment is finished :) Please wait until your server is successfully installed! (This will take upto 5mins.) Your browser will redirected to your server info page when your server is successfully installed. Get Azure Server ConfigurationsYou can access to your server info with clicking server icon-which tells Running. On this page you can see your server’s ‘Public IP Address’. ip is set of numbers which provides your computer access to internet. We can upload and deploy our django project through this ip. You can see this example server’s ip, 13.67.60.234. Surelly we can access to our server with this numbers but we can’t remember easily with it. So we’re going to use domain like djangogirls.com to that ip. First of all, copy(CTRL+C) your virtual machine’s ip! Get free domain and connect to Virtual MachineYou may know about popular domains like .com or .net. But they are paid one(10 dollars per year) so we’re going to use free domain. Let’s go to Dot.tk. This Dot.tk provides .tk domains as free! I’ll check djangogirls-seoul-tutorial-en.tk as example. You should think of your own domain name and click Check Availability! Oh, it’s available! Just click ‘get it now’ button and add to cart. You’ll see this page when you clicked ‘checkout’ button. Just click ‘Use DNS’ button and input ip address of your virtual machine(azure) and click ‘Continue’. If you forgot ip address of your virtual machine, go to Azure portal and check your machine’s ip again! You’ll see checkout page and you have to login. You can login with your social media account like Google or Facebook! Sometimes there are some errors(404 or others..) then you can restart from “Get free domain and connect to Virtual Machine” on this guide. If you successfully logined, you’ll see form to input your info, but you don’t have to fill it all. Just click Agress Terms and conditions and Conitnue button, your order will be finished! Great! You’ve just connect your own domain to your server! Install Fabric3Now your server is connected with yourdomainname.tk domain. But if you try to access to that address, you can see nothing at all. Because your server doesn’t have any django code and ofcourse, even django! We’ll upload and deploy our django project with just one command line through Fabric3. Let’s install fabric3 on our computer. You can install fabric3 with this command: 1pip install fabric3 Remember: NOT fabric BUT fabric3! Don’t forget 3. fabric is python2 project. Downlaod fabfile.py and edit deploy.jsonDownload Fabfile for Django and unzip it. You can see deploy.json and fabfile.py inside of it. Move 2 files into your django folder(where manage.py exists) Inside deploy.json, we can edit our server(virtual machine) info. 123456789{ \"REPO_URL\":\"Your Github Repository URL\", \"PROJECT_NAME\":\"DjangoProject folder's name(where settings.py exists)\", \"REMOTE_HOST\":\"Your domain(ex: djangogirls-seoul-tutorial-en.tk )\", \"REMOTE_USER\":\"django\", \"STATIC_ROOT\":\"static\", \"STATIC_URL\":\"static\", \"MEDIA_ROOT\":\"media\"} Change REPO_URL, PROJECT_NAME, REMOTE_HOST. Other values are already setup for djangogirls tutorial we followed. Every values must be in “”! Upload and deploy code thorugh Fabric3We can use fabric through fab command. Like this: fab new_server, fab deploy, fab create_superuser. This commands will execute commands on remote server(azure virtual machine which we made) When you use fabric for new server, just type this command and execute: fab new_server. this will install python3, apache2, and mod_wsgi to run django. 1fab new_server When you edit django code and committed &amp; pushed to github, then use fab deploy command. This will fetch latest code on github and migrate db. 1fab deploy When you want to create superuser, just execute fab create_superuser and there’ll be creating superuser prompt. 1fab create_superuser Whoa!You’ve just upload and deploy REAL working web service on Azure! Congratulation!","link":"/2017/06/10/Deploy-Django-to-MS-Azure-EN/"},{"title":"macOS 환경설정에서 Shift Space로 한영 전환하기","text":"이번 가이드는 macOS 10.12 Sierra 기준입니다. 맥을 사용하다 보면 한영 전환키가 계속 바뀌어 불편함을 느끼는 경우가 많습니다. 처음에 알게 된 전환키를 익숙하게 쓰고, 특히 여러 OS를 번갈아가며 사용할 때에는 언어 전환 키를 통일해 사용하는 것이 편리합니다. 물론 맥의 키보드 환경설정에서는 기본적으로 사용자가 원하는 키보드 조합을 통해 언어 전환을 할 수 있습니다. 우선 키보드 전환 설정으로 들어가봅시다. 환경설정에서 키보드로 들어가주세요. 키보드로 들어가신 후 아래 빨간 네모대로 클릭을 해 주세요. 여기서 마지막 네모를 더블클릭하고 키를 입력하면 언어 변경키를 지정할 수 있는데요, 더블클릭 후 Shift+Space를 누르면 아무런 반응이 일어나지 않는걸 보실 수 있습니다. 하지만 사실은 가능합니다! 입력키 변경(입력키 부분 더블클릭시 변경 가능)을 할 때 fn + shift + space를 이용하면 위 사진와 같이 Shift+Space조합으로 이전 언어로 변경하기 키가 사용가능하도록 바뀐 것을 볼 수 있습니다.","link":"/2017/07/03/Use-Shift-Space-for-Change-Language-on-macOS/"},{"title":"멋진 Terminal 만들기","text":"이번 가이드는 macOS를 위한 가이드입니다. 맥을 개발용으로 사용하는 경우 터미널을 좀 더 편리하고 다양하게 사용하는 방법 중 기본 Shell인 bash대신 zsh을 사용하는 경우가 많습니다. 그리고 oh-my-zsh을 함께 사용해 더 많은 기능을 편리하게 깔수 있기도 합니다. 물론 oh-my-zsh의 기본 테마인 robbyrussell도 예쁘지만, 약간 아쉬운 점이 남기도 하죠. 좀 더 예쁘고 사용하고싶어지는 기분이 들도록 agnoster테마를 깔고 Oceanic Next색 테마를 입힌 후 터미널에서 사용하는 명령어가 제대로 쳤는지 확인해주는 zsh-syntax-highlighting를 깔아봅시다. 참, 터미널은 iTerm2라는 멋진 맥용 터미널을 먼저 깔아야 해요. 앞으로 나오는 가이드는 이미 깐 경우 Pass해주시면 됩니다. iTerm2 설치하기 우선 iTerm2 다운로드 페이지에 들어가서 iTerm2를 받아주세요. Stable Releases중 최신 버전을 받아주세요. 다운 받은 후 압축을 풀면 iTerm2라는 맥 앱이 생길거에요. 맥 파인더에서 ‘응용 프로그램’으로 iTerm2를 옮겨주세요. HomeBrew 설치하기 우분투의 APT와 비슷하게 프로그램 패키지를 관리해 주는 프로그램이 있습니다. 바로 HomeBrew라는 프로그램인데요, brew라는 명령어로 패키지를 관리할 수 있습니다. HomeBrew 공식 홈페이지는 https://brew.sh/입니다. 아래 명령어를 터미널에 입력해주세요. 1/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" Zsh 설치하기 zsh은 bash에 추가적인 명령어를 추가하고 편의성을 개선한 새로운 쉘입니다. 한가지 예로 git 폴더 상태를 관리해주고 터미널에 상태를 나타내주는 점 등이 있습니다. zsh은 위에서 설치한 brew를 통해 설치할 수 있습니다. 아래 명령어를 터미널에 입력해 주세요. 1brew install zsh OhMyZsh 설치하기 oh-my-zsh은 zsh을 좀 더 편리하게 이용하게 이용해주는 일종의 zsh 플러그인입니다. oh-my-zsh은 아래 명령어를 통해 설치할 수 있습니다. 터미널에 아래 명령어를 입력해주세요. OhMyZsh을 설치하면 기본 쉘을 zsh로 바꾸기 위해 맥 잠금해제 암호를 물어봅니다. 암호 입력시에는 입력해도 *같은 표시는 뜨지 않으니 그냥 입력하고 엔터를 눌러주세요! 1sh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" Oceanic Next iTerm 색 테마 입히기 oceanic-next-iterm을 이용해 터미널의 색 테마를 바꿔봅시다. 우선 master.zip 파일을 받아줍시다. master.zip파일의 압축을 풀어주면 Oceanic-Next.itermcolors파일을 보실 수 있으실텐데요, 이 파일을 더블클릭으로 실행하면 iTerm2의 색 테마에 추가가 됩니다. iTerm2를 실행하고 맥 화면 상단 좌측의 사과 아이콘 옆 iTerm2를 누르고 나오는 메뉴 중 ‘Preferences…’를 눌러주세요. 이제 위 스크린샷처럼 Profiles &gt; Default &gt; Colors &gt; Color Presets… &gt; Oceanic-Next 로 차례대로 눌러주신 후 iTerm2를 껐다가 켜면 적용이 완료되어있을 거랍니다. Agnoster 테마 설치하기 이제 우리 zsh의 테마를 위 스크린샷처럼 Agnoster테마로 바꿔줍시다! 텍스트 편집기(vi, sublimetext3, atom등)로 ~/.zshrc파일을 열어주세요. 만약 .zshrc파일에 특별한 수정을 하지 않았다면 10번째줄 처럼 ZSH_THEME를 설정하는 코드가 보일거에요. 이 줄을 아래 스크린 샷처럼 바꿔주세요. 1ZSH_THEME=\"agnoster\" 이제 새 탭을 열면 Agnoster테마로 바뀐 쉘이 보일거에요! 하지만 이렇게 하면 폰트가 일부 깨진답니다. 아래 Ubuntu Mono Powerline폰트를 받아 설정을 진행해주세요. Ubuntu Mono derivative Powerline 폰트 설치 &amp; 설정하기 우선 Ubuntu_Mono_derivative_Powerline.ttf를 다운받아 서체 설치를 진행해 주세요. 서체 설치가 완료되면 iTerm2를 다시 실행해 주세요. 주의: 서체 설치 전 iTerm2이 켜져있었다면 완전히 종료 후 다시 켜 주세요. 켜져있는 상태에서 설정에 들어간다면 설치한 폰트가 뜨지 않을 수 있습니다! 위에서 Oceanic Next 테마를 설치한 것과 같이 위 스크린샷처럼 Preferences..에 들어가 주세요. 그리고 위 사진처럼 Profiles &gt; Default &gt; Text &gt; ChangeFont를 눌러주세요. 그러면 아래와 같은 창이 뜹니다. 시스템에 깔린 모든 폰트가 나오기 때문에 ‘고정폭’을 먼저 선택하고 폰트를 선택해 주세요. zsh-syntax-highlighting 설치하기 이제 시스템의 PATH에 등록된 명령어들을 자동으로 Syntax HighLighting을 해주는 zsh-syntax-highlighting를 설치해 봅시다. 아래 명령어 두줄을 터미널에 입력해 주세요. 12git clone https://github.com/zsh-users/zsh-syntax-highlighting.gitecho \"source ${(q-)PWD}/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\" &gt;&gt; ${ZDOTDIR:-$HOME}/.zshrc 수고하셨습니다 :D이제 모두 끝났습니다!! iTerm2를 완전히 종료한 후 다시 실행해 보면 잘 작동되는 것을 보실 수 있으실거에요!","link":"/2017/07/07/Beautify-ZSH/"},{"title":"나만의 웹 크롤러 만들기(6): N배빠른 크롤링, multiprocessing!","text":"좀 더 보기 편한 깃북 버전의 나만의 웹 크롤러 만들기가 나왔습니다! 이전 가이드: 나만의 웹 크롤러 만들기(5): 웹페이지 업데이트를 알려주는 Telegram 봇 들어가기전..나만의 웹 크롤러 만들기 6번째 가이드는 크롤링을 병렬화를 통해 빠르게 진행하는 방법을 안내합니다. 지금까지 만들어온 크롤러들은 모두 한번에 하나의 요청만을 처리하고 있습니다. 물론 지금까지 따라오셨다면 충분히 크롤러들을 여러분의 의도에 맞게 잘 수정해서 사용해보셨을거라 생각합니다. 하지만 한 페이지만을 여유로운 시간을 갖고 크롤링하는 것이 아니라 여러 사이트 혹은 여러 페이지를 좀더 빠르게 긁어오는 방법에는 역시 N개를 띄우는 방법이 제일 낫다고 볼 수 있습니다. 만약 100만개의 페이지가 있다면 125만/25만150만/50만175만/75만1100만와 같이 4개로 쪼개서 돌린다면 더 빠르게 도는 것은 당연합니다. 하지만 전달해주는 페이지의 목록이나 페이지 숫자에 직접 저 수들을 입력하는 것은 상당히 귀찮은 일이기도 하고, 크롤링이 ‘자동화’를 위한 것이라는 면에 반하는 측면도 있습니다. 따라서 우리는 Python 자체에 내장된 병렬화 모듈을 사용할 예정입니다. Python을 이용할 때 프로그램을 병렬적으로 처리하는 방법은 여러가지가 있습니다. 하지만 우리가 하는 일은 연산이 아니고 IO와 네트워크가 가장 큰 문제이기 때문에 multiprocessing을 사용합니다. threading 모듈도 사용 가능합니다. 하지만 multiprocessing모듈을 추천합니다. threading모듈은 싱글 프로세스 안의 스레드에서 동작하지만 이로인해 GIL의 제약에 걸리는 경우가 생기기 때문에 성능 향상에 제약이 있습니다. (물론 우리는 CPU 연산보다 IO/네트워크로 인한 지연이 훨씬 크기 때문에 큰 차이는 없습니다.) 멀티프로세스란?프로세스란 ‘실행 중인 프로그램’을 의미합니다. 간단하게 말해 멀티프로세스는 프로세스를 여러개 띄우는 것, 즉 프로그램을 여러개 실행시키는 것이라고 볼 수 있습니다. Python에는 멀티프로세싱 프로그램을 위한 모듈이 multiprocessing이라는 이름으로 내장되어 있습니다. 가장 단순한 예시로, 임의의 숫자 리스트 (ex: [20, 25, 30, 35])를 받고 그 자리의 피보나치 수를 구해주는 프로그램이 있다고 가정해 봅시다. 만약 for문을 통해 리스트를 순회하며 계산한다면 아래와 같이 코드를 짤 수 있습니다. 주의: 명백한 코드 실행 시간 차이를 보여주기 위한 느린 코드입니다. 12345678910111213141516171819import timestart_time = time.time()def fibo(n): # 회귀적 피보나치 함수 if n == 0: return 0 elif n == 1: return 1 else: return fibo(n-1) + fibo(n-2)num_list = [31, 32, 33, 34]result_list = []for num in num_list: result_list.append(fibo(num))print(result_list)print(\"--- %s seconds ---\" % (time.time() - start_time)) 실행을 해보면 다음과 같이 약 7s가 걸리는 것을 볼 수 있습니다. 너무 오래걸리네요. 그렇다면 multiprocessing을 이용하면 어떨까요? 12345678910111213141516171819202122from multiprocessing import Poolimport timestart_time = time.time()def fibo(n): if n == 0: return 0 elif n == 1: return 1 else: return fibo(n-1) + fibo(n-2)def print_fibo(n): # 피보나치 결과를 확인합니다. print(fibo(n))num_list = [31, 32, 33, 34] pool = Pool(processes=4) # 4개의 프로세스를 사용합니다.pool.map(print_fibo, num_list) # pool에 일을 던져줍니다.print(\"--- %s seconds ---\" % (time.time() - start_time)) 실행을 해 봅시다! 확실히 빨라진 모습을 볼 수 있습니다. 3s, 즉 절반정도로 줄어든 것을 확인 할 수 있습니다. 크롤링 병렬화 하기이번 가이드에서는 위에서 사용한 모듈인 multiprocessing을 이용해 진행합니다. 크롤링은 위에서 사용한 것처럼 연산집중적이지 않습니다. 그래서 크롤링하는 함수를 만들어두었다면 그 함수를 그대로 사용하시면 됩니다. 이번 가이드에서는 첫번째 가이드에서 사용했던 함수를 약간 변형해 사용해 봅니다. 하나씩 크롤링 하기123456789101112131415161718192021222324252627282930313233# parser.pyimport requestsfrom bs4 import BeautifulSoup as bsimport timedef get_links(): # 블로그의 게시글 링크들을 가져옵니다. req = requests.get('https://beomi.github.io/beomi.github.io_old/') html = req.text soup = bs(html, 'html.parser') my_titles = soup.select( 'h3 &gt; a' ) data = [] for title in my_titles: data.append(title.get('href')) return datadef get_content(link): abs_link = 'https://beomi.github.io'+link req = requests.get(abs_link) html = req.text soup = bs(html, 'html.parser') # 가져온 데이터로 뭔가 할 수 있겠죠? # 하지만 일단 여기서는 시간만 확인해봅시다. print(soup.select('h1')[0].text) # 첫 h1 태그를 봅시다.if __name__=='__main__': start_time = time.time() for link in get_links(): get_content(link) print(\"--- %s seconds ---\" % (time.time() - start_time)) 약 7.3s가 걸리는 것을 아래에서 확인해 볼 수 있습니다. Multiprocessing으로 병렬 크롤링하기이제 좀더 빠른 크롤링을 위해 병렬화를 도입해 봅시다. multiprocessing에서 Pool을 import 해 줍시다. 그리고 pool에 get_content함수를 넣어 줍시다. 1234567891011121314151617181920212223242526272829303132333435# parser.pyimport requestsfrom bs4 import BeautifulSoup as bsimport timefrom multiprocessing import Pool # Pool import하기def get_links(): # 블로그의 게시글 링크들을 가져옵니다. req = requests.get('https://beomi.github.io/beomi.github.io_old/') html = req.text soup = bs(html, 'html.parser') my_titles = soup.select( 'h3 &gt; a' ) data = [] for title in my_titles: data.append(title.get('href')) return datadef get_content(link): abs_link = 'https://beomi.github.io'+link req = requests.get(abs_link) html = req.text soup = bs(html, 'html.parser') # 가져온 데이터로 뭔가 할 수 있겠죠? # 하지만 일단 여기서는 시간만 확인해봅시다. print(soup.select('h1')[0].text) # 첫 h1 태그를 봅시다.if __name__=='__main__': start_time = time.time() pool = Pool(processes=4) # 4개의 프로세스를 사용합니다. pool.map(get_content, get_links()) # get_contetn 함수를 넣어줍시다. print(\"--- %s seconds ---\" % (time.time() - start_time)) 약 2.8초로 약 3~4배의 속도 향상이 있었습니다. 마무리 및 팁멀티프로세싱으로 크롤링을 할 때 유의할 점은 Pool을 생성시 processes의 개수가 많다고 빠르지는 않다는 점을 유의하셔야 합니다. 두번째 parser.py파일을 실행 시 process를 4개인 경우 2.8s, 8개로 할 때 1.85s, 16개로 할 때 0.96s, 32개로 할 때 0.63s로 속도 향상이 두드러집니다. 하지만 64개로 할 때는 오히려 1.30s로 속도 지연이 발생합니다. 프로세스는 CPU코어(Hyper-Thread인 경우 2배) 개수의 2배(ex: 4코어 i5는 8개, 4코어8스레드인 i7은 16개)로 하면 가장 빠르지는 않지만 적당히 빠른 속도를 가져다줍니다. 다음으로는 웹 사이트에서 이러한 공격적 크롤링을 차단할 수 있다는 문제입니다. 잘 관리되는 사이트의 경우 이와 같은 공격적 크롤링은 사실 시스템 관리자에게 있어서는 공격 시도와 같이 보일 수 있기 때문에 적당한 속도를 유지하며 robots.txt를 존중해주는 것이 중요합니다. 무작정 빠르게 긁는다고 빠르지 않은 점에는 이 코드를 실행하는 컴퓨터의 네트워크 환경 자체가 제약이 되기도 합니다. 만약 핫스팟이나 테더링을 이용하거나 인터넷 속도에 제약을 받는 환경에서 이러한 작업을 돌린다면 오히려 네트워크쪽 문제로 인해 에러나 지연이 발생할 가능성이 높습니다. 이런 경우에는 processes개수를 2~4개로 맞춰서 크롤링을 진행하는 것이 최적의 속도를 이끌어낼 수도 있습니다. 다음 가이드에서는 실제 명령어를 받아 사용자들의 정보를 저장하고 사용자들에게 실제로 유용한 정보(예: 글 제목과 링크)를 보내주는 내용으로 진행할 예정입니다.","link":"/2017/07/05/HowToMakeWebCrawler-with-Multiprocess/"},{"title":"자바스크립트: function declaration와 Arrow Function의 this 스코프 차이","text":"이번 포스팅은 ES6 JavaScript 대상입니다. 자바스크립트가 ES6로 개정되며 새로 들어온 것 중 Arrow Function이라는 것이 있습니다. () =&gt; {}의 모양을 갖고 있고 동작하는 것도 비슷하게 보입니다. 하지만 기존의 function() {} 함수형태를 1:1로 바로 변환할 수 있는 것은 아닙니다. this, arguments의 바인딩이 다르다.Arrow Function은 this 바인딩을 갖지 않습니다. 기존의 function에서 this의 탐색 범위가 함수의 {} 안에서 찾은 반면 Arrow Function에서 this는 일반적인 인자/변수와 동일하게 취급됩니다. 따라서 아래와 같은 상황이 발생합니다. 123456789101112// function(){}방식으로 호출할 때function objFunction() { console.log('Inside `objFunction`:', this.foo); return { foo: 25, bar: function() { console.log('Inside `bar`:', this.foo); }, };}objFunction.call({foo: 13}).bar(); // objFunction의 `this`를 오버라이딩합니다. 위 결과는 아래와 같습니다. 12Inside `objFunction`: 13 // 처음에 인자로 전달한 값을 받음Inside `bar`: 25 // 자신이 있는 Object를 this로 인지해서 25를 반환 우리가 기대한 그대로 나옵니다. 하지만 Arrow Function을 실행하면 이야기가 약간 달라집니다. 12345678910// Arrow Function방식으로 호출할 때function objFunction() { console.log('Inside `objFunction`:', this.foo); return { foo: 25, bar: () =&gt; console.log('Inside `bar`:', this.foo), };}objFunction.call({foo: 13}).bar(); // objFunction의 `this`를 오버라이딩합니다. 위 코드의 결과는 아래와 같습니다. 12Inside `objFunction`: 13 // 처음에 인자로 전달한 값을 받음Inside `bar`: 13 // Arrow Function에서 this는 일반 인자로 전달되었기 때문에 이미 값이 13로 지정됩니다. 즉, Arrow Function 안의 this는 objFunction의 this가 됩니다. 그리고 이 ArrowFunction은 this의 Scope를 바꾸고 싶지 않을 때 특히 유용합니다. 12345678910// ES5 function에서는 `this` Scope가 function안에 들어가면 변하기 때문에 새로운 변수를 만들어 씁니다.var someVar = this;getData(function(data) { someVar.data = data;});// ES6 Arrow Function에서는 `this` Scope의 변화가 없기 때문에 `this`를 그대로 사용하면 됩니다.getData(data =&gt; { this.data = data;}); 이와 같이 Arrow Function에서는 .bind method와 .call method를 사용할 수 없습니다. 즉, 비슷하게 보이지만 실제로 동작하는 것이 다르기 때문에 사용하는 때를 구별하는 것이 필요합니다. Arrow Function은 new로 호출할 수 없다ES6에서 함수는 callable한 것과 constructable한 것의 차이를 두고 있습니다. 만약 어떤 함수가 constructable하다면 new로 만들어야 합니다. 반면 함수가 callable하다면 일반적인 함수처럼 함수()식으로 호출하는 것이 가능합니다. function newFunc() {}와 const newFunc = function() {}와 같은 방식으로 만든 함수는 callable하며 동시에 constructable합니다. 하지만 Arrow Function(() =&gt; {})은 callable하지만 constructable하지 않기때문에 호출만 가능합니다. ps. ES6의 class는 constructable하지만 callable하지 않습니다. 정리함수 정의 방식을 바꿔서 사용할 수 있는 경우는 다음과 같습니다. this나 arguments를 사용하지 않는 경우 .bind(this)를 사용하는 경우 함수 정의 방식을 바꿔서 사용할 수 없는 경우는 다음과 같습니다. new등을 사용하는 constructable한 함수 prototype에 덧붙여진 함수나 method들(보통 this를 사용합니다.) arguments를 함수의 인자로 사용한 경우","link":"/2017/07/12/understanding_js_scope_function_vs_arrow/"},{"title":"Ubuntu Locale 한글로 바꾸기","text":"이번 가이드는 Ubuntu 12/14/16에 적용되는 가이드입니다. Locale이란?Locale이란 세계 각 나라에서 가지고 있는 언어, 날짜, 시간 등에 관해 i18n(국제화)를 통해 같은 프로그램이더라도 OS별로 설정되어있는 것에 따라 어떤 방식으로 출력할지 결정하게 되는 것을 말합니다. Locale은 단순히 언어 번역뿐만 아니라 시간과 날짜등을 표시하는 형태도 결정하게 되는데요, 예를들어 한국에서 2017년 7월 10일이라고 표현한다면 미국에서 07/10/2017와 같은 형식으로 표현할 수도 있는 것이죠. 영국이라면 10/07/2017이라고 표현할 수 있는 것 처럼요. 이와 같이 프로그래머가 한 코드에서 각 국가와 언어권에 맞도록 출력 형태를 결정하도록 OS에서 안내해 주는 것이 Locale입니다. 한국의 Locale한국의 Locale은 보통 ko_KR.UTF-8로 사용합니다. 만약 많이 오래된 서버라면 ko_KR.EUC-KR일 수도 있어요. Ubuntu의 기본 Locale만약 여러분이 AWS나 Vultr등의 외국 회사에서 제공하는 우분투 이미지를 사용하고 있다면 아마 기본 설정은 en-US.UTF-8일 가능성이 큽니다. 그리고 만약 여러분이 미국권에서 사용하는 형식에 익숙하다면 (그리고 프로그램에서도 Locale이슈가 없다면) 이 설정을 굳이 한글로 바꾸실 필요는 없습니다. 하지만 가끔 업체마다 Locale정보를 공란으로 둔 이미지를 제공하는 경우가 있습니다. 그런 경우 기본값으로 한국어 UTF-8을 이용하는 것은 나쁘지 않은 선택입니다. Ubuntu에 Locale변경하기우선 여러분의 우분투에 깔린 Locale을 확인하려면 아래와 같은 명령어를 입력하면 됩니다: 1locale 우분투에서 Locale을 변경하는 방법은 아래와 같습니다. 먼저 한글 패키지를 설치해 주세요.(이미 깔려있을수도 있습니다.) 1sudo apt-get install language-pack-ko 그 다음으로는 locale-gen을 통해 Locale을 설치해 줍시다. 1sudo locale-gen ko_KR.UTF-8 다음으로 dpkg-reconfigure을 이용하는 방법입니다. 아래와 같이 명령어를 쳐 주시고 나오는 화면에서 ko_KR.UTF-8을 스페이스로 선택(*모양이 뜨면 선택된 것입니다)후 엔터를 눌러 설정을 마무리 해 주세요. 1sudo dpkg-reconfigure locales 마지막으로 update-locale으로 시스템 LANG설정을 업데이트 해 줍시다. 1sudo update-locale LANG=ko_KR.UTF-8 LC_MESSAGES=POSIX 이 방법을 사용하면 시스템에서 자동으로 LANG에 지정된 한국어 UTF-8로 Locale세팅을 마무리해 줍니다. 다른 방법으로는 직접 시스템 파일을 수정해주는 방법이 있습니다. /etc/default/locale 파일을 수정하는 것인데요, nano나 vim등으로 아래와 같이 내용을 수정해주시면 됩니다. 12LANG=ko_KR.UTF-8LC_MESSAGES=POSIX 끝났어요!이 세 가지 방법 모두 시스템에 로그아웃 후 SSH로 재 접속시 적용됩니다. (서버를 Reboot하는 것도 괜찮습니다.)","link":"/2017/07/10/Ubuntu-Locale-to-ko_KR/"},{"title":"서브라임텍스트 터미널에서 실행하기(macOS)","text":"이번 가이드는 macOS 용입니다. 들어가기 전프로그램의 설치 경로macOS에 프로그램을 설치하면 기본적으로 /Users/유저이름/Library(~/Library와 같음)에 위치합니다. 사용자의 Library에 저장되는 이 경로는 root 권한 없이도 응용프로그램을 추가하거나 제거하는 것이 가능합니다. 터미널에서 실행되는 경로, PATH대부분의 운영체제에서는 PATH가 있습니다. 그리고 이 PATH에 등록된 경로는 시스템 전역에서 호출 가능한 위치가 됩니다. 예를들어 PATH에 등록된 경로 중 /usr/local/bin폴더가 있었다면 아래와 같이 python명령어를 실행할 경우 /usr/local/bin/python에 있는 파이썬이 실행됩니다. 12➜ ~ which python/usr/local/bin/python 그리고 기본적으로 /usr/local/bin폴더는 사용자 터미널의 PATH에 등록되어있습니다. 따라서 SublimeText3(혹은 2) 실행 프로그램을 ln -s명령어를 통해 /usr/local/bin폴더에 심볼릭 링크를 걸어줘야 합니다. 심볼릭 링크란? 파일을 이동하지 않고 어떤 위치에 바로가기를 하나 더 만드는 것입니다. 윈도우의 바로가기 아이콘과 비슷하다고 생각하시면 됩니다. SublimeText가 깔렸는지 확인하기최신 버전의 sublimetext는 3버전입니다. 하지만 2버전도 유사한 방식으로 사용할 수 있습니다. 우선 아래 명령어를 터미널에 입력할 경우 실행이 되는지 확인해보세요. 코드를 그대로 복사해서 사용하세요! 1234# SublimeText3 의 경우open /Applications/Sublime\\ Text.app/Contents/SharedSupport/bin/subl# SublimeText2 의 경우open /Applications/Sublime\\ Text\\ 2.app/Contents/SharedSupport/bin/subl 위 명령어를 쳐서 서브라임 텍스트 창이 뜬 경우 아래 가이드를 따라가 주시면 됩니다. 심볼릭 링크 등록하기우리는 subl이라는 명령어로 서브라임 텍스트 프로그램을 실행할 계획입니다. 터미널에 아래와 같은 명령어를 입력해 주세요. SublimeText3인 경우: 1ln -s /Applications/Sublime\\ Text.app/Contents/SharedSupport/bin/subl /usr/local/bin/subl SublimeText2인 경우: 1ln -s /Applications/Sublime\\ Text\\ 2.app/Contents/SharedSupport/bin/subl /usr/local/bin/subl 모두 끝났습니다!이제 터미널에서 subl이라는 명령어를 통해 서브라임 텍스트를 실행할 수 있습니다 :) 그냥 서브라임텍스트를 실행하려면 1subl 현 폴더를 열려면 1subl . 이와 같이 subl뒤에 파일/폴더등을 인자로 전달해 줄 수 있습니다. 만약 위 방식으로 되지 않으신다면 .zshrc이나 .bashrc등의 파일에 아래와 같이 입력해 주세요. 1alias subl=\"open -a /Applications/Sublime\\ Text.app\"","link":"/2017/07/04/Call-Sublime-from-Terminal/"},{"title":"Autoenv로 편리한 개발하기","text":"이번 가이드는 macOS를 대상으로 합니다. 프로젝트를 여러가지를 동시에 진행하고 프로젝트에서 사용하는 개발환경이 다양해지다 보니 사용하게 되는 도구들이 많습니다. Python에서는 virtualenv, pyenv등이 대표적이고 Node.js에서는 nvm이나 n등이 대표적인 사례입니다. 즉 시스템에 전역으로 설치되어있는 것과 다른 버전 혹은 다른 패키지들이 설치된 가상환경에서 개발을 진행해 각 프로젝트별로 다른 환경에서 개발을 진행합니다. 하지만 이러한 도구들을 사용하기 위해서는 프로젝트를 실행하기 전 특별한 명령어들(ex: workon venv_name등)을 사용해야 합니다. Autoenv는 이러한 명령어들을 각 프로젝트 폴더 진입시 자동으로 실행할 수 있도록 도와줍니다. Autoenv가 작동하는 방법Autoenv는 시스템의 cd명령어를 바꿔, 폴더 안에 진입한 후 폴더 안에 .env파일이 있는지를 탐색하고 만약 .env파일이 있으면 그 파일을 한줄한줄 사람이 터미널에 치듯 실행새줍니다. 예를 들어, hello라는 폴더 안의 .env에 아래와 같이 되어있다고 가정해 봅시다. 12# .env파일echo \"Hello World!\" 이후 이 hello폴더에 진입할 때마다 Hello World!가 출력됩니다. 123~ $ cd helloHello World!~/hello $ 이처럼 여러가지 방법으로 이용할 수 있습니다. Autoenv 설치하기Autoenv는 다음 두 절차를 통해 쉽게 설치할 수 있습니다. 우선 brew로 설치해 줍시다. (HomeBrew는 brew.sh에서 설치할 수 있습니다.) 1brew install autoenv 다음으로는 autoenv 실행 스크립트를 .zshrc나 .bash_profile 파일의 끝부분에 적어줍시다. 12# .zshrc 나 .bash_profile 의 파일 가장 끝source /usr/local/opt/autoenv/activate.sh 만약 아직 ZSH을 설치하지 않았다면 멋진 Terminal 만들기을 읽어보세요! 유의사항 .env파일 설정 후 첫 폴더 진입시 .env파일을 신뢰하고 실행할지 않을 지에 대한 동의가 나타납니다. 이 부분은 .env파일이 악의적으로 변경되었을때 사용자에게 알리기 위해서 있기 때문에 즐거운 마음으로 Y를 눌러줍시다. SSH키파일 등록하기SSH키파일을 .bash_profile등에 등록해 터미널이 켜질때마다 불러오는 방법도 있지만, 그 대신 ssh-add명령어를 통해 직접 현재 터미널에만 제한적으로 불러오는 방법이 있습니다. 만약 ~/.ssh폴더 안에 my_key_file.pem이라는 키 파일들이 있다면 아래와 같이 .env를 구성할 수 있습니다. 12# .env파일ssh-add ~/.ssh/my_key_file.pem 이와 같이 구성하면 폴더에 진입시마다 아래와 같이 키 파일이 등록된다는 것을 확인할 수 있습니다. 123~ $ cd projectIdentity added: /Users/beomi/.ssh/my_key_file.pem (/Users/beomi/.ssh/my_key_file.pem)~/project $ Python 가상환경 관리하기venv를 사용할 경우파이썬 3.4이후부터 내장된 venv를 이용한 경우 다음과 같이 .env를 구성할 수 있습니다. 12# .env파일source ./가상환경폴더이름/bin/activate virtualenv를 이용할 경우venv와 동일합니다. 아래와 같이 .env를 구성해 주세요. 12# .env파일source ./가상환경폴더이름/bin/activate virtualenv-wrapper를 이용중인 경우workon명령어를 그대로 사용할 수 있습니다. 아래와 같이 .env를 설정해 주세요. (저는 이 방법을 사용하고 있습니다.) 12# .env파일workon 가상환경이름 Pyenv를 이용중인 경우pyenv에서는 local이라는 명령어를 통해 기본적으로 폴더별 Python 버전을 관리해 줍니다. 따라서 .env를 통해 Global설정을 하는 경우를 제외하면 사용하지 않는 것을 추천합니다. Node.js 개발환경 관리하기n을 사용할 경우node버전을 관리해 주는 n은 sudo권한을 필요로 합니다. 시스템 전역에서 사용하는 node의 버전을 변경하기 때문입니다. 그래서 패스워드를 입력해 주는 과정이 필요할 수 있습니다. .env파일을 아래와 같이 만들어 주세요. 1sudo n latest # 버전은 사용 환경에 맞게 입력해 주세요. 마무리사실 Python을 주력 언어로 사용하다 보니 다른 언어들에 대해 언급은 적은 측면이 있습니다. 하지만 Autoenv 자체가 굉장히 심플한 스크립트로 이루어져 있기 때문에 필요에 맞춰 바꾸어 사용하는 것도 방법중 하나라고 생각합니다.","link":"/2017/07/16/Use-Autoenv/"},{"title":"나만의 웹 크롤러 만들기(7): 창없는 크롬으로 크롤링하기","text":"좀 더 보기 편한 깃북 버전의 나만의 웹 크롤러 만들기가 나왔습니다! 이번 가이드는 가이드 3편(Selenium으로 무적 크롤러 만들기)의 확장편입니다. 아직 selenium을 이용해보지 않은 분이라면 먼저 저 가이드를 보고 오시는걸 추천합니다. HeadLess Chrome? 머리없는 크롬?HeadLess란?Headless라는 용어는 ‘창이 없는’과 같다고 이해하시면 됩니다. 여러분이 브라우저(크롬 등)을 이용해 인터넷을 브라우징 할 때 기본적으로 창이 뜨고 HTML파일을 불러오고, CSS파일을 불러와 어떤 내용을 화면에 그러야 할지 계산을 하는 작업을 브라우저가 자동으로 진행해줍니다. 하지만 이와같은 방식을 사용할 경우 사용하는 운영체제에 따라 크롬이 실행이 될 수도, 실행이 되지 않을 수도 있습니다. 예를들어 우분투 서버와 같은 OS에서는 ‘화면’ 자체가 존재하지 않기 때문에 일반적인 방식으로는 크롬을 사용할 수 없습니다. 이를 해결해 주는 방식이 바로 Headless 모드입니다. 브라우저 창을 실제로 운영체제의 ‘창’으로 띄우지 않고 대신 화면을 그려주는 작업(렌더링)을 가상으로 진행해주는 방법으로 실제 브라우저와 동일하게 동작하지만 창은 뜨지 않는 방식으로 동작할 수 있습니다. 그러면 왜 크롬?일전 가이드에서 PhantomJS(팬텀)라는 브라우저를 이용하는 방법에 대해 다룬적이 있습니다. 팬텀은 브라우저와 유사하게 동작하고 Javascript를 동작시켜주지만 성능상의 문제점과 크롬과 완전히 동일하게 동작하지는 않는다는 문제점이 있습니다. 우리가 크롤러를 만드는 상황이 대부분 크롬에서 진행하고, 크롬의 결과물 그대로 가져오기 위해서는 브라우저도 크롬을 사용하는 것이 좋습니다. 하지만 여전히 팬텀이 가지는 장점이 있습니다. WebDriver Binary만으로 추가적인 설치 없이 환경을 만들 수 있다는 장점이 있습니다. 윈도우 기준 크롬 59, 맥/리눅스 기준 크롬 60버전부터 크롬에 Headless Mode가 정식으로 추가되어서 만약 여러분의 브라우저가 최신이라면 크롬의 Headless모드를 쉽게 이용할 수 있습니다. 크롬 버전 확인하기크롬 버전 확인은 크롬 브라우저에서 chrome://version/로 들어가 확인할 수 있습니다. 이와 같이 크롬 버전이 60버전 이상인 크롬에서는 ‘Headless’모드를 사용할 수 있습니다. 크롬드라이버(chromedriver) 업데이트크롬 버전이 올라감에 따라 크롬을 조작하도록 도와주는 chromedriver 역시 함께 업데이트를 진행해야 합니다. https://sites.google.com/a/chromium.org/chromedriver/downloads 위 링크에서 Latest Release 옆 크롬드라이버를 선택해 OS별로 알맞은 zip파일을 받아 압축을 풀어줍시다. 기존 코드 수정하기크롬의 헤드리스 모드를 사용하는 방식은 기존 selenium을 이용한 코드와 거의 동일합니다만, 몇가지 옵션을 추가해줘야합니다. 기존에 webdriver를 사용해 크롬을 동작한 경우 아래와 같은 코드를 사용할 수 있었습니다. 1234567891011from selenium import webdriver# 유의: chromedriver를 위에서 받아준 # chromdriver(windows는 chromedriver.exe)의 절대경로로 바꿔주세요!driver = webdriver.Chrome('chromedriver')driver.get('http://naver.com')driver.implicitly_wait(3)driver.get_screenshot_as_file('naver_main.png')driver.quit() 위 코드를 동작시키면 크롬이 켜지고 파이썬 파일 옆에 naver_main.png라는 스크린샷 하나가 생기게 됩니다.이 코드는 지금까지 우리가 만들었던 코드와 큰 차이가 없는걸 확인해 보세요. 하지만 이 코드를 몇가지 옵션만 추가해주면 바로 Headless모드로 동작하게 만들어줄 수 있습니다. 123456789101112131415from selenium import webdriveroptions = webdriver.ChromeOptions()options.add_argument('headless')options.add_argument('window-size=1920x1080')options.add_argument(\"disable-gpu\")# 혹은 options.add_argument(\"--disable-gpu\")driver = webdriver.Chrome('chromedriver', chrome_options=options)driver.get('http://naver.com')driver.implicitly_wait(3)driver.get_screenshot_as_file('naver_main_headless.png')driver.quit() 위 코드를 보시면 ChromeOptions()를 만들어 add_argument를 통해 Headless모드인 것과, 크롬 창의 크기, 그리고 gpu(그래픽카드 가속)를 사용하지 않는 옵션을 넣어준 것을 볼 수 있습니다. 제일 중요한 부분은 바로 options.add_argument('headless')라는 부분입니다. 크롬이 Headless모드로 동작하도록 만들어주는 키워드에요. 그리고 크롬 창의 크기를 직접 지정해 준 이유는, 여러분이 일반적으로 노트북이나 데스크탑에서 사용하는 모니터의 해상도가 1920x1080이기 때문입니다. 즉, 여러분이 일상적으로 보는 것 그대로 크롬이 동작할거라는 기대를 해볼수 있습니다! 마지막으로는 disable-gpu인데요, 만약 위 코드를 실행했을때 GPU에러~가 난다면 --disable-gpu로 앞에 dash(-)를 두개 더 붙여보세요. 이 버그는 크롬 자체에 있는 문제점입니다. 브라우저들은 CPU의 부담을 줄이고 좀더 빠른 화면 렌더링을 위해 GPU를 통해 그래픽 가속을 사용하는데, 이 부분이 크롬에서 버그를 일으키는 현상을 보이고 있습니다. (윈도우 크롬 61버전까지는 아직 업데이트 되지 않았습니다. 맥 61버전에는 해결된 이슈입니다.) 그리고 driver 변수를 만들 때 단순하게 chromedriver의 위치만 적어주는 것이 아니라 chrome_options라는 이름의 인자를 함께 넘겨줘야 합니다. 이 chrome_options는 Chrome을 이용할때만 사용하는 인자인데요, 이 인자값을 통해 headless등의 추가적인 인자를 넘겨준답니다. 자, 이제 그러면 한번 실행해 보세요. 크롬 창이 뜨지 않았는데도 naver_main_headless.png파일이 생겼다면 여러분 컴퓨터에서 크롬이 Headless모드로 성공적으로 실행된 것이랍니다! Headless브라우저임을 숨기기Headless모드는 CLI기반의 서버 OS에서도 Selenium을 통한 크롤링/테스트를 가능하게 만드는 멋진 모드지만, 어떤 서버들에서는 이런 Headless모드를 감지하는 여러가지 방법을 쓸 수 있습니다. 아래 글에서는 Headless모드를 탐지하는 방법과 탐지를 ‘막는’방법을 다룹니다.(창과 방패, 또 새로운 창!) 아래 코드의 TEST_URL은 https://intoli.com/blog/making-chrome-headless-undetectable/chrome-headless-test.html 인데요, 이곳에서 Headless모드가 감춰졌는지 아닌지 확인해 볼 수 있습니다. User Agent 확인하기Headless 탐지하기가장 쉬운 방법은 User-Agent값을 확인하는 방법입니다. 일반적인 크롬 브라우저는 아래와 같은 User-Agent값을 가지고 있습니다. 1Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36 하지만 Headless브라우저는 아래와 같은 User-Agent값을 가지고 있습니다. 잘 보시면 ‘HeadlessChrome/~~’와 같이 ‘Headless’라는 단어가 들어가있는걸 확인할 수 있습니다! 1Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/60.0.3112.50 Safari/537.36 Headless 탐지 막기따라서 기본적으로 갖고있는 User-Agent값을 변경해줘야합니다. 이것도 위에서 사용한 chrome_options에 추가적으로 인자를 전달해주면 됩니다. 위코드를 약간 바꿔 아래와 같이 만들어보세요. 123456789101112131415161718192021from selenium import webdriverTEST_URL = 'https://intoli.com/blog/making-chrome-headless-undetectable/chrome-headless-test.html'options = webdriver.ChromeOptions()options.add_argument('headless')options.add_argument('window-size=1920x1080')options.add_argument(\"disable-gpu\")# UserAgent값을 바꿔줍시다!options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")driver = webdriver.Chrome('chromedriver', chrome_options=options)driver.get(TEST_URL)user_agent = driver.find_element_by_css_selector('#user-agent').textprint('User-Agent: ', user_agent)driver.quit() 이제 여러분의 Headless크롬은 일반적인 크롬으로 보일거랍니다. 플러그인 개수 확인하기Headless 탐지하기크롬에는 여러분이 따로 설치하지 않아도 추가적으로 플러그인 몇개가 설치되어있답니다. PDF 내장 리더기같은 것들이죠. 하지만 Headless모드에서는 플러그인이 하나도 로딩되지 않아 개수가 0개가 됩니다. 이를 통해 Headless모드라고 추측할 수 있답니다. 아래 자바스크립트 코드를 통해 플러그인의 개수를 알아낼 수 있습니다. 123if(navigator.plugins.length === 0) { console.log(\"Headless 크롬이 아닐까??\");} Headless 탐지 막기물론 이 탐지를 막는 방법도 있습니다. 바로 브라우저에 ‘가짜 플러그인’ 리스트를 넣어주는 것이죠! 아래 코드와 같이 JavaScript를 실행해 플러그인 리스트를 가짜로 만들어 넣어줍시다. 1234567891011121314151617181920212223from selenium import webdriverTEST_URL = 'https://intoli.com/blog/making-chrome-headless-undetectable/chrome-headless-test.html'options = webdriver.ChromeOptions()options.add_argument('headless')options.add_argument('window-size=1920x1080')options.add_argument(\"disable-gpu\")options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")options.add_argument(\"lang=ko_KR\") # 한국어!driver = webdriver.Chrome('chromedriver', chrome_options=options)driver.get('about:blank')driver.execute_script(\"Object.defineProperty(navigator, 'plugins', {get: function() {return[1, 2, 3, 4, 5];},});\")driver.get(TEST_URL)user_agent = driver.find_element_by_css_selector('#user-agent').textplugins_length = driver.find_element_by_css_selector('#plugins-length').textprint('User-Agent: ', user_agent)print('Plugin length: ', plugins_length)driver.quit() 위와 같이 JS로 navigator 객체의 plugins속성 자체를 오버라이딩 해 임의의 배열을 반환하도록 만들어주면 개수를 속일 수 있습니다. 단, 출력물에서는 Plugin length가 여전히 0으로 나올거에요. 왜냐하면 사이트가 로딩 될때 이미 저 속성이 들어가있기 때문이죠 :’( 그래서 우리는 좀 더 다른방법을 뒤에서 써볼거에요. 언어 설정Headless 탐지하기여러분이 인터넷을 사용할때 어떤 사이트를 들어가면 다국어 사이트인데도 여러분의 언어에 맞게 화면에 나오는 경우를 종종 보고, 구글 크롬을 써서 외국 사이트를 돌아다니면 ‘번역해줄까?’ 하는 친절한 질문을 종종 봅니다. 이 설정이 바로 브라우저의 언어 설정이랍니다. 즉, 여러분이 선호하는 언어가 이미 등록되어있는 것이죠. Headless모드에는 이런 언어 설정이 되어있지 않아서 이를 통해 Headless모드가 아닐까 ‘추측’할 수 있습니다. Headless 탐지 막기Headless모드인 것을 감추기 위해 언어 설정을 넣어줍시다. 바로 add_argument를 통해 크롬에 전달해 줄 수 있답니다. 1234567891011121314151617181920212223242526from selenium import webdriverTEST_URL = 'https://intoli.com/blog/making-chrome-headless-undetectable/chrome-headless-test.html'options = webdriver.ChromeOptions()options.add_argument('headless')options.add_argument('window-size=1920x1080')options.add_argument(\"disable-gpu\")options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")options.add_argument(\"lang=ko_KR\") # 한국어!driver = webdriver.Chrome('chromedriver', chrome_options=options)driver.get(TEST_URL)driver.execute_script(\"Object.defineProperty(navigator, 'plugins', {get: function() {return[1, 2, 3, 4, 5]}})\")# lanuages 속성을 업데이트해주기driver.execute_script(\"Object.defineProperty(navigator, 'languages', {get: function() {return ['ko-KR', 'ko']}})\")user_agent = driver.find_element_by_css_selector('#user-agent').textplugins_length = driver.find_element_by_css_selector('#plugins-length').textlanguages = driver.find_element_by_css_selector('#languages').textprint('User-Agent: ', user_agent)print('Plugin length: ', plugins_length)print('languages: ', languages)driver.quit() 단, 출력물에서는 language가 빈칸으로 나올거에요. 왜냐하면 사이트가 로딩 될때 이미 저 속성이 들어가있기 때문이죠 :’( 그래서 우리는 좀 더 다른방법을 뒤에서 써볼거에요. WebGL 벤더와 렌더러Headless 탐지하기여러분이 브라우저를 사용할때 WebGL이라는 방법으로 그래픽카드를 통해 그려지는 방법을 가속을 한답니다. 즉, 실제로 디바이스에서 돌아간다면 대부분은 그래픽 가속을 사용한다는 가정이 기반인 셈이죠. 사실 이 방법으로 차단하는 웹사이트는 거의 없을거에요. 혹여나 GPU가속을 꺼둔 브라우저라면 구별할 수 없기 때문이죠. 위 코드에서 사용해준 disable-gpu옵션은 사실 이 그래픽 가속을 꺼주는 것이에요. 따라서 이부분을 보완해 줄 필요가 있습니다. Headless 탐지 막기가장 쉬운 방법은 크롬이 업데이트되길 기대하고 disable-gpu옵션을 꺼버리는 것이지만, 우선은 이 옵션을 함께 사용하는 방법을 알려드릴게요. 위에서 사용한 script실행방법을 또 써 볼 것이랍니다. 123456789101112131415161718192021222324252627282930from selenium import webdriverTEST_URL = 'https://intoli.com/blog/making-chrome-headless-undetectable/chrome-headless-test.html'options = webdriver.ChromeOptions()options.add_argument('headless')options.add_argument('window-size=1920x1080')options.add_argument(\"disable-gpu\")options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")options.add_argument(\"lang=ko_KR\") # 한국어!driver = webdriver.Chrome('chromedriver', chrome_options=options)driver.get(TEST_URL)driver.execute_script(\"Object.defineProperty(navigator, 'plugins', {get: function() {return[1, 2, 3, 4, 5]}})\")driver.execute_script(\"Object.defineProperty(navigator, 'languages', {get: function() {return ['ko-KR', 'ko']}})\")driver.execute_script(\"const getParameter = WebGLRenderingContext.getParameter;WebGLRenderingContext.prototype.getParameter = function(parameter) {if (parameter === 37445) {return 'NVIDIA Corporation'} if (parameter === 37446) {return 'NVIDIA GeForce GTX 980 Ti OpenGL Engine';}return getParameter(parameter);};\")user_agent = driver.find_element_by_css_selector('#user-agent').textplugins_length = driver.find_element_by_css_selector('#plugins-length').textlanguages = driver.find_element_by_css_selector('#languages').textwebgl_vendor = driver.find_element_by_css_selector('#webgl-vendor').textwebgl_renderer = driver.find_element_by_css_selector('#webgl-renderer').textprint('User-Agent: ', user_agent)print('Plugin length: ', plugins_length)print('languages: ', languages)print('WebGL Vendor: ', webgl_vendor)print('WebGL Renderer: ', webgl_renderer)driver.quit() 위 코드에서는 WebGL렌더러를 Nvidia회사와 GTX980Ti엔진인 ‘척’ 하고 있는 방법입니다. 하지만 WebGL print 구문에서는 여전히 빈칸일거에요. 이 역시 이미 사이트 로딩시 속성이 들어가있기 때문이에요. Headless 브라우저 숨기는 방법 다함께 쓰기위에서 사용한 방법 중 User-Agent를 바꾸는 방법 외에는 사실 모두 Javascript를 이용해 값을 추출하고 오버라이딩 하는 방식으로 바꿔보았습니다. 하지만 번번히 결과물이 빈칸으로 나오는 이유는 driver.execute_script라는 함수 자체가 사이트가 로딩이 끝난 후 (onload()이후) 실행되기 때문입니다. 즉, 우리는 우리가 써준 저 JS코드가 사이트가 로딩 되기 전 실행되어야 한다는 것이죠! 사실 기본 크롬이라면 사이트가 로딩 되기전 JS를 실행하는 Extension들을 사용할 수 있어요. 하지만 Headless크롬에서는 아직 Extension을 지원하지 않습니다 :’( 그래서 차선책으로 mitmproxy라는 Proxy 프로그램을 사용해볼거에요. mitmproxy 사용하기진행하기 전 앞서 만들었던 모든 JS코드가 들어있는 content.js를 아래처럼 만들어주세요. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172const CDP = require('chrome-remote-interface');const userAgent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'CDP(async function(client) { const {Network, Page} = client; await Page.enable(); await Network.enable(); await Network.setUserAgentOverride({userAgent}); // user-agent is now set});// overwrite the `languages` property to use a custom getterObject.defineProperty(navigator, 'languages', { get: function() { return ['ko-KR', 'ko']; },});// overwrite the `plugins` property to use a custom getterObject.defineProperty(navigator, 'plugins', { get: function() { return [1, 2, 3, 4, 5]; }});const getParameter = WebGLRenderingContext.getParameter;WebGLRenderingContext.prototype.getParameter = function(parameter) { // UNMASKED_VENDOR_WEBGL if (parameter === 37445) { return 'Intel Open Source Technology Center'; } // UNMASKED_RENDERER_WEBGL if (parameter === 37446) { return 'Mesa DRI Intel(R) Ivybridge Mobile '; } return getParameter(parameter);};['height', 'width'].forEach(property =&gt; { // store the existing descriptor const imageDescriptor = Object.getOwnPropertyDescriptor(HTMLImageElement.prototype, property); // redefine the property with a patched descriptor Object.defineProperty(HTMLImageElement.prototype, property, { ...imageDescriptor, get: function() { // return an arbitrary non-zero dimension if the image failed to load if (this.complete &amp;&amp; this.naturalHeight == 0) { return 20; } // otherwise, return the actual dimension return imageDescriptor.get.apply(this); }, });});// store the existing descriptorconst elementDescriptor = Object.getOwnPropertyDescriptor(HTMLElement.prototype, 'offsetHeight');// redefine the property with a patched descriptorObject.defineProperty(HTMLDivElement.prototype, 'offsetHeight', { ...elementDescriptor, get: function() { if (this.id === 'modernizr') { return 1; } return elementDescriptor.get.apply(this); },}); 우선 Mitmproxy를 pip로 설치해주세요. 1pip install mitmproxy 그리고 proxy 처리를 해 줄 파일인 inject.py파일을 만들어주세요. 12345678910111213141516171819202122232425# inject.pyfrom bs4 import BeautifulSoupfrom mitmproxy import ctx# load in the javascript to injectwith open('content.js', 'r') as f: content_js = f.read()def response(flow): # only process 200 responses of html content if flow.response.headers['Content-Type'] != 'text/html': return if not flow.response.status_code == 200: return # inject the script tag html = BeautifulSoup(flow.response.text, 'lxml') container = html.head or html.body if container: script = html.new_tag('script', type='text/javascript') script.string = content_js container.insert(0, script) flow.response.text = str(html) ctx.log.info('Successfully injected the content.js script.') 이제 터미널에서 아래 명령어로 mitmproxy 서버를 띄워주세요. 1mitmdump -p 8080 -s &quot;inject.py&quot; 이 서버는 크롤링 코드를 실행 할 때 항상 켜져있어야 해요! 이제 우리 크롤링 코드에 add_argument로 Proxy옵션을 추가해 주세요. 123456789101112131415161718192021222324252627from selenium import webdriverTEST_URL = 'https://intoli.com/blog/making-chrome-headless-undetectable/chrome-headless-test.html'options = webdriver.ChromeOptions()options.add_argument('headless')options.add_argument('window-size=1920x1080')options.add_argument(\"disable-gpu\")options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")options.add_argument(\"proxy-server=localhost:8080\")driver = webdriver.Chrome('chromedriver', chrome_options=options)driver.get(TEST_URL)print(driver.page_source)user_agent = driver.find_element_by_css_selector('#user-agent').textplugins_length = driver.find_element_by_css_selector('#plugins-length').textlanguages = driver.find_element_by_css_selector('#languages').textwebgl_vendor = driver.find_element_by_css_selector('#webgl-vendor').textwebgl_renderer = driver.find_element_by_css_selector('#webgl-renderer').textprint('User-Agent: ', user_agent)print('Plugin length: ', plugins_length)print('languages: ', languages)print('WebGL Vendor: ', webgl_vendor)print('WebGL Renderer: ', webgl_renderer)driver.quit() 하지만 사실 이 코드는 정상적으로 동작하지 않을거에요. 헤드리스모드를 끄면 잘 돌아가지만 헤드리스모드를 켜면 정상적으로 동작하지 않아요. 바로 SSL오류 때문입니다. 크롬에서 SSL을 무시하도록 만들수 있고, 로컬의 HTTP를 신뢰 가능하도록 만들 수도 있지만 아직 크롬 Headless모드에서는 지원하지 않습니다. 정확히는 아직 webdriver에서 지원하지 않습니다. 결론아직까지는 크롬 Headless모드에서 HTTPS 사이트를 ‘완전히 사람처럼’보이게 한뒤 크롤링 하는 것은 어렵습니다. 하지만 곧 업데이트 될 크롬에서는 익스텐션 사용 기능이 추가될 예정이기 때문에 이 기능이 추가되면 복잡한 과정 없이 JS를 바로 추가해 진짜 일반적인 크롬처럼 동작하도록 만들 수 있으리라 생각합니다. 사실 서버 입장에서 위와 같은 요청을 보내는 경우 처리를 할 수 있는 방법은 JS로 헤드리스 유무를 확인하는 방법이 전부입니다. 즉, 서버 입장에서도 ‘식별’은 가능하지만 이로 인해 유의미한 차단은 하기 어렵습니다. 현재로서는 UserAgent 값만 변경해주어도 대부분의 사이트에서는 자연스럽게 크롤링을 진행할 수 있으리라 생각합니다. Reference Detecting Chrome Headless MAKING CHROME HEADLESS UNDETECTABLE","link":"/2017/09/28/HowToMakeWebCrawler-Headless-Chrome/"},{"title":"PDF 책 구글 번역가 도구에서 번역하기","text":"시작하며번역을 할 때 대상이 이미 doc파일같은 것이라면 사실 이 부분은 걱정하지 않아도 괜찮습니다. 하지만 만약 여러분이 책 번역등 의뢰를 받아 진행한다면 PDF로 책을 받을 가능성이 꽤 높습니다.물론 PDF를 켜놓고 word창을 하나 옆으로 두 창을 띄워두며 한글로 번역해도 일이 가능하기는 합니다.하지만 최근 React문서를 번역하며 사용했던 도구인 crowdin이나 Django문서 번역할때 사용하는 transifex를 떠올려보면 이게 무슨 삽질인가, 하는 생각이 듭니다. 그래서 여러분이 번역을 하기 위해 crowdin이나 transifex를 사용하려 사이트에 들어가보면, 월 단위 pricing인 것을 넘어 가격대가 상당히 높게 형성되어있는 것을 볼 수 있습니다. (ㅠㅠ) 저 두 서비스는 분명히 멋지고 좋은 서비스이지만 매달 가격을 지불하기에는 애매한 측면이 있어 다른 방법을 찾아보았습니다. 그러다 찾게 된 것이 바로 구글 번역가 도구였습니다. 이 구글 번역가 도구를 사용하면 상당히 다양한 형식의 문서를 번역할 수 있습니다. 문서 HTML(.HTML) Microsoft Word (.DOC/.DOCX) 일반 텍스트(.TXT) 서식 있는 텍스트(.RTF) 위키백과 URL 광고 애드워즈 에디터 보관 파일 (.AEA) 애드워즈 에디터 공유 파일(.AES) 동영상 YouTube 캡션 SubRip(.SRT) SubViewer(.SUB) 기타 자바 애플리케이션 (.PROPERTIES) 애플리케이션 리소스 번들(.ARB) Chrome 확장 프로그램(.JSON) Apple iOS 애플리케이션(.STRINGS) 하지만 잘 보시면 우리가 받은 PDF파일을 바로 올릴수는 없게 되어있습니다. 자, 이제 ‘구글 번역가 도구에 올릴’ 파일을 만들기 위한 여정을 시작해봅시다. PDF를 쪼개기우리가 구글 번역가 도구에 올릴 최종 파일은 “1MB 이내의 .docx파일” 입니다. 왜 1MB냐고요? 구글이 그렇게 제한을 걸어서 그렇습니다 (ㅠㅠ) 만약 여러분이 1MB가 넘는 파일을 올리려 하시면….. 아래와 같이 에러가 납니다. 하지만 우리가 만약 PDF를 워드로 바꿔주고 나서 쪼개주려고 하면 상당히 귀찮습니다. 그래서 PDF를 먼저 목차대로 쪼개주는 것이 좋습니다. 우리는 https://www.sejda.com/split-pdf-by-outline라는 사이트를 이용할거에요. 사이트를 들어가주시면 아래와 같이 PDF를 올리라고 하는 부분이 나옵니다. 약간 아쉬운점은 무료는 200페이지 이내의 pdf로 제한이 걸린다는 점입니다. 그래서 저는 5달러를 지불하고 5일치 정액권을 구매해 이용했습니다. 무료는 속도 제한이 있기도 하고, 여기에서 여러 서비스를 사용할 것이기 때문에 5일권을 사서 진행하시는 것도 좋은 방법입니다. 물론 무료로 진행할 수 있는 곳도 있지만, 나중에 PDF Crop을 할 때 이 사이트를 또 사용하기 때문에 정액권을 사는 것을 추천합니다.(커피한잔값에 여러분의 정신건강을 지킬 수 있습니다.) PDF를 올려주면 얼만큼 자세하게 쪼갤지 물어봅니다. Bookmark level이 바로 그 옵션인데요, 저는 대제목(챕터)로 자를 예정이라 1을 선택했습니다. 만약 좀 더 자세하게 소제목으로 잘라주고 싶다면 2정도를 선택해주시면 됩니다. 이제 Split by bookmarks를 클릭해주면 아래와 같이 다운로드 버튼이 나옵니다! 다운로드 받은 zip파일을 풀어주면 다음과 같이 챕터별로 잘 쪼개졌다는 것을 확인할 수 있습니다. 하지만 이 상태는 책 각 페이지에 머리말과 꼬리말이 들어가 있어 이 파일을 바로 워드파일로 변환해주면 머리말과 꼬리말이 같이 들어가 번역하기 귀찮은 상태가 됩니다. 그래서 이 부분을 제거해주어야 합니다. 머리말/꼬리말 제거해주기이번에는 https://www.sejda.com/crop-pdf에서 진행합니다. 위에서 정액권을 구매했다면 여러개 파일을 동시에 넣어 crop을 돌릴 수 있습니다. (무료는 하나하나 넣어야 합니다) 위 사진의 Upload PDF Files를 눌러 파일 여러개를 동시에 crop할 수 있습니다. 우리가 위에서 목차대로 잘라준 경우처럼 책 사이즈가 같은 경우 굉장히 유용합니다. 우선 본문인 11~19번 파일만 업로드를 해보았습니다. 업로드가 완료되면 다음과 같이 Crop할 부분을 선택하라고 나옵니다. 문서 일부분이 화면에 겹쳐 나오기때문에 예상치 못하게 버려지는 부분이 생기는 것을 방지할 수 있습니다 :) 위 사진처럼 텍스트 부분만 선택하고 화면 아래의 CropPDF를 눌러주면 위에서와 같이 처리가 끝난 파일의 모음 zip을 받을 수 있습니다. 다운을 받아주고 확인해 봅시다. 글자 부분만 깔끔하게 잘 잘라준 것을 확인해 볼 수 있습니다! 글 일부분이 안보이는 것은 책이라 일부러 잘라 보이지 않는 부분입니다. 파일은 잘 처리되었다는걸 썸네일에서 확인할 수 있죠! PDF를 워드파일(.docx)로 바꿔주기이번에는 pdf2docx라는 서비스를 이용합니다. (무료입니다!) 우리가 방금 만들어준 ‘cropped_어쩌구.pdf’파일들을 업로드 해 주면 됩니다. 여러개 파일을 한번에 올릴 수 있어 편리합니다 ;) 업로드가 끝나고 변환작업이 완료되면 아래와 같이 Download All버튼이 활성화됩니다. 버튼을 누르면 pdf2docx.zip파일이 받아지고, 이 압축 파일을 풀어주면 다음과 같이 .docx파일로 변환된 파일들이 잘 들어오는 것을 확인할 수 있습니다. 하지만 잘 보시면 크기가 1MB를 넘는 파일이 보입니다. 저 파일들은 구글 번역가 도구에 올릴수 없습니다. 보통 문서가 1MB를 넘는 경우는 이미지의 크기가 큰 것이기 때문에, 이미지의 ppi를 조절해 파일 크기를 줄일 수 있습니다. .docx파일 크기 줄이기(이미지 ppi줄이기)1MB가 넘는 한 문서를 열어보니 이미지가 많아 보입니다. 하지만 이미지를 지우면 번역할때 어떤 내용을 다루는지 알아보기 어렵기 때문에 이미지의 해상도(ppi)만 낮춰주도록 하겠습니다. 우선 아래 스샷처럼 아무 이미지나 클릭해주고 나서 화면 위에 뜨는 ‘그림 서식’을 눌러주신 뒤, 핑크색으로 네모 표시 된 버튼을 눌러주세요. 그러면 ‘그림압축’ 메뉴가 뜨고 아래와 같이 그림 품질을 고를 수 있습니다. 최저 ppi인 96ppi로 맞춰주고 ‘잘려진 그림 영역 삭제’에 체크를 눌러주고 ‘이 파일의 모든 그림’으로 맞춰준 후 확인을 눌러주세요. 그리고 저장을 해주시면, 아래와 같이 파일 사이즈가 줄어든 것을 볼 수 있습니다. (기존 1.5MB -&gt; 현재 1MB 조금 덜 됨) 구글 번역가 도구에 업로드하기구글 번역가 도구 업로드에 다시 들어가 작아진 .docx파일을 올려줍시다. 언어 선택에 한국어는 기본적으로 없기 때문에 ‘ko’를 검색해 한국어를 추가하고 선택해줍시다. 이제 업로드가 끝나면 번역 업체를 누르라고 하는데, ‘아니오’를 눌러주면 됩니다. 업로드가 완료되면 아래와 같이 번역 목록에 뜹니다! 링크를 클릭해 들어가면 이제 아래처럼 번역 작업을 시작할 수 있습니다. 끝!자, 이제 PDF파일로 된 책을 구글 번역가 도구에서 번역할 수 있도록 하는 작업이 모두 끝났습니다. 하지만 이 방식으로는 아쉬운 것이 세가지가 있습니다. 책의 포맷을 맞춰주세요:우리가 책을 crop했기 때문에 어렵습니다. 코드가 Indent가 제대로 되지 않아요:pdf to docx는 코드도 일반 문서로 해석합니다. (ㅠㅠ) TM(Translation memory)이 완벽하지 않아요:구글 번역가 도구가 TM관리가 약간 기능이 부족합니다. 그래도 무료잖아요! 하지만 이 세가지를 감안한다면 이 가이드가 유용하실 것이라 생각합니다. 번역하시는 모든 분들 화이팅!","link":"/2017/10/01/PDF_to_GoogleTranslatorToolkit/"},{"title":"로컬 개발서버를 HTTPS로 세상에 띄우기(like ngork)","text":"이번 가이드를 따라가기 위해서는 HTTP(80/tcp) 포트가 열려있는 서버와 개인 도메인이 필요합니다. 들어가기 전django, node.js, react, vue와 같은 웹 개발(Backend &amp; Frontend)을 진행하다보면 모바일 디바이스나 타 디바이스에서 로컬 서버에 접근해야하는 경우가 있습니다. 하지만 보통 개발환경에서는 개발기기가 공인 IP를 갖고 있는것이 아니라 내부 NAT에서 개발이 이루어지고, 웹과 내부 개발기기 사이에는 방화벽이 있습니다. 집에서 개발한다면 공유기가, 회사에서 개발한다면 회사의 라우터 정책 기준이 있습니다. 일반적인 경우 네트워크 정책은 나가는(Outbound) 트래픽은 대부분의 포트가 열려있는 한편 들어오는(Inbound) 트래픽에는 극소수의 포트만 열려있습니다. 만약 로컬 서버에서 일반적으로 HTTP가 사용하는 80/tcp 포트로 서버를 띄어놓았다면 대부분의 경우 이 포트는 막혀있습니다. (개발용 서버인 8000/8080/4000/3000등도 마찬가지입니다. 극소수 빼고는 기본적으로 다 막아둡니다.) 이렇게 포트가 막혀있다면 우리가 로컬에 띄어둔 서버가 아무리 모든 IP에서의 접근을 허용한다고 해도 중간에 있는 라우터에서 막아버리기 때문에 LTE등의 모바일 셀룰러같은 외부에서의 접속은 사실상 불가능합니다. 따라서 이를 해결하기 위해 ngrok와 같은 SSH 터널링을 이용합니다. 하지만 ngrok 서비스 서버는 기본적으로 해외에 있고, 무료 Plan의 경우 분당 connection의 개수를 40개로 제한하고 있습니다. 만약 CSS나 JS, 이미지같은 static파일 요청 하나하나가 각각 connection을 사용한다면, 짧은 시간 내 여러번 새로고침은 수십개의 connection을 만들어버리고 ngrok은 요청을 즉시 차단해버립니다. 물론 keep-alive를 지원하는 클라이언트/서버 설정이 이루어지면 connection은 새로고침을 해도 늘어나지 않습니다. 하지만 모든 클라이언트가 keep-alive를 지원하지는 않습니다. 하지만 유료 플랜이라고 해서 무제한 connection을 지원하지는 않기 때문에 마음놓고 새로고침을 하기는 어렵습니다. 이번 가이드에서는 ngrok같이 로컬 개발 서버(장고의 runserver, webpack의 webpack-dev-server)를 다른 서버에 SSH Proxy를 통해 전달하는 법, 그리고 CloudFlare를 통해 HTTPS서버로 만드는 것까지를 다룹니다. 재료준비80/tcp가 열린 서버가 있어야 합니다이번 가이드에서는 80/tcp 포트가 열려있는 서버가 “꼭” 있어야 합니다. 물론 서버에는 공인 IP가 할당되어야 합니다. 그래야 나중에 CloudFlare에서 DNS설정을 해줄 수 있습니다. 만약 집에 이런 서버를 둔다면 포트포워딩을 통해 80/tcp만 열어줘도 됩니다. 한국서버가 가장 좋지만(물리적으로 가까우니까) 일본 VPS도 속도면에서 큰 손해를 보지는 않습니다. (물론 게임서버라면 약간 이야기가 다르지만, 웹 서버용으로는 충분합니다.) 이번엔 ubuntu server os를 세팅하는 방법으로 진행합니다. (ubuntu 14.04, 16.04 모두 가능합니다.) (HTTPS를 쓰려면) 도메인이 있어야 합니다개인 도메인이 있어야 CloudFlare라는 DNS서비스에 등록을 하고 HTTPS를 이용할 수 있습니다. 도메인이 없거나 HTTPS를 사용하지 않아도 되는 상황이라면 공인 IP만 있어도 무방합니다. 만들어보기ubuntu 서버와 도메인이 준비되었다면 이제 시작해봅시다! 서버 세팅하기서버 세팅은 크게 어렵지 않습니다. ssh로 서버에 접속해 아래 명령어를 그대로 입력해보세요. 1sshd -T | grep -E 'gatewayports|allowtcpforwarding' 위 명령어는 sshd의 gatewayports속성과 allowtcpforwarding속성값을 가져옵니다. 만약 여러분이 ubuntu를 설치하고 아무런 설정을 건드리지 않았다면 다음과 같이 뜰거에요. 12gatewayports noallowtcpforwarding yes 우리는 저 두개를 모두 yes로 만들어야 합니다. 아래 명령어를 ssh에 그대로 입력해주세요. 1sudo echo \"gatewayports yes\\nallowtcpforwarding yes\" &gt;&gt; /etc/ssh/sshd_config 물론 /etc/ssh/sshd_config 파일에서 직접 수정해주셔도 됩니다. 유의: 이와같이 사용하면 서버의 모든 유저가 SSH Proxy를 사용할수 있게 됩니다. 이를 막으려면 아래와 같이 Match User 유저이름을 넣고 진행해주세요. 123Match User beomi AllowTcpForwarding yes GatewayPorts yes 정말 간단하게 서버 설정이 끝났습니다 :) 로컬 8000포트를 원격 80포트로 연결하기로컬 터미널에서 아래와 같이 명령어를 입력하면 설정이 끝납니다. 12# ssh 원격서버유저이름@서버ip -N -R 서버포트:localhost:로컬포트 ssh beomi@47.156.24.36 -N -R 80:localhost:8000 위 명령어는 47.156.24.36라는 ip를 가진 서버에 beomi라는 사용자로 ssh접속을 하고, 로컬의 8000번 포트를 원격 서버의 80포트로 연결하는 명령어입니다. 즉, localhost:8000 은 47.156.24.36:80와 같아진거죠! 이제 모바일 디바이스에서도 http://47.156.24.36라고 입력하면 개발 서버에 들어올 수 있어요. CloudFlare로 SSL 붙이기만약 서버주소를 외우는게 불편하지 않으시고 &amp; HTTPS가 필요하지 않으시다면, 아래부분은 진행하지 않아도 괜찮습니다. 이 챕터에서는 CloudFlare에 도메인을 연결할 때 제공받을 수 있는 SSL서비스를 통해 HTTP로 서빙되는 우리 서비스를 ‘안전한’ HTTPS로 서빙하도록 도와줍니다. CloudFlare의 Flex SSL을 사용하면 우리 서버가 HTTPS가 아닌 HTTP로 서빙되더라도 클라우드 플레어에서 HTTPS로 만들어줍니다. 사실 이 기능은 보안을 위해서 있는 서비스라고 보기는 어렵습니다. 물론 브라우저/클라이언트와 CloudFlare 간 통신에서는 좀 더 안전한 통신이 가능하지만, 도메인별로 다른 SSL 인증서를 사용하지 않고 여러 도메인을 그룹핑한 인증서를 사용하고 있는 문제가 있고, 결국 CloudFlare와 우리 서버간에는 HTTP로 통신이 이루어지기 때문에 CloudFlare와 우리 서버 사이 Node에서 이루어지는 공격은 막기 어렵습니다. 따라서 이런 경우는 Geolocation와 같은 HTTPS 위에서만 사용할 수 있는 기능등을 테스트 서버를 통해 구동할 경우 유용합니다. 우선 CloudFlare에 가입하고 도메인을 CloudFlare에 등록해주세요. 도메인을 등록하고 DNS 탭에 들어가서 다음과 같이 서브 도메인(혹은 루트 도메인)을 서버 ip에 연결한 후 우측 하단의 구름모양을 켜 주세요. 이 구름모양을 켜 주면 이 도메인으로 온 요청은 CloudFlare의 CDN망을 통해 전달됩니다. (CSS/JS캐싱도 해줍니다!) 도메인을 등록했으면 아래와 같이 Crypto탭에서 SSL을 Flexible로 바꿔주세요. off: 말 그대로 HTTPS를 끕니다. flexible: 우리 서버가 HTTP라도 클라우드플레어로 온 HTTPS요청을 우리서버에 HTTP로 바꿔서 보내줍니다. full: 우리 서버도 HTTPS가 지원되어야 하지만, 꼭 CA에게 인증된 ‘안전한’ 인증서일 필요는 없습니다. 자체서명 인증서라도 괜찮아요. full (strict): 우리 서버가 CA에게 인증된 ‘안전한’ 인증서를 통해 HTTPS로 서빙을 해야만 합니다. 자체서명 인증서는 쓸 수 없어요. 이 설정은 off에서 다른 옵션으로 바꿔주면 약간의 시간이 걸리지만 안전한 SSL 인증서를 CloudFlare에서 만들어줍니다. proxy 명령어에 연결하기보통 runserver와 같은 개발 서버를 띄우는 명령은 자주 사용하지만 우리가 사용하는 긴 명령어는 한번에 치기도 어렵고 옵션 기억하기도 귀찮은 경우가 많습니다. 쉘에서 지원하는 alias를 통해 아래와 같이 만들어줍시다. 12345678910111213141516171819202122232425# .zshrc / .bashrc / .bash_profile 와 같이 쉘이 켜질때 실행되는 부분에 넣어주세요alias proxy=\"ssh beomi@47.156.24.36 -N -R 80:localhost:8000\"# alias proxy=\"ssh 원격서버유저이름@서버ip -N -R 서버포트:localhost:로컬포트\"``` 이와 같이 입력하고 저장한 후 터미널을 다시 켜주면 이제 `proxy`라는 명령어를 치면 로컬 개발 서버가 HTTPS로 세상에 오픈되는 것을 볼 수 있습니다 :)## 마치며ngrok는 아주 간편하고 좋은 서비스입니다. 하지만 모바일과 PC 웹을 동시에 테스트 하는 경우 connection개수를 금방 넘어버리고 ngrok를 새로 실행할 때마다 도메인 이름이 바뀌는점이 불편해 위와 같이 Proxy서버를 만들어 개발하는데 사용합니다.다만 CloudFlare의 CSS/JS캐싱 전략에 의해 변경된 파일이 가져와지지 않는 점은 있는데, 이때는 Apache등의 웹서버에서 제공하는 virtualhost기능과 let's encrypt의 무료 SSL 서비스를 조합해 사용하면 CloudFlare없이도 동일하게 환경을 만들어 줄 수 있습니다. 하지만 웹서버 자체에 대한 이해가 필요하며 SSL을 붙이는 일도 상당히 귀찮기때문에 단순하게 CloudFlare에서 도에인 모드를 아래와 같이 'Development Mode'로 설정해 주면 캐싱 하는 것을 방지할 수 있습니다.![](https://d1sr4ybm5bj1wl.cloudfront.net/img/dropbox/Screenshot%202017-08-27%2014.42.36.png?dl=1)### 여담django의 경우에는 `settings.py`파일의 `ALLOWED_HOSTS`에 우리가 지정한 도메인 (ex: shop.testi.kr)을 추가해줘야 합니다.```python# settings.pyALLOWED_HOSTS = ['*'] # 모든 Host에서의 접근을 허용# ALLOWED_HOSTS = ['shop.testi.kr'] # shop.testi.kr 도메인 host를 통한 접근을 허용 webpack의 webpack-dev-server에서 위와같이 사용하려면 webpack.config.js파일을 아래와 같이 만들어주면 됩니다. 12345678910111213// webpack.config.jsconst path = require('path');module.exports = { entry: './src/index.js', output: { path: path.resolve(__dirname, 'dist'), filename: 'bundle.js' }, devServer: { host: \"0.0.0.0\", // 모든 host에서의 접근을 허용 disableHostCheck: true // Host Check를 끕니다 }","link":"/2017/08/26/SSH-Reverse-Proxy-like-ngrok/"},{"title":"Fabric으로 Flask 자동 배포하기","text":"이번 글은 Ubuntu16.04 LTS / Python3 / Apache2.4 서버 환경으로 진행합니다. 들어가며플라스크를 서버에 배포하는 것은 장고 배포와는 약간 다릅니다. 기본적으로 Apache2를 사용하기 때문에 mod_wsgi를 사용하는 것은 동일하지만, 그 외 다른 점이 조금 있습니다. 우선 간단한 플라스크 앱 하나가 있다고 생각을 해봅시다. 가장 단순한 형태는 아래와 같이 루트로 접속시 Hello world!를 보여주는 것이죠. 12345678# app.pyfrom flask import Flaskapp = Flask(__file__)@app.route('/')def hello(): return \"Hello world!\" 물론 여러분이 실제로 만들고 썼을 프로젝트는 이것보다 훨씬 복잡하겠지만, 일단은 이걸로 시작은 할 수 있답니다. wsgi.py 파일 만들기로컬에서 app.run() 을 통해 실행했던 테스트서버와는 달리 실 배포 상황에서는 Apache나 NginX와 같은 웹서버를 거쳐 웹을 구동하고, 따라서 app.run()의 방식은 더이상 사용할 수 없습니다. 대신 여러가지 웹서버와 Flask를 연결시켜주는 방법이 있는데, 이번엔 그 중 wsgi를 통해 Apache서버가 Flask 앱을 실행하도록 만들어줄 것이랍니다. 우선 wsgi.py파일을 하나 만들어야 합니다. 이 파일은 나중에 Apache서버가 이 파일을 실행시켜 Flask서버를 구동하게 됩니다. 그리고 이 파일은 위에서 만든 변수인 app = Flask(__file__), 즉 app변수를 import할 수 있는 위치에 있어야 합니다. (app.py파일과 동일한 위치에 두면 무방합니다.) 12345678910# wsgi.py # app.py와 같은 위치import sysimport osCURRENT_DIR = os.getcwd()sys.stdout = sys.stderrsys.path.insert(0, CURRENT_DIR)from app import app as application 우리가 wsgi를 통해 실행할 경우 프로그램은 application이라는 변수를 찾아 run()와 비슷한 명령을 실행해 서버를 구동합니다. 따라서 우리는 wsgi.py파일 내 application이라는 변수를 만들어줘야 하는데, 이 변수는 바로 app.py내의 app변수입니다. 위 코드를 보시면 sys모듈과 os모듈을 사용합니다. os모듈의 getcwd()함수를 통해 현재 파일의 위치를 시스템의 PATH 경로에 넣어줍니다. 이 줄을 통해 바로 아래에 있는 from app import app이라는 구문에서 from app 부분이 현재 wsgi.py파일의 경로에서 app.py를 import할 수 있게 되는 것이죠. 만약 이 줄이 빠져있다면 ImportError가 발생하며 app이라는 모듈을 찾을 수 없다는 익셉션이 발생합니다. Fabric3 설치하기Fabric3은 Python2만 지원하던 fabric프로젝트를 포크해 Python3을 지원하도록 업데이트한 패키지입니다. 우선 pip로 패키지를 설치해 줍시다. 12pip install fabric3# 맥/리눅스라면 pip3 install fabric3 이제 우리는 fab이라는 명령어를 사용할 수 있습니다. 이 명령어를 통해 fabfile.py 파일 내의 함수를 실행할 수 있게 됩니다. fabfile.py 파일 만들기Fabric은 그 자체로는 하는 일이 없습니다. 사실 fabric은 우리가 서버에 들어가서 ‘Git으로 소스를 받고’, ‘DB를 업데이트하고’, ‘Static파일을 정리하며’, ‘웹서버 설정을 업데이트’해주는 일들을 하나의 마치 배치파일처럼 자동으로 실행할 수 있도록 도와주는 도구입니다. 하지만 이 도구를 사용하려면 우선 fabfile.py라는 파일이 있어야 fabric이 이 파일을 읽고 파일 속의 함수를 실행할 수 있게 됩니다. fabfile을 만들기 전 deploy.json이라는 이름의 json파일을 만들어 아래와 같이 설정을 담아줍시다. 우선 REPO_URL을 적어줍시다. 이 REPO에서 소스코드를 받아 처리해줄 예정이기 때문이죠. 그리고 PROJECT_NAME을 설정해 주세요. 일반적인 상황이라면 REPO의 이름과 같에 넣어주면 됩니다. 그리고 REMOTE_HOST는 서버의 주소가 됩니다. http등을 제외한 ‘도메인’부분만 넣어주세요. 그리고 서버에 SSH로 접속할 수 있는 IP를 REMOTE_HOST_SSH에 넣어주고, 마지막으로 sudo권한을 가진 유저이름을 REMOTE_USER에 넣어주세요. 1234567{ \"REPO_URL\": \"https://github.com/Beomi/our_project\", \"PROJECT_NAME\": \"our_project\", \"REMOTE_HOST\": \"our_project.com\", \"REMOTE_HOST_SSH\": \"123.32.1.4\", \"REMOTE_USER\": \"sudouser\"} 자, 이제 아래 코드를 통해 fabfile.py파일을 만들어 줍시다. (이것도 app.py와 같은 위치에 두면 관리하기가 편합니다.) 이부분은 파일에 설명을 담을 예정이니 코드의 주석을 참고해주세요. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146# fabfile.pyfrom fabric.contrib.files import append, exists, sed, putfrom fabric.api import env, local, run, sudoimport osimport json# 현재 fabfile.py가 있는 폴더의 경로PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))# deploy.json이라는 파일을 열어 아래의 변수들에 담아줍니다.envs = json.load(open(os.path.join(PROJECT_DIR, \"deploy.json\")))REPO_URL = envs['REPO_URL']PROJECT_NAME = envs['PROJECT_NAME']REMOTE_HOST = envs['REMOTE_HOST']REMOTE_HOST_SSH = envs['REMOTE_HOST_SSH']REMOTE_USER = envs['REMOTE_USER']# SSH에 접속할 유저를 지정하고,env.user = REMOTE_USER# SSH로 접속할 서버주소를 넣어주고,env.hosts = [ REMOTE_HOST_SSH,]# 원격 서버중 어디에 프로젝트를 저장할지 지정해준 뒤,project_folder = '/home/{}/{}'.format(env.user, PROJECT_NAME)# 우리 프로젝트에 필요한 apt 패키지들을 적어줍니다.apt_requirements = [ 'curl', 'git', 'python3-dev', 'python3-pip', 'build-essential', 'apache2', 'libapache2-mod-wsgi-py3', 'python3-setuptools', 'libssl-dev', 'libffi-dev',]# _로 시작하지 않는 함수들은 fab new_server 처럼 명령줄에서 바로 실행이 가능합니다.def new_server(): setup() deploy()def setup(): _get_latest_apt() _install_apt_requirements(apt_requirements) _make_virtualenv()def deploy(): _get_latest_source() _put_envs() _update_virtualenv() _make_virtualhost() _grant_apache2() _restart_apache2()# put이라는 방식으로 로컬의 파일을 원격지로 업로드할 수 있습니다.def _put_envs(): pass # activate for envs.json file # put('envs.json', '~/{}/envs.json'.format(PROJECT_NAME))# apt 패키지를 업데이트 할 지 결정합니다.def _get_latest_apt(): update_or_not = input('would you update?: [y/n]') if update_or_not == 'y': sudo('apt-get update &amp;&amp; apt-get -y upgrade')# 필요한 apt 패키지를 설치합니다.def _install_apt_requirements(apt_requirements): reqs = '' for req in apt_requirements: reqs += (' ' + req) sudo('apt-get -y install {}'.format(reqs))# virtualenv와 virtualenvwrapper를 받아 설정합니다.def _make_virtualenv(): if not exists('~/.virtualenvs'): script = '''\"# python virtualenv settings export WORKON_HOME=~/.virtualenvs export VIRTUALENVWRAPPER_PYTHON=\"$(command \\which python3)\" # location of python3 source /usr/local/bin/virtualenvwrapper.sh\"''' run('mkdir ~/.virtualenvs') sudo('pip3 install virtualenv virtualenvwrapper') run('echo {} &gt;&gt; ~/.bashrc'.format(script))# Git Repo에서 최신 소스를 받아옵니다.# 깃이 있다면 fetch를, 없다면 clone을 진행합니다.def _get_latest_source(): if exists(project_folder + '/.git'): run('cd %s &amp;&amp; git fetch' % (project_folder,)) else: run('git clone %s %s' % (REPO_URL, project_folder)) current_commit = local(\"git log -n 1 --format=%H\", capture=True) run('cd %s &amp;&amp; git reset --hard %s' % (project_folder, current_commit))# Repo에서 받아온 requirements.txt를 통해 pip 패키지를 virtualenv에 설치해줍니다.def _update_virtualenv(): virtualenv_folder = project_folder + '/../.virtualenvs/{}'.format(PROJECT_NAME) if not exists(virtualenv_folder + '/bin/pip'): run('cd /home/%s/.virtualenvs &amp;&amp; virtualenv %s' % (env.user, PROJECT_NAME)) run('%s/bin/pip install -r %s/requirements.txt' % ( virtualenv_folder, project_folder ))# (optional) UFW에서 80번/tcp포트를 열어줍니다.def _ufw_allow(): sudo(\"ufw allow 'Apache Full'\") sudo(\"ufw reload\")# Apache2의 Virtualhost를 설정해 줍니다. # 이 부분에서 wsgi.py와의 통신, 그리고 virtualenv 내의 파이썬 경로를 지정해 줍니다.def _make_virtualhost(): script = \"\"\"'&lt;VirtualHost *:80&gt; ServerName {servername} &lt;Directory /home/{username}/{project_name}&gt; &lt;Files wsgi.py&gt; Require all granted &lt;/Files&gt; &lt;/Directory&gt; WSGIDaemonProcess {project_name} python-home=/home/{username}/.virtualenvs/{project_name} python-path=/home/{username}/{project_name} WSGIProcessGroup {project_name} WSGIScriptAlias / /home/{username}/{project_name}/wsgi.py {% raw %} ErrorLog ${{APACHE_LOG_DIR}}/error.log CustomLog ${{APACHE_LOG_DIR}}/access.log combined {% endraw %} &lt;/VirtualHost&gt;'\"\"\".format( username=REMOTE_USER, project_name=PROJECT_NAME, servername=REMOTE_HOST, ) sudo('echo {} &gt; /etc/apache2/sites-available/{}.conf'.format(script, PROJECT_NAME)) sudo('a2ensite {}.conf'.format(PROJECT_NAME))# Apache2가 프로젝트 파일을 읽을 수 있도록 권한을 부여합니다.def _grant_apache2(): sudo('chown -R :www-data ~/{}'.format(PROJECT_NAME)) sudo('chmod -R 775 ~/{}'.format(PROJECT_NAME))# 마지막으로 Apache2를 재시작합니다.def _restart_apache2(): sudo('sudo service apache2 restart') 위 코드를 fabfile.py에 넣어주고 나서 첫 실행시에는 fab new_server 코드를 수정하고 push한 뒤 서버에 배포시에는 fab deploy 명령을 실행해 주면 됩니다. NOTE: _ 로 시작하는 함수는 fab 함수이름으로 실행하지 못합니다. 자, 이제 서버에 올릴 준비가 되었습니다. 서버에 올리기우분투 서버를 만들고 첫 배포라면 new_server를, 한번 new_server를 했다면 deploy로 배포를 진행합니다. 12fab new_server # 첫 배포fab deploy # 첫 배포를 제외한 나머지 끝났습니다!여러분의 사이트는 이제 http://REMOTE_HOST 으로 접속 가능할거에요! 유의할 점fabfile내의 apt_requirements 리스트에는 프로젝트마다 필요한 다른 패키지들을 적어줘야 합니다. 만약 여러분의 프로젝트에서 mysqlclient패키지등을 사용한다면 libmysqlclient-dev를 apt_requirements리스트에 추가해줘야 합니다. 혹은 PostgreSQL을 사용한다면 libpq-dev가 필요할 수도 있습니다. 그리고 여러분이 이미지 처리를 하는 pillow패키지를 사용한다면 libjpeg62-dev를 apt_requirements에 추가해야 할 수도 있습니다. 이처럼 여러분이 파이썬 패키지에서 어떤 상황이냐에 따라 다른 apt패키지 리스트를 넣어줘야 합니다. 이 부분만 유의해 넣어준다면 Fabric으로 한번에 배포에 성공할 수 있을거랍니다! :)","link":"/2017/10/17/Deploy-Flask-with-Fabric/"},{"title":"SQLAlchemy Query를 Pandas DataFrame로 만들기","text":"이번 글은 기존 DB를 Flask-SQLAlchemy ORM Model로 사용하기를 보고 오시면 좀더 빠르게 실 프로젝트에 적용이 가능합니다. 들어가며 전체 예시를 보시려면 TL;DR를 참고하세요. DB에 있는 정보를 파이썬 코드 속에서 SQL raw Query를 통해 정보를 가져오는 아래와 같은 코드의 형태는 대다수의 언어에서 지원합니다. 1234567import sqlite3# 굳이 sqlite3이 아닌 다른 MySQL와 같은 DB의 connect를 이뤄도 상관없습니다.# 여기서는 파이썬 파일과 같은 위치에 blog.sqlite3 파일이 있다고 가정합니다.conn = sqlite3.connect(\"blog.sqlite3\") cur = conn.cursor()cur.execute(\"select * from post where id &lt; 10;\") 위와 같은 형식으로 코드를 사용할 경우 웹이 이루어지는 과정 중 2~3번째 과정인 “SQL쿼리 요청하기”와 “데이터 받기”라는 부분을 수동으로 처리해 줘야 하는 부분이 있습니다. 이런 경우 파이썬 파일이더라도 한 파일 안에 두개의 언어를 사용하게 되는 셈입니다. (python와 SQL) 만약 여러분이 Pandas DataFrame객체를 DB에서 가져와 만들려면 이런 문제가 생깁니다. DB에 연결을 구성해야 함 가져온 데이터를 데이터 타입에 맞춰 파이썬이 이해하는 형태로 변환 정리한 데이터를 Pandas로 불러오기 음, 보기만 해도 상당히 귀찮네요. 설치하기우선 필요한 패키지들을 먼저 설치해 줍시다. 123pip install flaskpip install Flask-SQLAlchemypip install pandas Pandas로 SQL요청하기Pandas에서는 이런 귀찮은 점을 보완해 주기 위해 read_sql_query라는 함수를 제공합니다. 위 코드를 조금 바꿔봅시다. 1234567891011import sqlite3import pandas as pd # NoQAconn = sqlite3.connect(\"blog.sqlite3\")# 이 부분을 삭제# cur = conn.cursor()# cur.execute(\"select * from post where id &lt; 10;\")# 아래 부분을 추가df = pd.read_sql_query(\"select * from post where id &lt; 10;\", conn)# df는 이제 Pandas Dataframe 객체 단순하게 DB 커넥션, 그리고 read_sql_query만으로 SQL Query를 바로 Pandas DataFrame 객체로 받아왔습니다. 이제 데이터를 수정하고 가공하는 처리는 Pandas에게 맡기면 되겠군요! 하지만, 여전히 우리는 SQL을 짜고있어요. 복잡한 쿼리라면 몰라도, 단순한 쿼리를 이렇게까지 할 필요가 있을까요? SQLAlchemy 모델 이용하기Flask를 사용할때 많이 쓰는 SQLAlchemy는 ORM으로 수많은 DB를 파이썬만으로 제어하도록 도와줍니다. 그리고 이 점이 우리가 SQL을 SQLAlchemy를 통해 바로 만들 수 있도록 도와줍니다. NOTE: 이번 글에서는 Flask-SQLAlchemy 패키지를 사용합니다. SQLAlchemy와는 약간 다르게 동작할 수도 있습니다. 모델 클래스 만들기모델 클래스를 기존 DB를 참조해 만드는 것은 기존 DB를 Flask-SQLAlchemy ORM Model로 사용하기 를 참고하세요. 예제 모델: Post블로그에서 자주 쓸 법한 Post라는 이름의 모델 클래스를 하나 만들어 봅시다. 우선 SQLAlchemy를 flask_sqlalchemy에서 import 해옵시다. 그리고 Flask도 가져와 app을 만들어 줍시다. 그리고 db객체를 만들어줍시다. 1234567891011121314151617from datetime import datetimefrom flask import Flask, jsonifyfrom flask_sqlalchemy import SQLAlchemyimport pandas as pdapp = Flask(__name__)# 현재 경로의 blog.sqlite3을 불러오기app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///blog.sqlite3'db = SQLAlchemy(app)class Post(db.Model): __tablename__ = 'post' id = db.Column(db.Integer, primary_key=True) title = db.Column(db.String(100)) content = db.Column(db.Text) pub_date = db.Column(db.DateTime, nullable=False, default=datetime.utcnow) 자, 이제 여러분은 blog.sqlite3파일 안에 post라는 테이블에 값들을 넣거나 뺄 수 있게 되었습니다. 루트 View 만들기여러분이 app.run( ) 으로 Flask 개발 서버를 띄웠을 때 첫 화면(‘/‘ URL에서) 실행될 View 함수(post_all)를 만들어줍시다. 123456789101112131415161718192021222324from datetime import datetimefrom flask import Flask, jsonifyfrom flask_sqlalchemy import SQLAlchemyimport pandas as pdimport jsonapp = Flask(__name__)app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///blog.sqlite3'db = SQLAlchemy(app)class Post(db.Model): __tablename__ = 'post' id = db.Column(db.Integer, primary_key=True) title = db.Column(db.String(100)) content = db.Column(db.Text) pub_date = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)# 아래 줄을 추가해 줍시다.# List post which id is less then 10@app.route('/')def post_all(): df = pd.read_sql_query(\"select * from post where id &lt; 10;\", db.session.bind).to_json() return jsonify(json.loads(df)) 자, 분명히 ORM을 쓰는데도 아직 SQL 쿼리를 쓰고있네요! SQL쿼리문을 지워버립시다! queryset 객체를 만들기우리는 Post라는 모델을 만들어줬으니 이제 Post객체의 .query와 .filter()를 통해 객체들을 가져와 봅시다. 우선 queryset라는 이름에 넣어줍시다. 그리고 Pandas의 read_sql(유의: read_sql_query가 아닙니다.)에 queryset의 내용과 세션을 넘겨줘 DataFrame 객체로 만들어줍시다. 123456789101112131415161718192021222324252627from datetime import datetimefrom flask import Flask, jsonifyfrom flask_sqlalchemy import SQLAlchemyimport pandas as pdimport jsonapp = Flask(__name__)app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///blog.sqlite3'db = SQLAlchemy(app)class Post(db.Model): __tablename__ = 'post' id = db.Column(db.Integer, primary_key=True) title = db.Column(db.String(100)) content = db.Column(db.Text) pub_date = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)# List post which id is less then 10@app.route('/')def post_all(): # 이 줄은 지우고, # df = pd.read_sql_query(\"select * from post where id &lt; 10;\", db.session.bind).to_json() # 아래 두줄을 추가해주세요. queryset = Post.query.filter(Post.id &lt; 10) # SQLAlchemy가 만들어준 쿼리, 하지만 .all()이 없어 실행되지는 않음 df = pd.read_sql(queryset.statement, queryset.session.bind) # 진짜로 쿼리가 실행되고 DataFrame이 만들어짐 return jsonify(json.loads(df).to_json()) 자, 위와 같이 코드를 짜 주면 이제 SQLAlchemy ORM와 Pandas의 read_sql을 통해 df이 DataFrame 객체로 자연스럽게 가져오게 됩니다. 정리하기여러분이 Pandas를 사용해 데이터를 분석하거나 정제하려 할 때 웹앱으로 Flask를 사용하고 ORM을 이용한다면, 굳이 SQL Query를 직접 만드는 대신 이처럼 Pandas와 SQLAlchemy의 강력한 조합을 이용해 보세요. 조금 더 효율적인 시스템 활용을 고려한 파이썬 프로그램이 나올거에요! TL;DR아래 코드와 같이 모델을 만들고 db 객체를 만든 뒤 pandas의 read_sql을 사용하면 됩니다. 123456789101112131415161718192021222324from datetime import datetimefrom flask import Flask, jsonifyfrom flask_sqlalchemy import SQLAlchemyimport pandas as pdimport jsonapp = Flask(__name__)app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///blog.sqlite3'db = SQLAlchemy(app)class Post(db.Model): __tablename__ = 'post' id = db.Column(db.Integer, primary_key=True) title = db.Column(db.String(100)) content = db.Column(db.Text) pub_date = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)# List post which id is less then 10@app.route('/')def post_all(): queryset = Post.query.filter(Post.id &lt; 10) # SQLAlchemy가 만들어준 쿼리, 하지만 .all()이 없어 실행되지는 않음 df = pd.read_sql(queryset.statement, queryset.session.bind) # 진짜로 쿼리가 실행되고 DataFrame이 만들어짐 return jsonify(json.loads(df).to_json())","link":"/2017/10/21/SQLAlchemy-Query-to-Pandas-DataFrame/"},{"title":"Webpack과 Babel로 최신 JavaScript 웹프론트 개발환경 만들기","text":"이번 포스팅에서는 nodejs8.5.0, npm5.3.0 버전을 사용합니다. 들어가며파이썬의 버전 2와 3이 다른 것은 누구나 알고 2017년인 오늘은 대부분 Python3버전을 이용해 프로젝트를 진행합니다. 하지만 자바스크립트에 버전이 있고 새로운 기능이 나온다 하더라도 이 기능을 바로 사용하는 경우는 드뭅니다. 물론 node.js를 이용한다면 자바스크립트의 새로운 버전의 기능을 바로바로 이용해볼 수 있지만 프론트엔드 웹 개발을 할 경우 새로 만들어진 자바스크립트의 기능을 사용하는 것은 상당히 어렵습니다. 123456// 이런 문법은 사용하지 못합니다.const hello = 'world'const printHelloWorld = (e) =&gt; { console.log(e)}printHelloWorld(hello) 가장 큰 차이는 실행 환경의 문제인데요, 우리가 자주 사용하는 크롬브라우저의 경우에는 자동업데이트 기능이 내장되어있어 일반 사용자가 크롬브라우저를 실행만 해도 최신 버전을 이용하지만, 인터넷 익스플로러나 사파리와 같은 경우에는 많은 사용자가 OS에 설치되어있던 버전 그대로를 이용합니다. 물론 이렇게 사용하는 것도 심각한 문제를 가져오지는 않지만, 구형 브라우저들은 새로운 자바스크립트를 이해하지 못하기 때문에 이 브라우저를 사용하는 사용자들은 새로운 자바스크립트로 개발된 웹 사이트를 접속할 경우 전혀 다르게 혹은 완전히 동작하지 않는 페이지를 볼 수 있기 때문에 많은 일반 사용자를 대상으로 하는 서비스의 경우 새 버전의 자바스크립트를 사용해 개발한다는 것이 상당히 모험적인 성향이 강합니다. 글쓴 시점인 2017년 10월 최신 자바스크립트 버전은 ES2017로 ES8이라 불리는 버전입니다. 하지만 이건 정말 최신 버전의 자바스크립트이고, 중요한 변화가 등장한 버전이 2015년도에 발표된 ES2015, 다른 말로는 ES6이라고 불리는 자바스크립트입니다. 하지만 인터넷익스플로러를 포함한 대부분의 브라우저들이 지원하는 자바스크립트의 버전은 ES5로 이보다 한단계 낮은 버전을 사용합니다. 따라서 우리는 ES6혹은 그 이상 버전의 자바스크립트 코드들을 ES5의 아래 버전 자바스크립트로 변환해 사용하는 방법을 사용할 수 있습니다. Babel여기서 바로 Babel이 등장합니다. Babel은 최신 자바스크립트를 ES5버전에서도 돌아갈 수 있도록 변환(Transpiling)해줍니다. 우리가 자바스크립트 최신 버전의 멋진 기능을 이용하는 동안, Babel이 다른 브라우저에서도 돌아갈 수 있도록 처리를 모두 해주는 것이죠! 물론, Babel이 마법의 요술도구처럼 모든 최신 기능을 변환해주지는 못합니다. 하지만 아래 사진처럼 다양한 브라우저에 따라 최신 JavaScript문법 중 어떠 부분까지가 실행 가능한 범위인지 알려줍니다. WebpackES6에서 새로 등장한 것 중 유용한 문법이 바로 import .. from ..구문입니다. 다른 언어에서의 import와 유사하게 경로(상대경로 혹은 절대경로)에서 js파일을 불러오는 방식으로 동작합니다. 예를들어 어떤 폴더 안에 Profile.js와 index.js파일이 있다고 생각해 봅시다. 1234567891011// Profile.jsexport class Profile { constructor(name, email) { this.name = name this.email = email } hello() { return `Hello, ${this.name}(${this.email})` }} 하는일이라고는 name, email을 받는 것, 그리고 hello하는 함수밖에 없지만 우선 Profile이라는 class를 하나 만들었습니다. 여기서 Profile 클래스 앞에 export를 해 주었는데, export를 해 줘야 다른 파일에서 import가 가능합니다. 자, 아래와 같이 index.js파일을 하나 만들어 봅시다. 12345// index.jsimport { Profile } from './Profile'const pf = new Profile('Beomi', 'jun@beomi.net')console.log(pf.hello()) 이 파일은 현재 경로의 Profile.js파일 중 Profile 클래스를 import해와 새로운 인스턴스를 만들어 사용합니다. 하지만 안타깝게도 이 index.js파일은 실행되지 않습니다. 아직 webpack으로 처리를 해주지 않았기 때문이죠! webpack-dev-serverwebpack은 파일을 모아 하나의 js파일로 만들어줍니다.(보통 bundle.js라는 이름을 많이 씁니다.) 하지만 실제 개발중 js파일을 수정할 때마다 Webpack을 실행해 번들작업을 해준다면 시간도 많이 걸리고 매우 귀찮습니다. 이를 보완해 주는 패키지가 바로 webpack-dev-server 인데요, 이 패키지를 사용하면 여러분이 실제 빌드를 해 bundle.js파일을 만들지 않아도 메모리 상에 가상의 bundle.js파일을 만들어 여러분이 웹 사이트를 띄울때 자동으로 번들된 js파일을 띄워줍니다. 그리고 소스가 수정될 때 마다 업데이트된(번들링된) bundle.js파일로 띄워주고 화면도 새로고침해줍니다! NOTE: webpack-dev-server는 build를 자동으로 해주는 것은 아닙니다. 단지 미리 지정해둔 경로로 접근할 경우 (실제로는 파일이 없지만) bundle.js파일이 있는 것처럼 파일을 보내주는 역할을 맡습니다. 개발이 끝나고 실제 서버에 배포할때는 이 패키지 대신 실제 webpack을 통해 빌드 작업을 거친 최종 결과물을 서버에 올려야 합니다. 설치하기우선 npm프로젝트를 생성해야 합니다. index.js파일을 만든 곳(어떤 폴더) 안에서 다음 명령어로 “이 폴더는 npm프로젝트를 이용하는 프로젝트다” 라는걸 알려주세요. 12# -y 인자를 붙이면 모든 설정이 기본값으로 됩니다.npm init -y 이 명령어를 치면 폴더 안에 package.json파일이 생성되었을 거에요. 이제 다음 명령어로 Babel과 webpack등을 설치해 봅시다. 123# babel과 webpack은 개발환경에서 필요하기 때문에 --save-dev로 사용합니다.npm install --save-dev babel-loader babel-core babel-preset-envnpm install --save-dev webpack webpack-dev-server babel-loader는 webpack이 .js 파일들에 대해 babel을 실행하도록 만들어주고, babel-core는 babel이 실제 동작하는 코드이고, babel-preset-env는 babel이 동작할 때 지원범위가 어느정도까지 되어야 하는지에 대해 지정하도록 만들어주는 패키지입니다. 이렇게 설치를 진행하고 나면 Babel과 Webpack을 사용할 준비를 마친셈입니다. NOTE: package.json뿐 아니라 package-lock.json파일도 함께 생길수 있습니다. 이 파일은 npm패키지들이 각각 수많은 의존성을 가지고 있기 때문에 의존성 패키지들을 다운받는 URL을 미리 모아둬 다른 컴퓨터에서 package.json을 통해 npm install로 패키지들을 설치시 훨씬 빠른 속도로 패키지를 받을 수 있도록 도와줍니다. 이제 설정파일 몇개를 만들고 수정해줘야 해요. 설정파일 건드리기package.jsonpackage.json파일은 파이썬 pip의 requirements.txt처럼 패키지버전 관리만 해주는 것이 아니라 npm와 결합해 특정 명령어를 실행하거나 npm 프로젝트의 환경을 담는 파일입니다. 1npm run 명령어이름 위와 같은 명령어를 사용할 수 있도록 만들어 주기도 합니다. 현재 package.json파일은 아래와 같은 형태로 되어있을거에요. 12345678910111213141516171819{ \"name\": \"npm_blog\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" &amp;&amp; exit 1\" }, \"keywords\": [], \"author\": \"\", \"license\": \"ISC\", \"devDependencies\": { \"babel-core\": \"^6.26.0\", \"babel-loader\": \"^7.1.2\", \"babel-preset-env\": \"^1.6.1\", \"webpack\": \"^3.8.1\", \"webpack-dev-server\": \"^2.9.2\" }} 이제 package.json파일을 열어 &quot;scripts&quot;부분을 다음과 같이 build와 devserver명령어를 추가해 줍시다. 1234567891011121314151617181920{ \"name\": \"npm_blog\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"build\": \"webpack\", \"devserver\": \"webpack-dev-server --open --progress\" }, \"keywords\": [], \"author\": \"\", \"license\": \"ISC\", \"devDependencies\": { \"babel-core\": \"^6.26.0\", \"babel-loader\": \"^7.1.2\", \"babel-preset-env\": \"^1.6.1\", \"webpack\": \"^3.8.1\", \"webpack-dev-server\": \"^2.9.2\" }} 이제 여러분이 npm run build를 할 때는 webpack이 실행되고, npm run devserver를 할 때는 개발용 서버가 띄워질거에요. webpack.config.jswebpack.config.js 파일은 앞서 설치해준 webpack을 실행 시 어떤 옵션을 사용할지 지정해주는 js파일입니다. 우리 프로젝트 폴더에는 아직 webpack.config.js 파일이 없을거에요. package.json와 같은 위치에 webpack.config.js파일을 새로 만들어 아래 내용으로 채워줍시다. 1234567891011121314151617181920212223242526const webpack = require('webpack');const path = require('path');module.exports = { entry: './index.js', output: { path: path.resolve(__dirname, 'dist'), publicPath: '/dist/', filename: 'bundle.js' }, module: { rules: [ { test: /\\.js$/, include: path.join(__dirname), exclude: /(node_modules)|(dist)/, use: { loader: 'babel-loader', options: { presets: ['env'] } } } ] }}; 위 파일은 entry에 현재 위치의 index.js파일을 들어가 모든 import를 찾아오고, module -&gt; rules -&gt; include에 있는 .js로 된 모든 파일을 babel로 처리해줍니다.(exclue에 있는 부분인 node_modules폴더와 dist폴더는 제외합니다.) index.html사실 우리는 아직 번들링된 js파일을 보여줄 HTML파일이 없습니다! 우선 bundle.js를 보여주기만 할 단순한 HTML파일을 하나 만들어 봅시다.(index.js와 같은 위치) 1234567891011&lt;!-- index.html --&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;NPM Webpack&lt;/title&gt;&lt;/head&gt;&lt;body&gt; Webpack용 HTML &lt;script type=\"text/javascript\" src=\"/dist/bundle.js\"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; webpack을 사용하지 않았다면 HTML파일 아래 script태그의 src에 index.js를 넣어야 하지만, 우리는 webpack과 webpack-dev-server를 사용하기때문에 번들링된 파일의 위치인 /dist/bundle.js을 넣어줍니다. devserver 띄우기자, 이제 아래 명령어로 devserver를 띄워봅시다! 1npm run devserver 브라우저의 개발자 도구를 열어보면 아래와 같이 로그가 잘 찍힌걸 확인해 볼 수 있을거에요. 이제 여러분이 index.js파일이나 Profile.js등을 수정하면 곧바로 새로고침되고 새로운 bundle.js를 라이브로 불러올거에요. 배포용으로 만들기여러분이 프로젝트 개발을 끝내고 실제 서버에 배포할 때는 devserver가 아니라 실제로 번들링된 파일인 bundle.js를 만들어야 합니다. 아래 명령어로 현재 위치의 dist폴더 안에 bundle.js 파일을 만들어 줍시다. 1npm run build 위와 같이 나온다면 성공적으로 webpack이 마쳐진 것이랍니다! 그리고 여러분 프로젝트 폴더 안에 dist폴더가 생기고 그 안에 bundle.js파일이 생겼을 거에요. 이제 여러분은 index.html파일과 dist폴더를 묶어 서버에 올리면 페이지가 잘 동작하는것을 확인할 수 있을거에요!","link":"/2017/10/18/Setup-Babel-with-webpack/"},{"title":"Selenium Implicitly wait vs Explicitly wait","text":"좀 더 보기 편한 깃북 버전의 나만의 웹 크롤러 만들기가 나왔습니다! 들어가며Selenium WebDriver를 이용해 실제 브라우저를 동작시켜 크롤링을 진행할 때 가끔가다보면 NoSuchElementException라는 에러가 나는 경우를 볼 수 있습니다. 가장 대표적인 사례가 바로 JS를 통해 동적으로 HTML 구조가 변하는 경우인데요, 만약 사이트를 로딩한 직후에(JS처리가 끝나지 않은 상태에서) JS로 그려지는 HTML 엘리먼트를 가져오려고 하는 경우가 대표적인 사례입니다. (즉, 아직 그리지도 않은 요소를 가져오려고 했기 때문에 생기는 문제인 것이죠.) 그래서 크롤링 코드를 작성할 때 크게 두가지 방법으로 브라우저가 HTML Element를 기다리도록 만들어 줄 수 있습니다. Implicitly waitSelenium에서 브라우저 자체가 웹 요소들을 기다리도록 만들어주는 옵션이 Implicitly Wait입니다. 아래와 같은 형태로 카카오뱅크 타이틀을 한번 가져와 봅시다. 1234567891011121314151617181920from selenium import webdriverdriver = webdriver.Chrome('chromedriver')# driver를 만든 후 implicitly_wait 값(초단위)을 넣어주세요.driver.implicitly_wait(3)driver.get('https://www.kakaobank.com/')# 하나만 찾기title = driver.find_element_by_css_selector('div.intro_main &gt; h3')# 여러개 찾기small_titles = driver.find_elements_by_css_selector('div.cont_txt &gt; h3')print(title.text)for t in small_titles: print(t.text)driver.quit() 위 코드를 실행하면 여러분이 .get()으로 지정해준 URL을 가져올 때 각 HTML요소(Element)가 나타날 때 까지 최대 3초까지 ‘관용있게’ 기다려 줍니다. 즉, 여러분이 find_element_by_css_selector와 같은 방식으로 HTML엘리먼트를 찾을 때 만약 요소가 없다면 요소가 없다는 No Such Element와 같은 Exception을 발생시키기 전 모든 시도에서 3초를 기다려 주는 것이죠. 하지만 이런 방식은 만약 여러분이 크롤링하려는 웹이 ajax를 통해 HTML 구조를 동적으로 바꾸고 있다면 과연 ‘3초’가 적절한 값일지에 대해 고민을 하게 만듭니다.(모든 ajax가 진짜로 3초 안에 이루어질까요?) 그래서 우리는 조금 더 발전된 기다리는 방식인 Explicitly wait을 사용하게 됩니다. NOTE: 기본적으로 Implicitly wait의 값은 0초입니다. 즉, 요소를 찾는 코드를 실행시킨 때 요소가 없다면 전혀 기다리지 않고 Exception을 raise하는 것이죠. Explicitly wait자, 여러분이 인터넷 웹 사이트를 크롤링하는데 ajax를 통해 HTML 구조가 변하는 상황이고, 각 요소가 들어오는 시간은 몇 초가 될지는 예상할 수 없다고 가정해 봅시다. 위에서 설정해 준 대로 implicitly_wait을 이용했다면 어떤 특정한 상황(인터넷이 유독 느렸음)으로 인해 느려진 경우 우리가 평소에 기대했던 3초(n초)를 넘어간 경우 Exception이 발생할 것이고 이로 인해 반복적인 크롤링 작업을 진행할 때 문제가 생길 수 있습니다. 따라서 우리는 명확하게 특정 Element가 나타날 때 까지 기다려주는 방식인 Explicitly Wait을 사용할 수 있습니다. 아래 코드는 위에서 Implicitly wait을 통해 사용했던 암묵적 대기(get_element_by_id 등)을 사용한 대신 명시적으로 div.intro_main &gt; h3라는 CSS Selector로 가져오는 부분입니다. 1234567891011121314151617181920from selenium import webdriver# 아래 코드들을 import 해 줍시다.from selenium.webdriver.common.by import By# WebDriverWait는 Selenium 2.4.0 이후 부터 사용 가능합니다.from selenium.webdriver.support.ui import WebDriverWait# expected_conditions는 Selenium 2.26.0 이후 부터 사용 가능합니다.from selenium.webdriver.support import expected_conditions as ECdriver = webdriver.Chrome('chromedriver')driver.get('https://www.kakaobank.com/')try: # WebDriverWait와 .until 옵션을 통해 우리가 찾고자 하는 HTML 요소를 # 기다려 줄 수 있습니다. title = WebDriverWait(driver, 10) \\ .until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.intro_main &gt; h3\"))) print(title.text)finally: driver.quit() 위 코드를 사용하면 우리가 찾으려는 대상을 driver가 명시적으로 ‘10초’를 기다리도록 만들어 줄 수 있습니다. 마치며만약 여러분이 ajax를 사용하지 않는 웹 사이트에서 단순하게 DOM구조만 변경되는 상황이라면 사실 Explicitly wait을 사용하지 않아도 괜찮을 가능성이 높습니다. (DOM API처리속도는 굉장히 빠릅니다.) 하지만 최신 웹 사이트들은 대부분 ajax요청을 통해 웹 구조를 바꾸는 SPA(Single Page App)이 많기 때문에 크롤링을 진행할 때 Explicitly wait을 이용하는 것이 좋습니다.","link":"/2017/10/29/HowToMakeWebCrawler-ImplicitWait-vs-ExplicitWait/"},{"title":"PySpark & Hadoop: 1) Ubuntu 16.04에 설치하기","text":"들어가며Spark의 Python버전인 PySpark를 사용할 때 서버가 AWS EMR등으로 만들어진 클러스터가 존재하고, 우리가 만든 프로그램과 함수가 해당 클러스터 위에서 돌리기 위해서는 PySpark를 로컬이 아니라 원격 서버에 연결해 동작하도록 만들어야 합니다. 이번 글에서는 PySpark와 Hadoop을 설치하고 설정하는 과정으로 원격 EMR로 함수를 실행시켜봅니다. Note: 이번 글은 Ubuntu 16.04 LTS, Python3.5(Ubuntu내장)를 기준으로 진행합니다. AWS에서 EC2를 생성해 주세요. VPC는 기본으로 잡아주시면 됩니다. 성능은 t2-micro의 프리티어정도도 괜찮습니다. 무거운 연산은 나중에 다룰 AWS EMR 클러스터에 올려줄 것이기 때문에, 클라이언트 역할을 할 EC2 인스턴스는 저성능이어도 괜찮습니다. 들어가기 전에 우선 apt 업데이트부터 진행해 줍시다. 1sudo apt-get update &amp;&amp; sudo apt-get upgrade -y PySpark 설치하기Ubuntu 16 OS에는 기본적으로 Python3이 설치되어있습니다. 하지만 pip는 설치되어있지 않기 때문에 아래 명령어로 먼저 Python3의 pip를 설치해줍시다. 12# pip/pip3을 사용가능하게 만듭니다.sudo apt-get install python3-pip -y 설치가 완료되면 이제 Python3의 pip를 사용할 수 있습니다. 아래 명령어로 pip를 통해 PySpark를 설치해 봅시다. 12# 최신 버전의 PySpark를 설치합니다.pip3 install pyspark -U --no-cache 위 명령어에서 -U는 --upgrade의 약자로, 현재 설치가 되어있어도 최신버전으로 업그레이드 하는 것이고, --no-cache는 로컬에 pip 패키지의 캐싱 파일이 있더라도 pypi서버에서 다시 받아오겠다는 의미입니다. 현재 PySpark 2.2.0은 버전과 다르게 2.2.0.post0라는 버전으로 pypi에 올라가 있습니다. 이로인해 pip install pyspark 로 진행할 경우 Memeory Error가 발생하고 설치가 실패하므로, 2.2.0버전을 설치한다면 위 명령어로 설치를 진행해주세요. Hadoop 설치하기JAVA JDK 설치하기Hadoop을 설치하기 위해서는 JAVA(JDK)가 먼저 설치되어야 합니다. 아래 명령어로 openjdk를 설치해주세요. 12# Java 8을 설치합니다.sudo apt-get install openjdk-8-jre -y Hadoop Binary 설치하기Hadoop은 Apache의 홈페이지에서 최신 릴리즈 링크에서 바이너리 파일의 링크를 가져옵시다. 원하는 Hadoop 버전의 Binary 링크를 클릭해 바이너리를 받을 수 있는 페이지로 들어갑시다. 글쓰는 시점에는 2.8.2가 최신 버전입니다. 링크를 타고 들어가면 아래와 같이 HTTP로 파일을 받을 수 있는 링크가 나옵니다. 글을 보는 시점에는 링크 주소는 다를 수 있지만, HTTP 링크 중 하나를 복사하고 진행하면 됩니다. 이 글에서는 네이버 서버의 미러를 이용합니다. 이제 다시 서버로 돌아가봅시다. 아래 명령어를 통해 wget으로 Hadoop Binary를 서버에 받아줍시다. 123wget 여러분이_복사한_URL# 예시# wget http://mirror.navercorp.com/apache/hadoop/common/hadoop-2.8.2/hadoop-2.8.2.tar.gz 이제 압축을 풀어줍시다. 아래 명령어로 압축을 /usr/local에 풀어줍시다. 1sudo tar zxvf ./hadoop-* -C /usr/local 압축을 풀면 /usr/local/hadoop-2.8.2라는 폴더가 생기지만 우리가 사용할때 버전이 붙어있으면 사용하기 귀찮으므로 이름을 /usr/local/hadoop으로 바꾸어줍시다. 1sudo mv /usr/local/hadoop-* /usr/local/hadoop 이제 파일을 가져오고 설치는 완료되었지만, 실제로 Hadoop을 PySpark등에 붙여 사용하려면 PATH등록을 해줘야 합니다. 아래 명령어를 전체 복사-붙여넣기로 진행해 주세요. 12345678echo \"export JAVA_HOME=$(readlink -f /usr/bin/java | sed \"s:bin/java::\")export PATH=\\$PATH:\\$JAVA_HOME/binexport HADOOP_HOME=/usr/local/hadoopexport PATH=\\$PATH:\\$HADOOP_HOME/binexport HADOOP_CONF_DIR=\\$HADOOP_HOME/etc/hadoopexport YARN_CONF_DIR=\\$HADOOP_HOME/etc/hadoop\" &gt;&gt; ~/.bashrc 이제 ssh를 exit한 뒤 다시 서버에 ssh로 접속하신 후, 아래 명령어를 입력해 보세요. 아래 사진처럼 나오면 설치가 성공적으로 진행된 것이랍니다. 1/usr/local/hadoop/bin/hadoop 끝이지만 끝이 아닌..사실 PySpark와 Hadoop만을 사용하는 것은 큰 의미가 있는 상황은 아닙니다. AWS EMR와 같은 클러스터를 연결해 막대한 컴퓨팅 파워가 있는 서버에서 돌리는 목적이 Spark를 쓰는 이유입니다. 다음 글에서는 AWS EMR을 구동하고 우리가 방금 설정한 Ubuntu 서버에서 작업을 EMR로 보내는 내용을 다뤄봅니다.","link":"/2017/11/09/Install-PySpark-and-Hadoop-on-Ubuntu-16-04/"},{"title":"키노트로 고화질 타이틀 이미지 만들기","text":"이번 글은 애플 키노트를 다루니 당연히 macOS가 대상입니다 :) 들어가며블로그 글을 맥에서 작성하다보면 자연스럽게 키노트로 메인 타이틀이나 설명을 만드는 경우가 많습니다. 아래 화면처럼 이미지와 글자를 조금 배치하는 방식으로 깔끔한 이미지 하나를 만들게 됩니다. (이번 글 대표 이미지도 이 방식으로 만들었습니다.) 하지만 문제가 있습니다. 애플 키노트에서 기본적으로 제공하는 Export to Image, “다음으로 보내기 -&gt; 이미지..”를 사용하는 방법도 물론 있습니다. 하지만 사실 이렇게 작업하면… 화면에서 바라볼때는 고해상도의 레티나 결과물이 나왔지만, 실제로 Export된 이미지 파일을 보면 이미지 픽셀은 1920*1080으로 충분히 고화질이지만 ppi가 72밖에 되지 않는 것을 볼 수 있습니다. (아니 왜 레티나를 샀는데 보여주지를 못하니 ㅠㅠ) 해결방법해결방법은 한 단계를 더 거치는데, 다른 프로그램을 사용하는 것이 아니라 맥에 내장되어있는 ‘미리보기’를 이용하는 방법입니다. 단계는 다음과 같습니다. 키노트에서 PDF로 내보내기 PDF파일을 ‘미리보기’로 열기 미리보기에서 ‘내보내기’ 기능으로 고해상도 이미지 만들기 네, 귀찮습니다. 하지만 레티나 이미지를 포기할 수는 없으니까요. 요즘은 특히 모바일 기기도 해상도가 굉장히 높으니 그에 맞춰 높은 해상도를 제공해주는 것도 좋지 않을까 생각합니다. 그러면 이제 진행해 봅시다. 만들어봅시다!PDF로 내보내기우선 위처럼 키노트를 만들어주세요. 그리고 아래 사진처럼 파일 &gt; 다음으로 보내기 &gt; PDF… 순서대로 클릭을 해주시면 됩니다. 클릭을 해주면 다음과 같은 창이 하나 뜹니다. 여기서 유의하셔야되는 점은 ‘이미지 품질’을 ‘최상’으로 해 두셔야 300ppi로 내보내기가 이루어집니다. NOTE: 여러분이 글자만 있는 상태라면 (아이콘/사진이 없다면) 굳이 이미지 품질을 신경쓰지는 않으셔도 됩니다. PDF에서 글자는 폰트를 내장해 글자 정보 자체로 읽고 쓰기 때문에 이미지 품질로 인한 차이가 발생하지 않습니다. 미리보기로 PDF 열기앞서 내보내기로 만든 PDF 파일을 아래처럼 열어줍시다. 보기만해도 높은 해상도인것을 느낄 수 있습니다. 이제 상단 메뉴바 ‘파일’에서 ‘보내기’를 눌러봅시다. 이제 아래와 같은 화면이 보일텐데요, PDF가 아니라 JPEG로 바꿔준 뒤 해상도를 150정도로 맞춰준 뒤 저장을 해줍시다. 끝났습니다!이제 여러분은 여러분이 원하는 해상도의 고해상도 이미지를 얻게 되었습니다! 파일 사이즈를 조절하거나 무손실 압축 프로그램등을 사용한다면 좀 더 트래픽량이 줄어 사이트 로딩속도가 빨라질 수 있습니다. macOS에서는 ImageOptim을 이용해보세요. 이미지가 단순한 경우 용량이 절반으로 줄어들기도 합니다.","link":"/2017/11/10/Export-HighQuality-Image-with-KeyNote/"},{"title":"한글이 보이는 Flask CSV Response 만들기","text":"들어가며웹 사이트를 만들다 보면 테이블 등을 csv파일로 다운받을 수 있도록 만들어달라는 요청이 자주 있습니다. 이번 글에서는 Flask에서 특정 URL로 들어갈 때 CSV파일을 받을 수 있도록 만들고, 다운받은 CSV파일을 엑셀로 열 때 한글이 깨지지 않게 처리해 봅시다. 이번에는 Flask + SQLAlchemy + Pandas를 사용합니다. Flask 코드짜기우선 Flask 코드를 하나 봅시다. app.py라는 이름을 갖고 있다고 생각해 봅시다. 아래 코드는 Post라는 모델을 모두 가져와 df라는 DataFrame객체로 만든 뒤 .to_csv를 통해 csv 객체로 만들어 준 뒤 StringIO를 통해 실제 io가 가능한 바이너리형태로 만들어 줍니다. 또, output을 해주기 전 u'\\ufeff'를 미리 넣어줘 이 파일이 ‘UTF-8 with BOM’이라는 방식으로 인코딩 되어있다는 것을 명시적으로 알려줍니다. 인코딩 명시를 빼면 엑셀에서 파일을 열 때 한글이 깨져서 나옵니다. 12345678910111213141516171819202122232425262728# app.pyfrom io import StringIOfrom flask import Flask, jsonify, request, Responsefrom flask_sqlalchemy import SQLAlchemyapp = Flask(__name__) # Flask App 만들기app.config['SQLALCHEMY_DATABASE_URI'] = '데이터베이스 URI' # SQLAlchemy DB 연결하기db = SQLAlchemy()db.init_app(app)# 기타 설정을 해줬다고 가정합니다.@app.route('/api/post/csv/') # URL 설정하기def post_list_csv(self): queryset = Post.query.all() df = pd.read_sql(queryset.statement, queryset.session.bind) # Pandas가 SQL을 읽도록 만들어주기 output = StringIO() output.write(u'\\ufeff') # 한글 인코딩 위해 UTF-8 with BOM 설정해주기 df.to_csv(output) # CSV 파일 형태로 브라우저가 파일다운로드라고 인식하도록 만들어주기 response = Response( output.getvalue(), mimetype=\"text/csv\", content_type='application/octet-stream', ) response.headers[\"Content-Disposition\"] = \"attachment; filename=post_export.csv\" # 다운받았을때의 파일 이름 지정해주기 return response","link":"/2017/11/28/Flask-CSV-Response/"},{"title":"깃헙 Pages에 깃북 배포하기","text":"들어가며이번에 ‘나만의 웹 크롤러 만들기’ 가이드 시리즈를 이 블로그에서 관리하던 중, 글이 파편화되어있는 상황이며 가이드가 유의미하게 이어진다는 느낌이 적어서 깃북을 통해 새로 가이드를 배포하기로 결정했다. 깃북의 경우 https://gitbook.io에서 제공하는 자체 호스팅 서비스가 있고 오픈소스로 정적 사이트를 제작하는 Gitbook 프로젝트도 있다. 깃북 웹 사이트의 경우 느려지는 경우도 종종 있어 좀 더 관리의 범위가 넓은 깃헙에서 깃북 레포를 통해 깃북을 관리하려고 생각했다. Gitbook 설치하기우선 깃북의 경우 node.js기반이기 때문에 시스템에 node와 npm이 설치되어있어야 한다. npm은 node.js를 설치할때 보통 같이 설치된다. 글쓴날짜 기준 9.2.0버전이 node.js의 최신 버전이다. Node.js설치하기: https://nodejs.org/en/ 시스템에 npm이 설치 완료되었다면 이제 아래 명령어로 gitbook-cli를 설치해 주자. 12# 콘솔 / cmd / terminal에서 아래줄을 입력후 엔터!npm install gitbook-cli -g Gitbook Init깃북은 SUMMARY.md파일을 통해 화면 좌측의 내비게이션/목록 부분을 만든다. 한번에 폴더와 기본파일을 모두 생성하려면 아래 명령어를 입력해 주자. 123# 콘솔 / cmd / terminal에서 아래줄을 입력후 엔터!gitbook init my_gitbook# 사용법: gitbook init 사용하려는폴더이름 위 명령어를 입력하면 현재 위치 아래 my_gitbook이라는 폴더가 생기고, 그 안에 README.md와 SUMMARY.md가 생긴다. Git Init깃북을 배포하려면 깃헙에 레포지토리를 만들고 파일을 올려야 하기 때문에 우선 git init으로 현재 폴더를 git이 관리하도록 만들어준다. gh-pages 브랜치 만들기Github Pages의 경우 크게 3가지 방법으로 호스팅을 진행한다. 유저이름.github.io라는 레포의 master브랜치 어떤 레포든 docs 폴더 어떤 레포든 gh-pages 브랜치 위 세가지가 충족되는 경우 자동으로 깃헙 페이지용도로 인식하고 https://유저이름.github.io/레포명 주소로 정적 호스팅을 진행해 준다. 이번에는 세번째 방법인 gh-pages 브랜치를 이용한다. 물론 1,2,3번 모두 커스텀 도메인 사용이 가능하다. publish_gitbook.sh 만들기README.md가 있는 곳 옆에 publish_gitbook.sh파일을 다음 내용으로 만들어 주자. 123456789101112131415161718192021222324252627# gitbook 의존 파일을 설치하고 gitbook 빌드를 돌린다.gitbook install &amp;&amp; gitbook build# github pages가 바라보는 gh-pages 브랜치를 만든다.git checkout gh-pages# 최신 gh-pages 브랜치 정보를 가져와 rebase를 진행한다.git pull origin gh-pages --rebase# gitbook build로 생긴 _book폴더 아래 모든 정보를 현재 위치로 가져온다.cp -R _book/* .# node_modules폴더와 _book폴더를 지워준다.git clean -fx node_modulesgit clean -fx _book# NOQAgit add .# 커밋커밋!git commit -a -m \"Update docs\"# gh-pages 브랜치에 PUSH!git push origin gh-pages# 다시 master 브랜치로 돌아온다.git checkout master 위와 같이 publish_gitbook.sh파일을 만들어 주면, 앞으로 작업을 끝낼때마다 ./publish_gitbook.sh라는 명령어로 한번에 깃헙에 작업한 결과물을 빌드해 올릴 수 있다. SUMMARY.md 관리하기깃북이 파일을 관리하는 것은 폴더별 관리라기보다는 SUMMARY.md파일 내의 정보를 기반으로 URL을 만들고 글의 순서와 목차를 관리한다. 보통 카테고리/챕터별로 폴더를 만들어 관리하는 방법을 사용하는 것 같은데, 나만의 웹 크롤러 만들기 깃북의 경우 아래와 같은 형태를 사용하고 있다. 12345678910111213# Summary- [나만의 웹 크롤러 만들기 시리즈](README.md) - [requests와 BeautifulSoup으로 웹 크롤러 만들기](posts/2017-01-20-HowToMakeWebCrawler.md) - [Session을 이용해 로그인하기](posts/2017-01-20-HowToMakeWebCrawler-With-Login.md) - [Selenium으로 무적 크롤러 만들기](posts/2017-02-27-HowToMakeWebCrawler-With-Selenium.md) - [Django로 크롤링한 데이터 저장하기](posts/2017-03-01-HowToMakeWebCrawler-Save-with-Django.md) - [웹페이지 업데이트를 알려주는 Telegram 봇](posts/2017-04-20-HowToMakeWebCrawler-Notice-with-Telegram.md) - [N배빠른 크롤링, multiprocessing](posts/2017-07-05-HowToMakeWebCrawler-with-Multiprocess.md) - [Headless 크롬으로 크롤링하기](posts/2017-09-28-HowToMakeWebCrawler-Headless-Chrome.md)- Tips - [Selenium Implicitly wait vs Explicitly wait](posts/2017-10-29-HowToMakeWebCrawler-ImplicitWait-vs-ExplicitWait.md) 위와 같이 -를 사용해 글과 카테고리를 구별하고 스페이스를 통해 제목과 링크를 마크다운 문법으로 걸어주면 깃북이 이 파일을 읽고 각각의 파일을 html파일로 만들어준다. 유의할점깃북에서 ![](이미지링크)와 같은 방식을 사용하면 ‘/‘로 시작하는 절대경로 URL을 상대경로인 ‘../‘로 변환해버리는 문제가 있다. 따라서 SUMMARY.md파일과 같은 위치에 book.json파일을 만들어주고 전역 변수를 사용할 수 있다. book.json 관리하기깃북은 book.json파일이 세팅 파일이다. 플러그인을 넣고, 지우고, 전역 변수 등을 설정할 수도 있다. 다음은 Google Analytics 플러그인을 ‘넣고’ 소셜 공유 아이콘을 ‘빼고’, ‘BASE_URL’에 블로그 절대경로를 만들기 위해 URL을 등록해둔 부분이다. 토큰은 Google Analytics에서 받아서 넣어주면 된다. 1234567891011{ \"plugins\": [\"ga\", \"-sharing\"], \"pluginsConfig\": { \"ga\": { \"token\": \"UA-12341234-1\" } }, \"variables\": { \"BASE_URL\": \"https://beomi.github.io\" }}","link":"/2017/11/20/Deploy-Gitbook-to-Github-Pages/"},{"title":"PySpark & Hadoop: 2) EMR 클러스터 띄우고 PySpark로 작업 던지기","text":"이번 글은 PySpark &amp; Hadoop: 1) Ubuntu 16.04에 설치하기와 이어지는 글입니다. 들어가며이전 글에서 우분투에 JAVA/Hadoop/PySpark를 설치해 spark를 통해 EMR로 작업을 던질 EC2를 하나 생성해보았습니다. 이번에는 동일한 VPC그룹에 EMR 클러스터를 생성하고 PySpark의 yarn설정을 통해 원격 EMR에 작업을 던져봅시다. EMR 클러스터 띄우기AWS 콘솔에 들어가 EMR을 검색해 EMR 대시보드로 들어갑시다. EMR 대시보드에서 ‘클러스터 생성’을 클릭해주세요. 이제 아래와 같이 클러스터이름을 적어주고, 시작 모드를 ‘클러스터’로, 릴리즈는 최신 릴리즈 버전(현 5.10이 최신)으로, 애플리케이션은 Spark를 선택해주세요. 그리고 EC2 키 페어를 갖고있다면 기존에 갖고있는 .pem파일을, 없다면 새 키를 만들고 진행하세요. 주황색 표시 한 부분 외에는 기본 설정값 그대로 두면 됩니다. 로깅은 필요한 경우 켜고 필요하지 않은 경우 꺼두면 됩니다. 그리고 할 작업에 따라 인스턴스 유형을 r(많은 메모리), c(많은 CPU), i(많은 스토리지), p(GPU)중 선택하고 인스턴스 개수를 원하는 만큼 선택해주면 됩니다. 많으면 많을수록 Spark작업이 빨리 끝나는 한편 비용도 그만큼 많이 듭니다. 여기서는 기본값인 r3.xlarge 인스턴스 3개로 진행해 봅시다. 인스턴스 3대가 생성되면 한대는 Master 노드가, 나머지 두대는 Core 노드가 됩니다. 앞으로 작업을 던지고 관리하는 부분은 모두 Master노드에서 이루어집니다. 설정이 끝나고 나면 아래 ‘클러스터 생성’ 버튼을 눌러주세요. 클러스터가 시작되고 ‘준비’ 단계가 될 때까지는 약간의 시간(1~3분)이 걸립니다. ‘마스터 퍼블릭 DNS’가 화면에 뜰 때까지 잠시 기다려 줍시다. 클러스터가 준비가 완료되면 아래와 같이 ‘마스터 퍼블릭 DNS’ 주소가 나옵니다. ‘마스터 퍼블릭 DNS’는 앞으로 설정할때 자주 사용하기 때문에 미리 복사를 해 둡시다. 12# 이번에 만들어진 클러스터의 마스터 퍼블릭 DNSec2-13-124-83-135.ap-northeast-2.compute.amazonaws.com 이렇게 나오면 우선 클러스터를 사용할 준비가 완료된 것으로 볼 수 있습니다. 이제 다시 앞 글에서 만든 EC2를 설정해봅시다. EC2 설정 관리하기이제 EMR 클러스터가 준비가 완료되었으니 EC2 인스턴스에 다시 ssh로 접속을 해 봅시다. 이전 편인 PySpark &amp; Hadoop: 1) Ubuntu 16.04에 설치하기글을 읽고 따라 왔다면 여러분의 EC2에는 아마 JAVA와 PySpark, 그리고 Hadoop이 설치가 되어있을겁니다. 우리는 Hadoop의 yarn을 통해서 EMR 클러스터에 spark작업을 던져주기 때문에 이 부분을 설정을 조금 해줘야 합니다. 이전 편을 따라왔다면 아래 두 파일을 수정해주면 되고, 만약 따로 Hadoop을 설치해줬다면 which hadoop을 통해서 나오는 주소를 약간 수정해 사용해주면 됩니다. 우선 앞서 우리가 Hadoop을 설치해준 곳은 /usr/local/hadoop/bin/hadoop 입니다. 그리고 우리가 수정해줘야 하는 두 파일은 위와 같은 위치에 있는 core-site.xml파일, 그리고 yarn-site.xml 파일입니다. 즉, 절대 경로는 아래와 같습니다. 1234# core-site.xml/usr/local/hadoop/etc/hadoop/core-site.xml# yarn-site.xml/usr/local/hadoop/etc/hadoop/yarn-site.xml 만약 다른 곳에 설치했다면 /하둡을설치한위치/etc/hadoop/ 안의 core-site.xml와 yarn-site.xml을 수정하면 됩니다. core-site.xml 수정하기이제 core-site.xml파일을 수정해 봅시다. core-site.xml에는 다음과 같이 fs.defaultFS라는 name을 가진 property를 하나 추가해주면 됩니다. 그리고 그 값을 hdfs://마스터퍼블릭DNS로 넣어줘야 합니다. 123456789&lt;!-- core-site.xml --&gt;&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ec2-13-124-83-135.ap-northeast-2.compute.amazonaws.com&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 수정은 vim이나 nano등의 편집기를 이용해주세요. yarn-site.xml 수정하기이제 다음 파일인 yarn-site.xml파일을 수정해 봅시다. yarn-site.xml에는 다음과 같이 두가지 설정을 마스터퍼블릭DNS로 넣어줘야 합니다. address에는 포트도 추가적으로 붙여줘야 합니다. 1234567891011&lt;?xml version=\"1.0\"?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;ec2-13-124-83-135.ap-northeast-2.compute.amazonaws.com:8032&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;ec2-13-124-83-135.ap-northeast-2.compute.amazonaws.com&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 이렇게 두 파일을 수정해주었으면 EC2에서 설정을 수정할 부분은 끝났습니다. EMR 클러스터 설정 관리하기같은버전 Python 설치하기Spark에서 파이썬 함수(혹은 파일)을 실행할 때 Spark가 실행되고있는 파이썬 버전과 PySpark등을 통해 Spark 서버로 요청된 파이썬 함수의 버전과 일치하지 않으면 Exception을 일으킵니다. 현재 EMR에 설치되어있는 python3은 3.4버전인데, EC2(Ubuntu 16.04)의 파이썬 버전은 3.5버전이기 때문에 Exception이 발생합니다. 아래 세 가지 방법 중 하나를 선택해 해결해주세요. 첫번째 방법: Ubuntu EC2에 Python3.4를 설치하기Ubuntu16은 공식적으로 Python3.4를 지원하지 않습니다. 하지만 간단한 방법으로 Python3.4를 설치할 수 있습니다. 아래 세 줄을 입력해주세요. 123sudo add-apt-repository ppa:fkrull/deadsnakessudo apt-get updatesudo apt-get install python3.4 -y 이렇게 하면 Python3.4를 사용할 수 있습니다. 막 설치해준 Python3.4에는 아직 pyspark가 설치되어있지 않으니 아래 명령어로 pyspark를 설치해 줍시다. 1python3.4 -m pip install -U pyspark --no-cache 이제 EMR 클러스터에 작업을 던져줍시다. 두번째 방법: EMR을 이루는 인스턴스에 원하는 Python버전(3.5)를 설치하기EMR에 python3.5를 설치해 문제를 해결해 봅시다. 만약 여러분이 Ubuntu 17버전을 사용한다면 기본적으로 Python3.6이 설치되어있기 때문에 아래 코드에서 35 대신 36을 이용해주시면 됩니다. 이제 SSH를 통해 EMR Master에 접속해 봅시다. 123chmod 400 sshkey.pem # ssh-add는 권한을 따집니다. 400으로 읽기권한만 남겨두세요.ssh-add sshkey.pem # 여러분의 .pem 파일 경로를 넣어주세요.ssh hadoop@ec2-13-124-83-135.ap-northeast-2.compute.amazonaws.com 로그인을 하고 나서 파이썬 버전을 알아봅시다. 파이썬 버전은 아래 사진처럼 볼 수 있습니다. 파이썬이 3.4버전인것을 확인할 수 있습니다. 한편, EC2의 파이썬은 아래와 같이 3.5버전입니다. 이제 EMR에 Python3.5를 설치해 줍시다. 1sudo yum install python35 위 명령어를 입력하면 python3.5버전이 설치됩니다. Y/N을 물어보면 y를 눌러줍시다. python3 -V를 입력해보면 성공적으로 파이썬 3.5버전이 설치된 것을 볼 수 있습니다. 이 과정을 Master / Core 각 인스턴스별로 진행해주시면 됩니다. SSH로 접속 후 python35만 설치하면 됩니다. 이제 EMR 클러스터에 작업을 던져줍시다. 세번째 방법: EMR 클러스터 부트스트랩 이용하기두번째 방법과 같이 EMR 클러스터를 이루는 인스턴스 하나하나에 들어가 설치를 진행하는 것은 굉장히 비효율적입니다. 그래서 EMR 클러스터가 생성되기 전에 두번째 방법에서와 같이 EMR 클러스터 내에 Python35, Python36을 모두 설치해두면 앞으로도 문제가 없을거에요. 이때 사용할 수 있는 방법이 ‘bootstrap action’ 입니다. bootstrap action은 EMR 클러스터가 생성되기 전 .sh같은 쉘 파일등을 실행할 수 있습니다. 우선 우리가 실행해줄 installpy3536.sh 파일을 로컬에서 하나 만들어 줍시다. 12345#!/bin/bashsudo yum install python34 -ysudo yum install python35 -ysudo yum install python36 -y EMR 클러스터는 아마존리눅스상에서 돌아가기 때문에 yum을 통해 패키지를 설치할 수 있습니다. 각각 python3.4/3.5/3.6버전을 받아 설치해주는 명령어입니다. 이 파일을 s3에 올려줍시다. 우선 빈 버킷 혹은 기존 버킷에 파일을 올려주세요. 파일 권한은 기본 권한 그대로 두면 됩니다. 그리고 이 파일은 AWS 외부에서 접근하지 않기 때문에 퍼블릭으로 해둘 필요는 없습니다. 이제 EMR을 실행하러 가 봅시다. 앞서서는 ‘빠른 옵션’을 이용했지만 이제 ‘고급 옵션’을 이용해야 합니다. 고급 옵션에서 소프트웨어 구성을 다음과 같이 체크하고 ‘다음’을 눌러줍시다. 다음 단계인 ‘하드웨어’는 기본값 혹은 필요한 만큼 설정해준 뒤 ‘다음’을 눌러줍시다. 여기서는 기본값으로 넣어줬습니다. 이번 단계인 ‘일반 클러스터 설정’이 중요합니다. 여기에서 ‘부트스트랩 작업’을 누르고 ‘사용자 지정 작업’을 선택해주세요. ‘사용자 지정 작업’을 선택한 뒤 ‘구성 및 추가’를 눌러주세요. 추가를 누르면 다음과 같이 ‘이름’, ‘스크립트 위치’를 찾아줘야 합니다. 이름을 InstallPython343536이라고 지어봅시다. 이제 스크립트 옆 폴더 버튼을 눌러줍시다. 아까 만든 installpy3536.sh파일이 있는 버킷에 찾아들어가 installpy3536.sh 파일을 선택해줍시다. 선택을 눌러준 뒤 ‘추가’를 눌러줍시다. 아래와 같이 ‘부트스트랩 작업’에 추가되었다면 ‘다음’을 눌러 클러스터를 만들어 줍시다. 이제 마지막으로 SSH접속을 위한 키 페어를 선택한 후 ‘클러스터 생성’을 눌러줍시다. 이제 생성된 EMR 클러스터에는 python3.4/3.5/3.6이 모두 설치되어있습니다. 이 버전 선택은 아래 PYSPARK_PYTHON 값을 설정할때 변경해 사용하면 됩니다. ubuntu 유저 만들고 hadoop그룹에 추가하기python 설치를 마쳤다면 이제 ubuntu유저를 만들어줘야 합니다. EMR 클러스터 마스터 노드에 작업을 추가해 줄 경우 기본적으로 작업을 실행한 유저(우분투 EC2에서 요청시 기본 유저는 ubuntu)의 이름으로 마스터 노드에서 요청한 유저의 홈 폴더를 찾습니다. 만약 EC2에서 EMR로 요청한다면 ubuntu라는 계정 이름으로 EMR 마스터 노드에서 /home/ubuntu라는 폴더를 찾아 이 폴더에 작업할 파이썬 파일과 의존 패키지 등을 두고 작업을 진행합니다. 하지만 EMR은 기본적으로 hadoop이라는 계정을 사용하고, 따라서 ubuntu라는 유저는 추가해줘야 합니다. 그리고 우리가 새로 만들어준 ubuntu 유저는 하둡에 접근할 권한이 없기 때문에 이 유저를 hadoop그룹에 추가해줘야 합니다. 위 사진처럼 두 명령어를 입력해 줍시다. 12sudo adduser ubuntusudo usermod -a -G hadoop ubuntu 첫 명령어는 ubuntu라는 유저를 만들고 다음에서 hadoop이라는 그룹에 ubuntu유저를 추가합니다. 이제 우리는 EC2에서 EMR로 분산처리할 함수들을 보낼 수 있습니다. 마무리: 파이(pi) 계산 예제 실행하기한번 PySpark의 기본 예제중 하나인 pi(원주율) 계산을 진행해 봅시다. 공식 예제: https://github.com/apache/spark/blob/master/examples/src/main/python/pi.py 공식 예제는 스파크와 하둡을 로컬에서 사용합니다. 하지만 우리는 EMR 클러스터에 작업을 던져줄 것이기 때문에 약간 코드를 변경해줘야 합니다. 123456789101112131415161718192021222324252627282930313233343536373839# pi.pyimport sysfrom random import randomfrom operator import addimport osos.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python34\" # python3.5라면 /usr/bin/python35from pyspark.sql import SparkSessionif __name__ == \"__main__\": \"\"\" Usage: pi [partitions] \"\"\" # 이 부분을 추가해주시고 spark = SparkSession \\ .builder \\ .master(\"yarn\") \\ .appName(\"PySpark\") \\ .getOrCreate() # 이부분을 주석처리해주세요. #spark = SparkSession\\ # .builder\\ # .appName(\"PythonPi\")\\ # .getOrCreate() partitions = int(sys.argv[1]) if len(sys.argv) &gt; 1 else 2 n = 100000 * partitions def f(_): x = random() * 2 - 1 y = random() * 2 - 1 return 1 if x ** 2 + y ** 2 &lt;= 1 else 0 count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add) print(\"Pi is roughly %f\" % (4.0 * count / n)) spark.stop() 기존 코드는 builder를 통해 로컬에서 작업을 던져주지만 이렇게 .master(&quot;yarn&quot;)을 추가해주면 yarn 설정을 통해 아래 작업이 EMR 클러스터에서 동작하게 됩니다. EC2상에서 아래 명령어로 위 파이썬 파일을 실행해 봅시다. 1python3.4 pi.py 실행을 해 보면 결과가 잘 나오는 것을 볼 수 있습니다. 만약 두번째/세번째 방법으로 Python3.5를 설치해주셨다면 별다른 설정 없이 python3 pi.py로 실행하셔도 됩니다. 자주 보이는 에러/경고WARN yarn.Client: Same path resource새 task를 Spark로 넘겨줄 때 마다 패키지를 찾기 때문에 나오는 에러입니다. 무시해도 됩니다. Initial job has not accepted any resourcesEMR 설정 중 spark.dynamicAllocation.enabled가 True일 경우 생기는 문제입니다. 위 pi.py파일 코드를 일부 수정해주세요. 기존에 있던 spark 생성하는 부분에 아래 config 몇줄을 추가해주세요. 1234567891011121314151617# 기존 코드를 지우고# spark = SparkSession \\# .builder \\# .master(\"yarn\") \\# .appName(\"PySpark\") \\# .getOrCreate()# 아래 코드로 바꿔주세요.spark = SparkSession.builder \\ .master(\"yarn\") \\ .appName(\"PySpark\") \\ .config(\"spark.executor.memory\", \"512M\") \\ .config(\"spark.yarn.am.memory\", \"512M\") \\ .config(\"spark.executor.cores\", 2) \\ .config(\"spark.executor.instances\", 1) \\ .config(\"spark.dynamicAllocation.enabled\", False) \\ .getOrCreate() 이때 각 config별로 설정되는 값은 여러분이 띄운 EMR에 따라 설정해줘야 합니다. 만약 여러분이 r3.xlarge를 선택했다면 8개의 vCPU, 30.5 GiB 메모리를 사용하기 때문에 저 설정 숫자들을 높게 잡아도 되지만, 만약 c4.large를 선택했다면 2개의 vCPU, 3.8 GiB 메모리를 사용하기 때문에 코드에서 설정한 CPU코어수 혹은 메모리 용량이 클러스터의 CPU개수와 메모리 용량을 초과할 경우 에러가 납니다.","link":"/2017/11/27/EMR-and-PySpark/"},{"title":"HTML Table을 CSV로 다운로드하기","text":"들어가며웹 개발을 하다보면 &lt;table&gt;의 내용물을 모두 csv 파일로 받게 해달라는 요구사항이 종종 생깁니다. 가장 일반적인 방법은 csv 형태로 파일을 받을 수 있는 API를 서버가 제공해주는 방법입니다. (백엔드 개발자에게 일을 시킵시다.) 하지만 csv를 던져주는 API 서버가 없다면 프론트에서 보여지는 &lt;table&gt;만이라도 csv로 만들어줘야 합니다. 이번 글은 이럴때 쓰는 방법입니다. 소스코드우선 이렇게 생긴 HTML이 있다고 생각해 봅시다. 본문이라고는 #, title, content가 들어있는 자그마한 &lt;table&gt; 하나가 있습니다. 1234567891011121314151617181920212223242526272829303132&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang=\"ko\"&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;빈 HTML&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;table id=\"mytable\"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;title&lt;/th&gt; &lt;th&gt;content&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;Lorem Ipsum&lt;/td&gt; &lt;td&gt;로렘 입섬은 빈칸을 채우기 위한 문구입니다.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;Hello World&lt;/td&gt; &lt;td&gt;헬로 월드는 언어를 배우기 시작할때 화면에 표준 출력을 할때 주로 사용하는 문구입니다.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;button id=\"csvDownloadButton\"&gt;CSV 다운로드 받기&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; 테이블 엘리먼트의 id는 mytable이고, CSV 다운로드 버튼의 id는 csvDownloadButton 입니다. 이제 JS를 조금 추가해봅시다. ES6/ES5에 따라 선택해 사용해주세요. 아래 코드를 &lt;script&gt;&lt;/script&gt; 태그 사이에 넣어 &lt;/body&gt; 바로 앞에 넣어주세요. ES6을 사용할 경우123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class ToCSV { constructor() { // CSV 버튼에 이벤트 등록 document.querySelector('#csvDownloadButton').addEventListener('click', e =&gt; { e.preventDefault() this.getCSV('mycsv.csv') }) } downloadCSV(csv, filename) { let csvFile; let downloadLink; // CSV 파일을 위한 Blob 만들기 csvFile = new Blob([csv], {type: \"text/csv\"}) // Download link를 위한 a 엘리먼스 생성 downloadLink = document.createElement(\"a\") // 다운받을 csv 파일 이름 지정하기 downloadLink.download = filename; // 위에서 만든 blob과 링크를 연결 downloadLink.href = window.URL.createObjectURL(csvFile) // 링크가 눈에 보일 필요는 없으니 숨겨줍시다. downloadLink.style.display = \"none\" // HTML 가장 아래 부분에 링크를 붙여줍시다. document.body.appendChild(downloadLink) // 클릭 이벤트를 발생시켜 실제로 브라우저가 '다운로드'하도록 만들어줍시다. downloadLink.click() } getCSV(filename) { // csv를 담기 위한 빈 Array를 만듭시다. const csv = [] const rows = document.querySelectorAll(\"#mytable table tr\") for (let i = 0; i &lt; rows.length; i++) { const row = [], cols = rows[i].querySelectorAll(\"td, th\") for (let j = 0; j &lt; cols.length; j++) row.push(cols[j].innerText) csv.push(row.join(\",\")) } // Download CSV this.downloadCSV(csv.join(\"\\n\"), filename) }}document.addEventListener('DOMContentLoaded', e =&gt; { new ToCSV()}) ES5를 사용하실 경우123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051function downloadCSV(csv, filename) { var csvFile; var downloadLink; // CSV 파일을 위한 Blob 만들기 csvFile = new Blob([csv], {type: \"text/csv\"}) // Download link를 위한 a 엘리먼스 생성 downloadLink = document.createElement(\"a\") // 다운받을 csv 파일 이름 지정하기 downloadLink.download = filename; // 위에서 만든 blob과 링크를 연결 downloadLink.href = window.URL.createObjectURL(csvFile) // 링크가 눈에 보일 필요는 없으니 숨겨줍시다. downloadLink.style.display = \"none\" // HTML 가장 아래 부분에 링크를 붙여줍시다. document.body.appendChild(downloadLink) // 클릭 이벤트를 발생시켜 실제로 브라우저가 '다운로드'하도록 만들어줍시다. downloadLink.click()}function getCSV(filename) { // csv를 담기 위한 빈 Array를 만듭시다. var csv = [] var rows = document.querySelectorAll(\"#mytable table tr\") for (var i = 0; i &lt; rows.length; i++) { var row = [], cols = rows[i].querySelectorAll(\"td, th\") for (var j = 0; j &lt; cols.length; j++) row.push(cols[j].innerText) csv.push(row.join(\",\")) } // Download CSV downloadCSV(csv.join(\"\\n\"), filename)}document.addEventListener('DOMContentLoaded', e =&gt; { // CSV 버튼에 이벤트 등록 document.querySelector('#csvDownloadButton').addEventListener('click', e =&gt; { e.preventDefault() getCSV('mycsv.csv') })}) 전체 예시 예제 html_to_csv.html에서 직접 동작하는 것을 확인해 보세요! 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang=\"ko\"&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;빈 HTML&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;table id=\"mytable\"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;title&lt;/th&gt; &lt;th&gt;content&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;Lorem Ipsum&lt;/td&gt; &lt;td&gt;로렘 입섬은 빈칸을 채우기 위한 문구입니다.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;Hello World&lt;/td&gt; &lt;td&gt;헬로 월드는 언어를 배우기 시작할때 화면에 표준 출력을 할때 주로 사용하는 문구입니다.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;button id=\"csvDownloadButton\"&gt;CSV 다운로드 받기&lt;/button&gt;&lt;/body&gt;&lt;script type=\"text/javascript\"&gt; class ToCSV { constructor() { // CSV 버튼에 이벤트 등록 document.querySelector('#csvDownloadButton').addEventListener('click', e =&gt; { e.preventDefault() this.getCSV('mycsv.csv') }) } downloadCSV(csv, filename) { let csvFile; let downloadLink; // CSV 파일을 위한 Blob 만들기 csvFile = new Blob([csv], {type: \"text/csv\"}) // Download link를 위한 a 엘리먼스 생성 downloadLink = document.createElement(\"a\") // 다운받을 csv 파일 이름 지정하기 downloadLink.download = filename; // 위에서 만든 blob과 링크를 연결 downloadLink.href = window.URL.createObjectURL(csvFile) // 링크가 눈에 보일 필요는 없으니 숨겨줍시다. downloadLink.style.display = \"none\" // HTML 가장 아래 부분에 링크를 붙여줍시다. document.body.appendChild(downloadLink) // 클릭 이벤트를 발생시켜 실제로 브라우저가 '다운로드'하도록 만들어줍시다. downloadLink.click() } getCSV(filename) { // csv를 담기 위한 빈 Array를 만듭시다. const csv = [] const rows = document.querySelectorAll(\"#mytable tr\") for (let i = 0; i &lt; rows.length; i++) { const row = [], cols = rows[i].querySelectorAll(\"td, th\") for (let j = 0; j &lt; cols.length; j++) row.push(cols[j].innerText) csv.push(row.join(\",\")) } // Download CSV this.downloadCSV(csv.join(\"\\n\"), filename) }}document.addEventListener('DOMContentLoaded', e =&gt; { new ToCSV()})&lt;/script&gt;&lt;/html&gt; 하지만 이렇게하면 한글이 깨지는 문제가 있습니다. 한글 깨지는 문제 해결하기앞서 한글이 깨지는 이유는 기본적으로 엑셀이 인코딩을 UTF-8로 인식하지 않기 때문에 문제가 발생합니다. 이때 아래 코드를 csv blob을 만들기 전 추가해주면 됩니다. 123// 한글 처리를 해주기 위해 BOM 추가하기const BOM = \"\\uFEFF\";csv = BOM + csv 123456789101112131415161718192021222324252627282930// downloadCSV 함수를 이렇게 수정해 주세요.downloadCSV(csv, filename) { let csvFile; let downloadLink; // 한글 처리를 해주기 위해 BOM 추가하기 const BOM = \"\\uFEFF\"; csv = BOM + csv // CSV 파일을 위한 Blob 만들기 csvFile = new Blob([csv], {type: \"text/csv\"}) // Download link를 위한 a 엘리먼스 생성 downloadLink = document.createElement(\"a\") // 다운받을 csv 파일 이름 지정하기 downloadLink.download = filename; // 위에서 만든 blob과 링크를 연결 downloadLink.href = window.URL.createObjectURL(csvFile) // 링크가 눈에 보일 필요는 없으니 숨겨줍시다. downloadLink.style.display = \"none\" // HTML 가장 아래 부분에 링크를 붙여줍시다. document.body.appendChild(downloadLink) // 클릭 이벤트를 발생시켜 실제로 브라우저가 '다운로드'하도록 만들어줍시다. downloadLink.click()}","link":"/2017/11/29/HTML-Table-to-CSV/"},{"title":"TreeShaking으로 webpack 번들 결과 용량 줄이기","text":"이번 글은 webpack을 사용하고 있다고 가정합니다. 만약 webpack이 뭔지 아직 모르시거나 설치하지 않으셨다면 Webpack과 Babel로 최신 JavaScript 웹프론트 개발환경 만들기를 먼저 읽고 따라가보세요. 들어가며웹 프론트 개발을 할 때 npm과 webpack을 통해 bundle.js와 같은 번들링된 js파일 하나로 만들어 싱글 페이지 앱을 만드는 경우가 많습니다. 우리가 사용하는 패키지들을 찾아 간단하게 묶고 babel을 통해 하위버전 브라우저에서도 돌아가도록 만들어주는 작업은 마치 마법과 같이 편리합니다. 하지만 이 마법같은 번들링에도 심각한 문제점이 있습니다. 바로 용량이 어마어마해진다는 것이죠. 아무런 처리를 하지 않고 webpack으로 빌드를 할 때의 용량은 스크린샷에 나온 것처럼 무려 1.61MB됩니다. 사실 아직 lodash, bootstrap3, axios와 같은 아주 기본적인 라이브러리들만 넣었음에도 다음과 같이 어마어마하게 무거운 js파일이 생성됩니다. 이제 이 파일을 1/3 크기로 줄여봅시다. uglifyjs-webpack-pluginwebpack과 함께 파일의 용량을 줄여주는 도구인 uglifyjs를 사용해봅시다. 우선 다음 명령어로 uglifyjs-webpack-plugin를 설치해주세요. 1npm install --save-dev uglifyjs-webpack-plugin webpack 실행시 자동으로 용량줄이기여러분이 webpack을 사용하고 있다면 아마 다음과 같은 webpack.config.js파일을 만들어 사용하고 있을거에요. (세부적인 설정은 다를 수 있어요.) 12345678910111213141516171819202122232425262728const webpack = require('webpack');const path = require('path');module.exports = { entry: './src/js/index.js', output: { path: path.resolve(__dirname, 'dist'), publicPath: '/dist/', filename: 'bundle.js' }, module: { rules: [ { test: /\\.js$/, include: path.join(__dirname, 'src'), exclude: /(node_modules)|(dist)/, use: { loader: 'babel-loader', options: { presets: [ [\"env\"] ] } } } ] }} 위 설정은 단순히 src파일 안의 js들을 dist폴더 안의 bundle.js파일로 묶어주고 있습니다. 이제 여기에서 몇줄만 추가해 주면 됩니다. 123456789101112131415161718192021222324252627282930313233343536373839const webpack = require('webpack');const path = require('path');// 1. UglifyJSPlugin을 가져오세요.const UglifyJSPlugin = require('uglifyjs-webpack-plugin');module.exports = { entry: './src/js/index.js', output: { path: path.resolve(__dirname, 'dist'), publicPath: '/dist/', filename: 'bundle.js' }, module: { rules: [ { test: /\\.js$/, include: path.join(__dirname, 'src'), exclude: /(node_modules)|(dist)/, use: { loader: 'babel-loader', options: { presets: [ [\"env\", { \"targets\": { \"browsers\": [\"last 2 versions\", \"safari &gt;= 7\"] } }] ] } } } ] }, // 2. plugins를 새로 만들고, new UglifyJsPlugin() 을 통해 // UglifyJS를 빌드 과정에 합쳐주세요. plugins: [ new UglifyJsPlugin() ]} 이제 빌드를 실행해보면 아래 스크린샷과 같이 bundle.js파일의 용량이 획기적으로 줄어든 것을 볼 수 있습니다. 용량이 1.6MB에서 667KB로 1/3정도로 줄어든 것을 볼 수 있습니다. 간단하죠? 하지만 여기에는 작은 함정이 있습니다. 바로 time, 즉 빌드시마다 걸리는 시간도 그에따라 늘어난 것인데요, 만약 여러분이 webpack-dev-server와 같이 실시간으로 파일을 감시하며 변화 발생시마다 빌드하는 방식을 사용하고 있다면 코드 한줄, 띄어쓰기 하나 수정한 정도로 무려 12초에 달하는 빌드 시간을 기다려야 합니다. (treeshaking 하기 전에는 3초정도밖에 걸리지 않았습니다.) 그래서 항상 treeshaking을 해주는 대신 빌드작업, 즉 서버에 실제로 배포하기 위해 bundle.js파일을 생성할 때만 treeshaking을 해주면 개발도 빠르고 실제 배포시에도 빠르게 작업이 가능합니다. 빌드할때만 사용하기앞서 다뤘던 package.json파일 중 script부분 아래 build를 다음과 같이 수정해주세요. 1234567{ ... \"scripts\": { \"build\": \"webpack --optimize-minimize\", }, ...} 그리고 webpack.config.js 파일 중 위에서 넣어주었던 plugins를 통채로 지워주세요.(더이상 필요하지 않아요!) 만약 여러분이 webpack.config.js파일을 정확히 설정해 webpack이라는 명령어가 성공적으로 실행되고있던 상태라면 --optimize-minimize라는 명령어만 뒤에 붙여주면 곧바로 실행됩니다. 이제 여러분이 개발할 때 webpack-dev-server를 통해 빌드가 실행될때는 treeshaking이 되지 않고, 대신 배포를 위해 빌드를 할 때는 최소화된 작은 번들된 js파일을 가질 수 있게 됩니다. 마무리여러분이 위 과정을 모두 따라왔다면 아마 package.json과 webpack.config.js파일은 이와 유사하게 생겼을거에요. 123456789// 앞뒤생략한 package.json{ ... \"scripts\": { \"build\": \"webpack --optimize-minimize\", \"devserver\": \"webpack-dev-server --open\" }, ...} 1234567891011121314151617181920212223242526272829303132333435// webpack.config.js 파일const webpack = require('webpack');const path = require('path');const UglifyJSPlugin = require('uglifyjs-webpack-plugin');module.exports = { entry: './src/js/index.js', output: { path: path.resolve(__dirname, 'dist'), publicPath: '/dist/', filename: 'bundle.js' }, module: { rules: [ { test: /\\.js$/, include: path.join(__dirname, 'src'), exclude: /(node_modules)|(dist)/, use: { loader: 'babel-loader', options: { presets: [ [\"env\", { \"targets\": { \"browsers\": [\"last 2 versions\", \"safari &gt;= 7\"] } }] ] } } } ] } // Plugin은 필요한 것만 넣어주세요. UglifyJSPlugin은 필요없어요!}","link":"/2017/11/29/JS-TreeShaking-with-Webpack/"},{"title":"AWS Lambda에 Tensorflow/Keras 배포하기","text":"Update @ 20190306: amazonlinux:latest 버전이 2버전이 latest로 변경됨에 따라 아래 코드를 amazonlinux:1로 변경 이번 글은 macOS을 기반으로 작성되었지만, docker 명령어를 사용할 수 있는 모든 플랫폼(윈도우/맥/리눅스)에서 따라올 수 있습니다. 들어가며여러분이 이미지를 받아 텐서플로로 분류를 해 준 뒤 결과를 반환해주는 작업을 하는 모델을 만들었다고 가정해봅시다. 이때 가장 빠르고 간단하게 결과를 내는(Inference/추론을 하는)방법은 Cuda가속을 할 수 있는 고급 시스템 위에서 텐서플로 모델을 이용해 결과를 반환하도록 서비스를 구현할 수 있습니다. 혹은 조금 느리더라도 CPU를 사용하는 EC2와 같은 VM위에서 텐서플로 코드를 동작하게 만들 수도 있습니다. 하지만 만약 여러분이 처리해야하는 이미지가 몇장이 아니라 수십, 수백장을 넘어 수천장을 처리해야한다면 어떻게 될까요? 한 EC2 위에서 서비스를 제공하는 상황에서는 이런 경우라면 for문처럼 이미지 하나하나를 돌며 추론을 한다면 전체 이미지의 결과를 내려면 한참 시간이 걸리게 됩니다. 따라서 일종의 병렬 처리를 생각해 보아야 합니다. 즉, 이미지를 순서대로 하나씩 추론하는 대신, 추론하는 코어 함수만을 빼고 결과를 반환하도록 만들어 주면 됩니다. 물론 병렬 처리를 위해서 여러가지 방법들이 있습니다. 여러개의 GPU를 사용하고 있다면 각 GPU별로 작업을 진행하도록 할 수도 있고, 혹은 EC2를 여러개 띄워서 작업을 분산해 진행할 수도 있습니다. 하지만 이 방법보다 조금 더(혹은 상당히 많이) 빠르게 결과를 얻어낼 수 있는 방법이 있습니다. 바로 AWS Lambda를 이용하는 방법입니다. AWS Lambda는 현재 각 실행별 최대 3GB메모리와 5분의 실행시간 내에서 원하는 코드를 실행해 한 리전에서 동시에 최대 1000개까지 병렬로 실행할 수 있습니다. 즉, 우리가 1만개의 이미지를 처리해야 한다면 한 리전에서만 1000개를 동시에 진행해 10개를 처리하는 시간 내 모든 작업을 마칠 수 있다는 것이죠. 그리고 작업이 끝나고 서버가 자동으로 끝나기 때문에 동작하지 않는 시간에도 돈을 내는 EC2와는 가격차이가 많이 나게됩니다. 이번 가이드는 다음과 같은 시나리오로 작성했습니다. 시나리오 s3의 어떤 버킷의 특정한 폴더에 ‘이미지 파일’을 올린다. 이미지 파일이 ‘생성’될 때 AWS Lambda 함수가 실행이 트리거된다. (Lambda) 모델을 s3에서 다운받아 Tensorflow로 읽는다. (Lambda) 함수가 트리거 될때 발생한 event 객체를 받아 s3에 업로드된 파일의 정보(버킷, 버킷내 파일의 경로)를 가져온다. (Lambda) s3에 업로드된 이미지 파일을 boto3을 통해 가져와 Tensorflow로 Inference를 진행한다. (Lambda) Inference가 끝난 결과물을 s3에 저장하거나 결과값을 DynamoDB에 저장하거나 혹은 API Gateway를 통해 json으로 반환한다. 위와같이 진행할 경우 s3에 파일 업로드를 1개를 하든, 1000개를 하든 업로드 자체에 필요한 시간을 제외하면 실행 시간 자체는 동일하게 유지할 수 있습니다. (마치 시간복잡도가 O(1)인 척 할 수 있는 것이죠!) 환경 준비하기 오늘 사용한 예제는 Github tf-keras-on-lambda Repo에서 확인할 수 있습니다. AWS Lambda는 아마존에서 RedHat계열의 OS를 새로 만든 Amazon Linux위에서 동작합니다. 그렇기 때문에 만약 우리가 C의존적인 라이브러리를 사용해야한다면 우선 Amazon Linux에 맞게 pip로 설치를 해줘야 합니다. 그리고 간혹 빌드가 필요한 패키지의 경우 사용하고자 하는 OS에 맞춰 빌드작업 역시 진행해야 합니다. Tensorflow 역시 C의존적인 패키지이기 때문에 OS에 맞는 버전을 받아줘야 합니다. 우리가 사용하는 OS는 macOS혹은 windows이기 때문에 docker를 통해 Amazon Linux를 받아 그 안에서 빌드를 진행합니다. 도커를 사용하고있다면 그대로 진행해주시면 되고, 도커를 설치하지 않으셨다면 우선 도커를 먼저 설치해주세요. 도커는 Docker Community Edition Download Page에서 받으실 수 있습니다. 여러분이 다음부분을 진행하기 전, docker라는 명령어를 터미널 혹은 cmd상에서 입력시 도커가 실행되어야 합니다. 도커가 실행된다면, 우선은 실행할 준비를 마친 것이랍니다. 물론 여러분 각자의 모델파일이 있어야 합니다. 이번 가이드에서는 Keras 예시 중 Pre-trained squeezenet을 이용한 Image Classification(imagenet)으로 진행합니다. 결과 저장용 DynamoDB 만들기(Optional)우리가 predict를 진행하고 나서 나온 결과물을 어딘가에 저장해둬야 합니다. AWS에는 DynamoDB라는 간단한 NoSQL DB가 있으니, 이걸 이용해 결과물을 저장해 봅시다. 우선 DynamoDB 메뉴에서 아래와 같이 새 테이블 하나를 만들어 줍시다. 기본키 정도만 문자열 필드 filename을 만들고 테이블을 생성해 줍시다. 이제 기본키만 지키면 나머지 필드는 자유롭게 올릴 수 있습니다. (물론 기본키와 정렬키만 인덱스가 걸리기 때문에, 빠른 속도가 필요하다면 인덱스를 건 뒤 데이터를 추가해줘야 합니다.) squeezenet ImageNet 모델 s3에 올리기이번 글에서는 squeezenet Imagenet 모델을 이용해 predict를 진행합니다. squeezenet_weights_tf_dim_ordering_tf_kernels.h5파일이 필요한데, AWS Lambda에서 비용이 들지 않으며 빠른 속도로 모델을 받아오기 위해서는 같은 리전의 s3에 파일을 올려둬야 합니다. 이번 글에서는 keras-blog라는 s3 버킷의 squeezenet 폴더에 파일을 올려두었습니다. 각 파일은 아래 링크에서 받을 수 있습니다. 아래 두 파일을 받아 s3 버킷에 올려주세요. squeezenet_weights_tf_dim_ordering_tf_kernels.h5 squeezenet.py 만들기Keras의 squeezenet은 squeezenet.py을 참조합니다. 하지만 이 파일에는 Pre-Trained Model의 경로를 바꿔주기 때문에 이 부분을 약간 수정한 커스텀 squeezenet.py를 만들어주었습니다. 이 파일을 다운받아 여러분의 index.py 옆에 놓아두세요. 수정된 squeezenet.py 도커 + Amazon Linux로 빌드 준비하기이제 docker라는 명령어로 도커를 사용해봅시다. 우선 여러분이 작업할 폴더 하나를 만들어 주세요. 저는 지금 tf_on_lambda라는 폴더에서 진행하고 있습니다. 이 폴더 안에는 Lambda에서 실행할 python파일이 들어가게 되고, 도커와 이 폴더를 이어줄 것이기 때문에 새 폴더 하나를 만들어서 진행하시는 것을 추천합니다. 폴더를 만들고 들어가셨다면 다음 명령어를 입력해 AmazonLinux 이미지를 받아 도커로 띄워주세요. 1docker run -v $(pwd):/outputs --name lambdapack -d amazonlinux:1 tail -f /dev/null 도커 컨테이너의 이름을 lambdapack으로 지정하고 현재 폴더($(pwd))를 도커의 /outputs폴더로 연결해줍니다. 성공적으로 받아졌다면 다음과 같이 임의의 난수 id가 생깁니다. 그리고 docker ps라는 명령어로 현재 실행중인 컨테이너들을 확인해보면 다음과 같이 lambdapack라는 이름을 가진 컨테이너가 생성된 것을 볼 수 있습니다. 람다가 실행할 python 파일 작성하기그러면 이제 람다가 실제로 실행할 python 파일을 만들어줍시다. 이번 가이드에서는 이 파일의 이름을 index.py라고 지어보았습니다. index.py 파일은 사실 어떤 이름으로 해도 상관없습니다만, AWS에 람다 함수를 만들 때 handler함수 위치 지정을 파일이름.함수이름, 즉 index.handler와 같이 적어줄 것이기 때문에 대표적인 이름을 가진 파일로 만들어 주시면 됩니다. index.py안에는 다음과 같은 내용으로 작성해 봅시다. 중요한 부분은 handler함수입니다. 전체 코드를 바로 이용하시려면 index.py on GIST을 이용하세요. 제일 먼저 해줘야 하는 부분은 우리가 사용할 라이브러리를 import하는 것이죠. 123456789101112import boto3 # AWS S3 접근용from tensorflow.python import keras # Keras!from tensorflow.python.keras.preprocessing import imagefrom tensorflow.python.keras.applications.resnet50 import preprocess_input, decode_predictionsimport numpy as npimport io # File 객체를 메모리상에서만 이용하도록import os # os.path / os.environfrom PIL import Image # Image 객체 import urllib.request # 파일받기 # (.h5경로변경추가, 레포의 squeezenet.py를 확인하세요.)from squeezenet import Squeezenet # 커스텀한 squeezenet 이 중 boto3 라이브러리는 AWS Lambda의 python3내에 이미 설치되어있기 때문에 특정 버전의 boto3을 이용하시려는게 아니라면 도커 컨테이너에 설치하지 않아도 됩니다. (즉, Lambda에 올릴 패키지 zip파일에 boto3이 들어있지 않아도 괜찮습니다.) import를 끝냈으니 코드를 작성해 봅시다. 우선 S3에서 파일을 다운로드/업로드하는 함수를 만들어줍시다. 123456789101112131415161718ACCESS_KEY = os.environ.get('ACCESS_KEY')SECRET_KEY = os.environ.get('SECRET_KEY')def downloadFromS3(strBucket, s3_path, local_path): s3_client = boto3.client( 's3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, ) s3_client.download_file(strBucket, s3_path, local_path)def uploadToS3(bucket, s3_path, local_path): s3_client = boto3.client( 's3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, ) s3_client.upload_file(local_path, bucket, s3_path) AWS 콘솔에서 람다 함수별로 환경변수를 설정해줄 수 있기 때문에, 위과 같이 os.environ을 통해 설정한 환경변수 값을 가져옵시다. 물론 여기에 설정한 Access Key와 Secret Key의 iam 유저는 당연히 해당 S3 버킷에 R/W권한이 있어야 합니다. 이 두가지 함수를 통해 S3에서 모델을 가져오고, 모델로 추론한 결과물을 S3에 넣어줄 수 있습니다. Note: s3 버킷과 람다 함수는 같은 Region에 있어야 데이터 전송 비용이 발생하지 않습니다. 이제 가장 중요한 부분인 handler함수를 살펴봅시다. handler함수는 기본적으로 event와 context를 인자로 전달받습니다. 이때 우리가 사용하는 인자는 event인자입니다. 우리의 사용 시나리오 중 ‘S3에 이미지 파일을 올린다’, 이 부분이 바로 event의 내용이 됩니다. S3 Event의 내용AWS에서 Lambda 실행이 트리거 될 때 전달되는 event는 파이썬의 딕셔너리 형태로 전달됩니다. 만약 여러분이 s3에 파일이 추가되는 이벤트를 Lambda에 연결해두셨다면 파일이 업로드 될 때 마다 아래와 같은 딕셔너리가 전달됩니다. 아래 예시는 csv_icon.png를 keras-blog라는 버킷내 wowwow 폴더에 올렸을때 발생한 event 객체입니다. 12345678910111213141516171819202122232425262728293031323334# event 객체{ 'Records': [ { 'eventVersion': '2.0', 'eventSource': 'aws:s3', 'awsRegion': 'ap-northeast-2', # 버킷 리전 'eventTime': '2017-12-13T03:28:13.528Z', # 업로드 완료 시각 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'AFK2RA1O3ML1F'}, 'requestParameters': {'sourceIPAddress': '123.24.137.5'}, 'responseElements': { 'x-amz-request-id': '1214K424C14C384D', 'x-amz-id-2': 'BOTBfAoB/gKBbn412ITN4t2psTW499iMRKZDK/CQTsjrkeSSzSdsDUMGabcdnvHeYNtbTDHoHKs=' }, 's3': { 's3SchemaVersion': '1.0', 'configurationId': 'b249eeda-3d48-4319-a7e2-853f964c1a25', 'bucket': { 'name': 'keras-blog', # 버킷 이름 'ownerIdentity': { 'principalId': 'AFK2RA1O3ML1F' }, 'arn': 'arn:aws:s3:::keras-blog' }, 'object': { 'key': 'wowwow/csv_icon.png', # 버킷 내 파일의 절대경로 'size': 11733, # 파일 크기 'eTag': 'f2d12d123aebda1cc1fk17479207e838', 'sequencer': '125B119E4D7B2A0A48' } } } ]} 여기서 봐야 하는 것은 bucket 내 name와 object내 key입니다. 각각 업로드된 버킷의 이름과 버킷 내 파일이 업로드된 경로를 알려주기 때문에 S3 내 업로드된 파일의 절대경로를 알 수 있습니다. 따라서 handler함수 내 다음과 같이 버킷이름과 버킷 내 파일의 경로를 얻을 수 있습니다. 123456# 윗부분 생략def handler(event, context): bucket_name = event['Records'][0]['s3']['bucket']['name'] # bucket_name은 'keras-blog' 가 됩니다. file_path = event['Records'][0]['s3']['object']['key'] # file_path는 'wowwow/csv_icon.png'가 됩니다. 이를 통해 파일 업로드 이벤트 발생시마다 어떤 파일을 처리해야할 지 알 수 있습니다. s3에서 이미지 파일 받아오기이제 어떤 파일을 처리해야 할지 알 수 있게 되었으니 downloadFromS3 함수를 통해 실제로 파일을 가져와봅시다. 123456# 윗부분 생략def handler(event, context): bucket_name = event['Records'][0]['s3']['bucket']['name'] file_path = event['Records'][0]['s3']['object']['key'] file_name = file_path.split('/')[-1] # csv_icon.png downloadFromS3(bucket_name, file_path, '/tmp/'+file_name) 위 코드를 보면 s3에 올라간 파일을 /tmp안에 받는 것을 볼 수 있습니다. AWS Lambda에서는 ‘쓰기’ 권한을 가진 것은 오직 /tmp폴더뿐이기 때문에 우리가 파일을 받아 사용하려면 /tmp폴더 내에 다운받아야 합니다. (혹은 온메모리에 File 객체로 들고있는 방법도 있습니다.) Prediction Model 다운받기 (Optional) 만약 여러분이 그냥 사용한다면 Github에서 파일을 다운받게 됩니다. 이때 속도가 굉장히 느려 lambda비용이 많이 발생하기 때문에 여러분의 s3 버킷에 실제 파일을 올리고 s3에서 파일을 받아 사용하시는 것을 추천합니다. 우리는 위에서 squeezenet모델을 사용했는데, 이때 모델 가중치를 담은 .h5파일을 먼저 받아야 합니다. handler 함수 내 다음 두 파일을 더 받아줍시다. 12345678910def handler(event, context): bucket_name = event['Records'][0]['s3']['bucket']['name'] file_path = event['Records'][0]['s3']['object']['key'] file_name = file_path.split('/')[-1] downloadFromS3(bucket_name, file_path, '/tmp/'+file_name) downloadFromS3( 'keras-blog', 'squeezenet/squeezenet_weights_tf_dim_ordering_tf_kernels.h5', '/tmp/squeezenet_weights_tf_dim_ordering_tf_kernels.h5' ) # weights용 h5를 s3에서 받아오기 squeezenet 모델로 Predict 하기이제 s3에 올라간 이미지 파일을 Lambda내 /tmp폴더에 받았으니 Predict를 진행해봅시다. predict라는 함수를 아래와 같이 이미지 경로를 받아 결과를 반환하도록 만들어 줍시다. 12345678910111213# index.py 파일, handler함수보다 앞에 def predict(img_local_path): model = Squeezenet(weights='imagenet') img = image.load_img(img_local_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) preds = model.predict(x) res = decode_predictions(preds) return resdef handler(event, context): # 내용 생략 ... predict함수는 squeezenet의 Pre-trained 모델을 이용해 이미지 예측을 진행합니다. 그러면 실제 handler함수에서 predict함수를 실행하도록 수정해줍시다. 123456789101112131415def predict(img_local_path): # 내용 생략 ...def handler(event, context): bucket_name = event['Records'][0]['s3']['bucket']['name'] file_path = event['Records'][0]['s3']['object']['key'] file_name = file_path.split('/')[-1] downloadFromS3(bucket_name, file_path, '/tmp/'+file_name) downloadFromS3( 'keras-blog', 'squeezenet/squeezenet_weights_tf_dim_ordering_tf_kernels.h5', '/tmp/squeezenet_weights_tf_dim_ordering_tf_kernels.h5' ) result = predict('/tmp/'+file_name) # 파일 경로 전달 return result 완성이네요! DynamoDB에 결과 올리기(Optional)위 predict의 결과는 단순히 결과가 생기기만 하고 결과를 저장하거나 알려주는 부분은 없습니다. 이번 글에서는 간단한 예시로 AWS DynamoDB에 쌓아보는 부분을 추가해보겠습니다. predict함수를 통해 생성된 result는 다음과 같은 모습이 됩니다. 12# result[0]의 내용, tuples in list[('n02099712', 'Labrador_retriever', 0.68165195), ('n02099601', 'golden_retriever', 0.18365686), ('n02104029', 'kuvasz', 0.12076716), ('n02111500', 'Great_Pyrenees', 0.0042763283), ('n04409515', 'tennis_ball', 0.002152696)] 따라서 map을 이용해 다음과 같이 바꿔줄 수 있습니다. 12_tmp_dic = {x[1]:{'N':str(x[2])} for x in result[0]}dic_for_dynamodb = {'M': _tmp_dic} 그러면 dic_for_dynamodb는 아래와 같은 형태로 나오게 됩니다. NOTE: 숫자는 float나 int가 아닌 str로 바꾸어 전달해야 오류가 나지 않습니다. DynamoDB의 제약입니다. 123456789{ 'M':{ 'Labrador_retriever': {'N': '0.68165195'}, 'golden_retriever': {'N': '0.18365686'}, 'kuvasz': {'N': '0.12076716'}, 'Great_Pyrenees': {'N': '0.0042763283'}, 'tennis_ball': {'N': '0.002152696'} }} 데이터를 넣기 위해서는 dict타입으로 만든 객체를 put할수 있는데, 이때 각각의 키에 대해 타입을 알려줘야 합니다. M은 이 객체가 dict타입이라는 것을, N은 이 타입이 숫자라는 것을, S는 문자열이라는 것을 의미합니다. 더 상세한 내용은 DynamoDB에 데이터 넣기를 참고하세요. 이제 이 방식을 이용해 result를 반환하기 전 DynamoDB에 데이터를 넣어줄 수 있습니다. 123456789101112131415161718192021def handler(event, context): # 중간 생략 ... result = predict('/tmp/'+file_name) _tmp_dic = {x[1]:{'N':str(x[2])} for x in result[0]} dic_for_dynamodb = {'M': _tmp_dic} dynamo_client = boto3.client( 'dynamodb', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='ap-northeast-2' # DynamoDB는 리전 이름이 필요합니다. ) dynamo_client.put_item( TableName='keras-blog-result', # DynamoDB의 Table이름 Item={ 'filename': { 'S': file_name, }, 'predicts': dic_for_dynamodb, } ) return result 도커로 Lambda에 올릴 pack.zip파일 만들기AWS Lambda에 함수를 올리려면 AmazonLinux에 맞게 pip패키지들을 index.py 옆에 같이 설치해준 뒤 압축파일(.zip)으로 묶어 업로드해야 합니다. 이때 약간의 제약이 있는데, AWS콘솔에서 Lambda로 바로 업로드를 하려면 .zip파일이 50MB보다 작아야 하고, S3에 .zip파일을 올린 뒤 Lambda에서 가져와 사용하려면 압축을 푼 크기가 250MB보다 작아야 합니다. 문제는 Tensorflow나 기타 라이브러리를 모두 설치하면 용량이 무지막지하게 커진다는 점인데요, 이를 해결하기 위해 사용하지 않는 부분을 strip하는 방법이 들어갑니다. 앞서만든 AmazonLinux 기반 컨테이너인 lambdapack를 이용해 패키지들을 설치하고 하나의 압축파일로 만들어줍시다. 아래 내용의 buildPack.sh파일을 index.py 옆에 만들어 주세요. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# buildPack.shdev_install () { yum -y update yum -y upgrade yum install -y \\ wget \\ gcc \\ gcc-c++ \\ python36-devel \\ python36-virtualenv \\ python36-pip \\ findutils \\ zlib-devel \\ zip}pip_rasterio () { cd /home/ rm -rf env python3 -m virtualenv env --python=python3 source env/bin/activate text=\" [global] index-url=http://ftp.daumkakao.com/pypi/simple trusted-host=ftp.daumkakao.com \" echo \"$text\" &gt; $VIRTUAL_ENV/pip.conf echo \"UNDER: pip.conf ===\" cat $VIRTUAL_ENV/pip.conf pip install -U pip wheel pip install --use-wheel \"h5py==2.6.0\" pip install \"pillow==4.0.0\" pip install protobuf html5lib bleach --no-deps pip install --use-wheel tensorflow --no-deps deactivate}gather_pack () { # packing cd /home/ source env/bin/activate rm -rf lambdapack mkdir lambdapack cd lambdapack cp -R /home/env/lib/python3.6/site-packages/* . cp -R /home/env/lib64/python3.6/site-packages/* . cp /outputs/squeezenet.py /home/lambdapack/squeezenet.py cp /outputs/index.py /home/lambdapack/index.py echo \"original size $(du -sh /home/lambdapack | cut -f1)\" # cleaning libs rm -rf external find . -type d -name \"tests\" -exec rm -rf {} + # cleaning find -name \"*.so\" | xargs strip find -name \"*.so.*\" | xargs strip rm -r pip rm -r pip-* rm -r wheel rm -r wheel-* rm easy_install.py find . -name \\*.pyc -delete echo \"stripped size $(du -sh /home/lambdapack | cut -f1)\" # compressing zip -FS -r1 /outputs/pack.zip * &gt; /dev/null echo \"compressed size $(du -sh /outputs/pack.zip | cut -f1)\"}main () { dev_install pip_rasterio gather_pack}main dev_install 함수에서는 운영체제에 Python3/pip3등을 설치해 주고, pip_rasterio 함수에서는 가상환경에 들어가 tensorflow등 pip로 패키지들을 설치해 주고, gather_pack 함수에서는 가상환경에 설치된 패키지들과 index.py파일을 한 폴더에 모은 뒤 pack.zip파일로 압축해줍니다. 중간에 pip.conf를 바꾸는 부분을 통해 느린 pip global cdn대신 kakao의 pip 미러서버로 좀 더 패키지들을 빠르게 받을 수 있습니다. 이 방법은 여러분의 pip에도 바로 적용할 수 있습니다. 이 sh 파일을 도커 내에서 실행하기 위해서 다음 명령어를 사용해 실행해주세요. 1docker exec -it lambdapack /bin/bash /outputs/buildPack.sh 이 명령어는 lambdapack이라는 컨테이너에서 buildPack.sh파일을 실행하게 됩니다. 실행하고 나면 약 50MB안팎의 pack.zip파일 하나가 생긴것을 볼 수 있습니다. 하지만 앞서 언급한 것처럼, AWS 콘솔에서 ‘ZIP 파일 올리기’로 한번에 올릴수 있는 압축파일의 용량은 50MB로 제한됩니다. 따라서 이 zip 파일을 s3에 올린 뒤 zip파일의 HTTP주소를 넣어줘야 합니다. zip파일 s3에 올리고 AWS Lambda 트리거 만들기이제 AWS 콘솔을 볼 때가 되었습니다. 지금까지 작업한 것은 Lambda에 올릴 패키지/코드를 압축한 파일인데요, 이 부분을 약간 수정해 이제 실제로 AWS Lambda의 이벤트를 통해 실행해 봅시다. S3에 파일을 올린 뒤 파일의 HTTPS주소를 복사해주세요. 여기에서는 https://s3.ap-northeast-2.amazonaws.com/keras-blog/pack.zip가 주소가 됩니다. 이제 진짜로 AWS Lambda 함수를 만들어봅시다. Lambda 콘솔 함수만들기에 들어가 “새로 작성”을 선택 후 아래와 같이 내용을 채운 뒤 함수 생성을 눌러주세요. 함수가 생성되고 나면 화면 아래쪽 ‘함수 코드’에서 다음과 같이 AWS s3에서 업로드를 선택하고 런타임을 Python3.6으로 잡은 뒤 핸들러를 index.handler로 바꾸고 S3링크를 넣어준 뒤 ‘저장’을 눌러주세요. 그 뒤, ‘기본 설정’에서 메모리를 1500MB 이상으로, 그리고 제한시간은 30초 이상으로 잡아주세요. 저는 테스트를 위해 3000MB/5분으로 잡아주었습니다. 이제 s3에서 파일이 추가될때 자동으로 실행되도록 만들어 주기 위해 다음과 같이 ‘구성’에서 s3를 선택해주세요. 이제 화면 아래에 ‘트리거 구성’ 메뉴가 나오면 아래 스크린샷처럼, 파일을 올릴 s3, 그리고 어떤 이벤트(파일 업로드/삭제/복사 등)를 탐지할지 선택하고, 접두사에서 폴더 경로를 폴더이름/으로 써 준 뒤, 필요한 경우 접미사(주로 파일 확장자)를 써 주면 됩니다. 이번에는 keras-blog라는 버킷 내 uploads폴더 내에 어떤 파일이든 생성되기만 하면 모두 람다 함수를 실행시키는 것으로 만들어 본 것입니다. NOTE: 접두사/접미사에 or 조건은 AWS콘솔에서 지원하지 않습니다. 추가버튼을 누르고 난 뒤 저장을 눌러주면 됩니다. 트리거가 성공적으로 저장 되었다면 다음과 같은 화면을 볼 수 있을거에요. Lambda 환경변수 추가하기우리가 앞서 index.py에서 os.environ을 통해 시스템의 환경변수를 가져왔습니다. 이를 정상적으로 동작하게 하기 위해서는 ACCESS_KEY와 SECRET_KEY을 추가해주어야 합니다. 아래 스크린샷처럼 각각 값을 입력하고 저장해주세요. 이 키는 AWS iam을 통해 가져올 수 있습니다. 해당 iam계정은 s3 R/W권한, DynamoDB write 권한이 있어야 합니다. Lambda 내 테스트 돌리기(Optional)AWS Lambda 콘솔에서도 테스트를 돌릴 수 있습니다. 아래와 같이 event 객체를 만들어 전달하면 실제 이벤트처럼 동작합니다. 12345678910{ \"Records\": [ { \"s3\": { \"bucket\": {\"name\": \"keras-blog\"}, \"object\": {\"key\": \"uploads/kitten.png\"} } } ]} 유의: 실제로 keras-blog버킷 내 uploads폴더 내 kitten.png파일이 있어야 테스트가 성공합니다! (인터넷의 아무 사진이나 넣어두세요.) 테스트가 성공하면 다음과 같이 return된 결과가 json으로 보입니다. 마무리: 파일 업로드하고 DB에 쌓이는지 확인하기이제 s3에 가서 파일을 업로드 해 봅시다. keras-blog 버킷 내 uploads폴더에 고양이 사진 몇 개를 올려봅시다. 몇초 기다리면 DynamoDB에 다음과 같이 파일 이름과 Predict된 결과가 쌓이는 것을 볼 수 있습니다. 이제 우리는 파일이 1개가 올라가든 1000개가 올라가든 모두 동일한 속도로 결과를 얻을 수 있습니다.","link":"/2017/12/07/Deploy-Tensorflow-Keras-on-AWS-Lambda/"},{"title":"Direct S3 Upload with Lambda","text":"들어가며이전 글인 AWS Lambda에 Tensorflow/Keras 배포하기에서 Lambda 함수가 실행되는 트리거는 s3버킷에 파일이 생성되는 것이었습니다. 물론 파일을 올릴 수 있는 방법은 여러가지가 있습니다. 아주 단순하게 POST 폼 전송 요청을 받고 boto3등을 이용해 서버에서 s3으로 파일을 전송할 수도 있고, 더 단순하게는 AWS s3 콘솔을 이용해 파일을 올리라고 할 수도 있습니다. 하지만 이런 부분에는 약간의 단점이 함께 있습니다. 첫 번째 방법처럼 파일을 수신해 다시 s3에 올린다면 그 과정에서 서버 한대가 상시로 켜져있어야 하고 전송되는 속도 역시 서버에 의해 제약을 받습니다. 한편 두 번째 방법은 가장 단순하지만 제3자에게서 파일을 받기 위해서 AWS 계정(비록 제한된 권한이 있다 하더라도)을 제공한다는 것 자체가 문제가 됩니다. 따라서 이번 글에서는 사용자의 브라우저에서 바로 s3으로 POST 요청을 전송할 수 있도록 만드는 과정을 다룹니다. 시나리오 사용자는 아주 일반적인 Form 하나를 보게 됩니다. 여기에서 드래그-드롭 혹은 파일 선택을 이용해 일반적으로 파일을 올리게 됩니다. 물론 이 때 올라가는 주소는 AWS S3의 주소가 됩니다. 하지만 이게 바로 이뤄진다면 문제가 생길 소지가 많습니다. 아무나 s3 버킷에 파일을 올린다면 악의적인 파일이 올라올 수도 있고, 기존의 파일을 덮어쓰기하게 될 수도 있기 때문입니다. 따라서 중간에 s3에 post 요청을 할 수 있도록 인증(signing)해주는 서버가 필요합니다. 다만 이 때 서버는 요청별로 응답을 해주면 되기 때문에 AWS Lambda를 이용해 제공할 수 있습니다. 따라서 다음과 같은 형태로 진행이 됩니다. S3에 POST 요청을 하기 전 Signing 서버에 업로드하는 파일 정보와 위치등을 보낸 뒤, Lambda에서 해당 POST 요청에 대한 인증 정보가 들어간 header를 반환하면 그 헤더 정보를 담아 실제 S3에 POST 요청을 하는 방식입니다. 만약 Signing하는 과정 없이 업로드가 이뤄진다면 s3 버킷을 누구나 쓸 수 있는 Public Bucket으로 만들어야 하는 위험성이 있습니다. 하지만 이와 같이 제한적 권한을 가진 iam 계정을 생성하고 Signing하는 과정을 거친다면 조금 더 안전하게 사용할 수 있습니다. Note: 이번 글에서는 API Gateway + Lambda 조합으로 Signing서버를 구성하기 때문에 만약 추가적인 인증 과정을 붙인다면 API Gateway단에서 이뤄지는 것이 좋습니다. 버킷 만들기 &amp; 권한 설정 새로운 버킷은 https://s3.console.aws.amazon.com/s3/home에서 추가할 수 있습니다. 새로운 버킷을 하나 만들어주세요. 이번 글에서는 s3-signature-dev-py3라는 이름으로 만들어 진행해 보았습니다. 버킷에 GET/POST 요청을 하기 위해 CORS 설정을 해줘야 합니다.(localhost:8000와 같은 제 3의 URL에서 s3 버킷의 주소로 POST 요청을 날리기 위해서는 CORS 설정이 필수입니다.) 아래 스크린샷과 같이 CORS 설정을 진행해 주세요. 1234567891011121314&lt;CORSConfiguration&gt; &lt;CORSRule&gt; &lt;AllowedOrigin&gt;*&lt;/AllowedOrigin&gt; &lt;AllowedMethod&gt;GET&lt;/AllowedMethod&gt; &lt;MaxAgeSeconds&gt;3000&lt;/MaxAgeSeconds&gt; &lt;AllowedHeader&gt;Authorization&lt;/AllowedHeader&gt; &lt;/CORSRule&gt; &lt;CORSRule&gt; &lt;AllowedOrigin&gt;*&lt;/AllowedOrigin&gt; &lt;AllowedMethod&gt;POST&lt;/AllowedMethod&gt; &lt;MaxAgeSeconds&gt;3000&lt;/MaxAgeSeconds&gt; &lt;AllowedHeader&gt;Authorization&lt;/AllowedHeader&gt; &lt;/CORSRule&gt;&lt;/CORSConfiguration&gt; 이처럼 구성해주면 모든 도메인(*)에서 요청한 GET/POST 요청을 정상적인 크로스-도메인 요청으로 받아들입니다. Note: 만약 여러분이 여러분의 프론트 서비스에서만 이 요청을 허용하려면 AllowedOrigin부분을 여러분이 사용하는 프론트 서비스의 도메인으로 변경해주세요. 이제 s3을 사용할 준비는 마쳤습니다. 버킷 액세스 iam 계정 만들기 새로운 iam 계정은 https://console.aws.amazon.com/iam/home?region=ap-northeast-2#/users$new?step=details에서 만들 수 있습니다. 다음으로는 앞서 만들어준 버킷에 액세스를 할 수 있는 iam 계정을 만들어야 합니다. 이번에 사용할 유저 이름도 s3-signature-dev-py3로 만들어 줍시다. 아래 스크린샷처럼 Programmatic access를 위한 사용자를 만들어 줍시다. 우리는 버킷내 uploads폴더에 파일을 ‘업로드만 가능’한, PutObject와 PutObjectAcl이라는 아주 제한적인 권한을 가진 계정을 만들어 줄 것이기 때문에 다음과 같이 Create Policy를 눌러 json 기반으로 계정 정책을 새로 생성해 줍시다. 새 창이 뜨면 아래와 같이 arn:aws:s3:::s3-signature-dev-py3/uploads/* 리소스에 대해 PutObject와 PutObjectAcl에 대해 Allow를 해 주는 json을 입력하고 저장해줍시다. 12345678910111213141516{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"s3UploadsGrant\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:PutObjectAcl\" ], \"Resource\": [ \"arn:aws:s3:::s3-signature-dev-py3/uploads/*\" ] } ]} 이제 policy의 name을 입력하고 저장해줍시다. 저장해주고 창을 끈 뒤 이전 페이지로 돌아와 Refresh를 누르면 다음과 같이 앞서 만들어준 Policy가 나오는 것을 볼 수 있습니다. 체크박스에 체크를 누른 뒤 다음을 눌러주세요. 이제 마지막 확인을 눌러주세요. 확인을 누르면 다음과 같이 Access key ID와 Secret access key가 나옵니다. 이 키는 지금만 볼 수 있으니 csv로 받아두거나 따로 기록해 두세요. 그리고 글 아래부분에서 이 키를 사용하게 됩니다. Signing Lambda 함수 만들기이제 POST 요청을 받아 인증을 해줄 AWS Lambda함수를 만들어 줍시다. 아래 코드를 받아 AWS Lambda 새 함수를 만들어주세요. (역시 s3-signature-dev-py3라는 이름으로 만들었습니다.) Github Gist: index.py 이번 함수는 python3의 내장함수만을 이용하기 때문에 따로 zip으로 만들 필요없이 AWS 콘솔 상에서 인라인 코드 편집으로 함수를 생성하는 것이 가능합니다. 아래 스크린샷처럼 lambda_function.py 파일을 위의 gist 코드로 덮어씌워주세요. 그리고 Handler부분을 lambda_function.index로 바꿔 index함수를 실행하도록 만들어 주세요. 그리고 저장을 눌러야 입력한 코드가 저장됩니다. 코드를 조금 뜯어보면 아래와 같이 ACCESS_KEY와 SECRET_KEY를 저장하는 부분이 있습니다. 12ACCESS_KEY = os.environ.get('ACCESS_KEY')SECRET_KEY = os.environ.get('SECRET_KEY') AWS Lambda에서 함수를 실행할 때 아래 환경변수를 가져와 s3 버킷에 액세스하기 때문에 위 두개 값을 아래 스크린샷처럼 채워줍시다. 입력을 마치고 저장을 눌러주면 환경변수가 저장됩니다. Note: 각 키의 값은 앞서 iam 계정 생성시 만든 값을 넣어주세요! API Gateway 연결하기 API Gateway는 https://ap-northeast-2.console.aws.amazon.com/apigateway/home?region=ap-northeast-2#/apis/create에서 만들 수 있습니다. 이렇게 만들어 준 AWS Lambda 함수는 각각은 아직 외부에서 결과값을 받아올 수 있는 형태가 아닙니다. 람다 함수를 트리거해주고 결과값을 받아오기 위해서는 AWS API Gateway를 통해 웹 URL로 오는 요청에 따라서 람다 함수가 실행되도록 구성해야 합니다. 또한, CORS역시 활성화 해줘야 합니다. API Gateway 만들고 Lambda와 연결하기 Resources에서는 Api URL의 하위 URL와 root URL에 대해 각각 메소드들을 정의해 사용할 수 있습니다. 우리는 요청을 받을 때 POST 방식으로 요청을 받아 처리해줄 것이랍니다. 여기서 새 메소드 중 POST를 선택해 줍시다. 메소드에 Lambda 함수를 연결해 주기 위해 다음과 같이 Lambda Function을 선택하고 Proxy는 체크 해제한 뒤, Region은 ap-northeast-2(서울)리전을 선택하고, 아까 만들어준 함수 이름을 입력한 뒤 Save를 눌러줍시다. Tip: Lambda Proxy를 활성화 시킬 경우 HTTP 요청이 그대로 들어오는 대신, AWS에서 제공하는 event 객체가 대신 Lambda함수로 넘어가게 됩니다. 우리는 HTTP 요청을 받아 Signing해주는 과정에서 Header와 Body를 유지해야하기 때문에 Proxy를 사용하지 않습니다. Save를 누르면 다음과 같이 API Gateway에 Lambda함수를 실행할 권한을 연결할지 묻는 창이 뜹니다. 가볍게 OK를 눌러줍시다. 연결이 완료되면 API Gateway가 아래 사진처럼 Lambda 함수와 연결 된 것을 볼 수 있습니다. CORS 활성화하기조금만 더 설정을 해주면 API Gateway를 배포할 수 있게 됩니다. 지금 해줘야 하는 작업이 바로 CORS 설정인데요, 우리가 나중에 만들 프론트 페이지의 URL와 s3의 URL이 다르기 때문에 브라우저에서는 보안의 이유로 origin이 다른 리소스들에 대해 접근을 제한합니다. 따라서 CORS를 활성화 해 타 URL(프론트 URL)에서도 요청을 할 수 있도록 설정해줘야 합니다. Actions에서 Enable CORS를 눌러주세요. 다음과 같이 Access-Control-Allow-Headers의 값을 '*'로 설정한 뒤 Enable CORS 버튼을 눌러 저장해주세요. 다시한번 Confirm을 눌러주시면… CORS가 활성화되고 Options 메소드가 새로 생기게 됩니다. 이제 API Gateway를 ‘배포’해야 실제로 사용할 수 있습니다. API Gateway 배포하기API Gateway의 설정을 모두 마치고나서는 배포를 진행해야 합니다. 아래와 같이 Actions에서 Deploy API를 눌러주세요. API Gateway는 Deployment Stage를 필요로 합니다. Stage name을 live로 설정하고 Deploy를 눌러줍시다. Tip: Deployment Stage는 API Gateway의 URL 뒤 /stagename의 형식으로 추가 URL을 지정해줍니다. 이를 통해 API를 개발 버전과 실 서비스 버전을 분리해 제공할 수 있습니다. 배포가 완료되면 아래와 같이 API Gateway를 사용할 수 있는 URL을 받을 수 있습니다. 이번에는 https://9n2qae2nak.execute-api.ap-northeast-2.amazonaws.com/live가 Signing Lambda 함수를 실행할 수 있는 API Gateway URL이 됩니다. 파일 업로드 프론트 만들기이제 파일을 업로드할 form이 있는 Static 웹 사이트를 만들어봅시다. 이번 글에서는 이미 만들어진 파일 업로더인 VanillaJS용 Fine Uploader를 이용해 최소한의 업로드만 구현합니다. React용 React Fine Uploader와 Vue용 Vue Fine Uploader도 있습니다. https://github.com/Beomi/s3-direct-uploader-demo 깃헙 레포를 clone받아 app.js를 열어 아래 목록을 수정해주세요. request/endpoint: 여러분이 사용할 s3 버킷 이름 + .s3.amazonaws.com request/accessKey: 앞서 만든 iam 계정의 Access Key objectProperties/region: s3 버킷의 리전 objectProperties/key/prefixPath: s3 버킷 내 올릴 폴더 이름(putObject 권한을 부여한 폴더) signature/endpoint: 앞서 만든 AWS Lambda의 API Gateway URL 123456789101112131415161718192021222324252627var uploader = new qq.s3.FineUploader({ debug: false, // defaults to false element: document.getElementById('fine-uploader'), request: { // S3 Bucket URL endpoint: 'https://s3-signature-dev-py3.s3.amazonaws.com', // iam ACCESS KEY accessKey: 'AKIAIHUAMKBO27EZQ6RA' }, objectProperties: { region: 'ap-northeast-2', key(fileId) { var prefixPath = 'uploads' var filename = this.getName(fileId) return prefixPath + '/' + filename } }, signature: { // version version: 4, // AWS API Gate URL endpoint: 'https://9n2qae2nak.execute-api.ap-northeast-2.amazonaws.com/live' }, retry: { enableAuto: true // defaults to false }}); 그리고나서 index.html 파일을 열어보시면 아래 사진과 같은 업로더가 나오게 됩니다. DEMO: https://beomi.github.io/s3-direct-uploader-demo/ 맺으며이제 여러분은 Serverless하게 파일을 s3에 업로드 할 수 있게 됩니다. 권한 관리와 같은 부분은 API Gateway에 접근 가능한 부분에 제약을 걸어 업로드에 제한을 걸어 줄 수도 있습니다. ec2등을 사용하지 않고도 간단한 signing만 갖춰 s3에 파일을 안전하게 업로드 하는 방식으로 전체 프로세스를 조금씩 Serverless한 구조로 바꾸는 예시였습니다. Reference Browser-Based Upload using HTTP POST (Using AWS Signature Version 4) How to access HTTP headers using AWS API Gateway and Lambda","link":"/2017/12/15/Direct-upload-to-S3-with-Lambda/"},{"title":"macOS 터미널에서 사용자이름 숨기기","text":"Before우리가 macOS를 사용하고 터미널을 켜면 다음과 같이 username@computername와 같은 형식으로 나타납니다. 이처럼 계정과 컴퓨터 이름이 나오는 경우 SSH와 같은 원격 접속시에는 어떤 계정으로 어떤 기기에 접속했는지 알 수 있기 때문에 편리하지만 로컬 개발 컴퓨터같은 경우에는 위와같은 정보가 터미널 앞에 붙어있으면 명령어가 길어질 경우 한 줄 내에 나오지 않을 수 있습니다. macOS의 터미널 기본 너비는 80자(영문)입니다. After위의 이유로 아래와 같이 ~ 표시만 나오게 하면 좀 더 사용에 편리합니다. Solution만약 여러분이 “beomi”라는 계정명을 사용중이라면 다음과 같이 .zshrc 파일에 코드를 추가해주세요. 12# DEFAULT_USERDEFAULT_USER=\"beomi\" 만약 여러분이 계정명이 무엇인지 알지 못한다면 다음과 같이 .zshrc 파일에 코드를 추가해주세요. 12# DEFAULT_USERDEFAULT_USER=\"$(whoami)\" 여러분이 터미널을 종료하고 다시 켜면 After의 스크린샷처럼 ~만 터미널에 나오게 됩니다.","link":"/2018/01/30/Hide-username-on-MAC-terminal/"},{"title":"2017년 회고 & 블로그 연말정산","text":"올해, 2017년.올해는 정말 내게 큰 변화가 있던 한해였다. 특히 개발/프로그래밍쪽으로 다양한 기회를 여러가지 받게 되었고, 정말 많은 성장을 했다고 느낀다. 올해를 월별로 정리해보자면… 1월, DjangoGirls Seoul 1월, 장고걸스 서울에 운영진으로 함께 시작하고 ‘나만의 웹크롤러 만들기’ 시리즈 연재를 시작했다. 그리고 작년 12월에 시작한 TDD 스터디도 함께 진행했다. 2월, 스터디 &amp; 첫 외주2월, 장고를 이용해 교대 학생 대상으로 새로운 서비스를 오픈했다. 토이 프로젝트로 실제 배포까지 이뤄본 케이스. 이때 Vue를 처음 듣고 이용해보았다. 그리고 데이터분석 관련한 스터디도 진행해 데이터분석 분야에도 관심을 갖기 시작했다. 장고걸스 서울에서 장고 스터디를 운영하는 등 스터디를 다양하게 진행했다. 그리고 장고를 사용한 웹 사이트 개발을 하는 외주를 하나 받기도 했다. 처음으로 돈을 받고 개발을 하는 것이라 굉장한 부담이 되었지만 지금 생각해보면, 이때 약간의 자신감을 얻었다. 3월, 개강!3월, 블로그 테마를 현재 테마로 바꾸고 웹 크롤러 시리즈를 조금씩 더 쌓아갔다. 그리고 React 기초를 조금씩 배워보려고 시작했다가, JS에 대해 이해가 없어(ES6와 Babel이 뭔지도 모르는 상황) 전혀 진도를 내지 못했다. 게다가 개강이 겹쳐 일정을 내기가 상당히 힘들어졌었다. TDD 스터디도 조금씩 사람이 빠져 3월에는 더이상 진행하지 못하고 마무리 되었다. 4월, 9XD4월, 9XD 8회 모임을 DevSisters에서 가지게 되었다. 사람들과 이야기를 나누며 느꼈던 것이 작년 말과 이때 내가 공부한 것과 만든 것의 큰 차이가 없다는 것이어서 약간 아쉬웠지만..(1Q에 대체 뭘했나..하던 생각) 그래도 글을 쓰고있는 지금 돌아보니 한 것이 조금은 있는것 같아보인다. :) 9XD 모임에서 발표하는 사람들을 보고 ‘아, 나도 저렇게 발표할 수 있을만큼 실력이 되면 좋겠다’ 라고 생각도 해보고 GraphQL에 대해 알게 되는 계기가 되기도 했다. 그리고 4월말 장고걸스 서울에서 격월 Meetup을 토즈에서 열었는데 생각보다 반응이 좋았다! 내가 사회를 맡았는데 진행에 사람들의 집중을 모으는 것이 어려워 약간 아쉬웠지만 전체적으로 굉장히 즐거운 모임이었다. 5월, ETH5월, 학교 내 오버워치 대회를 마무리 했다. 우리과 친구들이 (나빼고) 다 잘해서 우승! 그리고 베를린 필하모닉 유로파 콘서트를 보고왔다.(메가박스에서 실시간 스트리밍으로 보여준다. 짱짱.) 그리고 3학년 1학기 실습도 다녀왔다. 학교 생활에 바빴던 5월. 이 기간에 비트코인과 이더리움이 급상승하고 주변에서 아는 사람들은 비트코인과 이더리움을 이야기하기 시작했다.(이때 샀어야 했는데) 이때 가격과 지금 가격이 10배 차이(…) 그리고 5월에 있었던 4회 파이썬 격월 세미나에서 “굥대생의 Hello World!”라는 제목으로 발표를 진행했다. 이 발표는 ‘내가 어떻게 개발을 시작하게 되었나’ 였는데, 지금 글을 쓰는 12월에 돌아보면 이 사이에 정말 많은 성장의 기회를 가졌다는 체감이 된다. 7개월이 아니라 마치 1년 7개월 전의 일 같은 기분. 6월, 터닝포인트6월, 아는 분의 추천으로 키움증권에서 Python강의를 몇차례 진행했다. 종강 직후 학교 동아리 친구들과 3일정도 일본에 여행을 다녀왔다. 이때 에어팟을 사려고 재고를 미친듯이 찾아다니다 귀국하는 날 아침에 애플스토어에 재고가 들어온 것을 보고 체크아웃하기 직전에 뛰어가 구매했다.(이 에어팟은 지금도 잘 쓰고 다닌다.) 그리고 6월에는 장고걸스 행사 중 가장 큰 행사인 워크숍을 열었다. 이때 MS에서 장소 제공과 Azure Credit을 제공해줘 이 가이드로 수정 가이드를 만들었는데, 블로그에 올려뒀던 가이드에 약간 문제가 있어서 정신없이 뛰어다녔던 기억이 난다. 그리고 6월에는 정말 큰 이벤트가 있었다. 바로 우아한형제들에서 우아한테크캠프 인턴을 모집했던 것. 이때 지원하면서 내가 얻고싶었던 것은 바로 ‘내가 개발을 해도 될까?’라는 질문에 대한 답이었다. 사실 이 질문은 아직도 내가 품고있는 질문이다. ‘나는 과연 개발자로 살아갈 수 있는 사람일까?’ 그래도 굉장히 멋진 기회(코드스쿼드에서 교육을 진행했다!)라고 생각해 떨어질 때 떨어지더라도 지원을 하자! 라는 생각으로 지원서를 넣었고 운좋게 서류와 코딩테스트를 통과했지만 마지막 면접에서 제대로 대답을 하지 못했는지 최종적으로는 불합격 메일을 받았다. 사실 이때 나는 ‘아, 그냥 이길이 내 길이 아닌 것을 빨리 알게 되어서 다행이다.’ 라는 생각을 했다. 말 그대로 취미로 개발을 하더라도 직업으로 삼지는 못하겠구나, 라고 생각하던 중…. 전화가 왔다. 자리가 났는데 혹시 아직 올 생각이 있냐는 전화. 바로 네 라고 대답하고 아, 그래도 문 닫고 들어왔구나.. 하고 생각했다. 그리고 이때 참여하기로 결정한 것은 정말 최고의 선택이었다. 7월, 우아한테크캠프 7월, 우아한테크캠프에서 웹 프론트 트랙에 들어가 JS에 대해 정말 친숙하게 되었다. 그전까지는 잘 쓰지도 못하면서 투덜댔다면 지금은 잘 쓰지는 못하더라도 조금은 알고 투털대는 수준이 되었다. 코드스쿼드 윤지수마스터님이 메인으로 진행해주시고 김정마스터님과 정호영마스터님이 진행하신 트랙도 정말 재미있게 들었다. 그리고 알고리즘의 ㅇ 도 모르던 내가 4차례 정도 진행한 김범준CTO님의 수업을 통해 nlogn이 뭔지, 그리고 문제 해결 능력에 대해 다시한번 생각해보게 되었다. 그뿐만 아니라 7월에는 정말 다양한 제의가 들어왔다. 인프런에서 크롤링 온라인 강의 제안과 패스트캠퍼스에서 크롤링 강의제안을 받았다. 어디서 보셨나 물으니 블로그 글을 보고 연락을 주셨다고 했다. 게다가 출판사에서 집필 제의까지 들어왔다. 정말 믿기지 않을 정도로 좋은 제의를 해주셔서 정말 감사한 마음으로 받았다. (지금 생각해도 놀라운 기회들이다.) 여러분 블로깅하세요, 꼭 하세요!! 8월, 파이콘 8월, 우아한테크캠프에서 팀별로 프로젝트를 진행했다. 웹 프론트에서 CSS가..(한숨) 정말로 까다롭다는 것을 다시한번 느끼게 되었다. 그리고 혼자 개발하는 것이 아니라 여럿이서 팀으로 개발을 한다는 것에서 협업에 대해 많은 경험을 하고 여러가지 생각을 하는 기회가 되었다. 그리고 파이콘 튜토리얼을 신청한 것을 진행하기 위한 사전 준비모임을 갔다가, 발표할 기회까지 얻게 되었다.(원래는 파이콘 발표 신청을 했다가 떨어졌었다.) 준비할 시간이 몇주 없었지만 작년 파이콘을 보며 올해 이루고 싶었던 목표 중 하나였던 파이콘 발표하기를 할 수 있게 된 것이 굉장히 기뻤다. 파이콘 전날 리허설을 하니 딱 45분이 맞았는데, 행사 당일에도 시간이 잘 맞아 다행이었다. 사실 행사장이 2층이라 사람들이 많이 안올까 걱정했는데 사람들이 의자를 다 채우는 것도 모자라 바닥에 앉고 뒤에 가득가득 서있는 모습을 보고 ‘세상에…’ 라는 생각을 했다. 그리고 이 발표는 또다른 기회들을 불러왔다. 여러분 커뮤니티하세요, 꼭 하세요!! 9월, 휴학!!9월, 휴학하기로 마음먹었다. 사실 이 결정을 하기까지 굉장히 많은 고민을 거쳤다. 교대에서는 휴학을 웬만하면 하지 않고 4년을 쭉 다니다 임용고시를 치른 뒤 졸업하는 것이 일반적이기 때문. 하지만 나에게 주어진 수많은 기회들을 손끝사이로 흘려보내고 싶지 않았다. 욕심이라고 말할 수 있지만, 이 멋진 기회들을 모두 잡고 싶었다. 그래서 휴학을 하기로 결정했다. 휴학하면 여유로울 줄 알았더니… 사실 휴학하고 나서 잠시 여유로웠지만 여유로울 새도 없이 금새 바빠졌다. 크롤링 강의를 진행할 때 만약 이때 학교를 다니고 있었다면 패캠에서 첫기 강의를 망칠 뻔 했다. 강의자료가 모두 준비된 상태가 아니라 매주 2회차씩 강의자료를 준비했더니.. 매주 두번 마감에 쫓기는 기분으로 4주를 보냈던 것 같다. 그러다가 파이콘에서 받은 제의가 하나가 나왔다. 넥슨에서 아르바이트 형식으로 웹 개발 하기. 10월, 넥슨 넥슨 인텔리전스랩스 어뷰징탐지TF 팀장님이 파이콘에서 10월~12월에 짧게라도 웹 개발(주로 프론트)을 해보지 않겠냐는 제의를 해 주셨다. 내가 과연 회사에서 개발을 할 수 있는 실력이 될까-하는 약간의 두려움이 있었지만 제의를 받기로 결정했다. 우아한형제들에서 인턴을 진행했지만 실무에 대한 경험은 아니었기 때문에 ‘나는 과연 개발자로 살아갈 수 있는 사람일까?’에 대한 답을 명확히 얻지는 못했기 때문이다. 우아한테크캠프에서 배운 프론트와 기존에 하던 백엔드 지식을 기반으로 개발을 나름 열심히 진행했다. 작지만 조금씩 서비스를 만들어가며 기존에 사용하지 않던 스택(PySpark, Hadoop,EMR와 같은 분산처리 등)도 알게 되고 딥러닝 등을 이용한 데이터 분석을 하는 모습을 바로 옆에서 지켜보니 자연스럽에 데이터분석 분야에도 관심을 갖게 되었다. 그리고 10월에는 메이커페어 이벤트가 있었다. GEEKHUB이라는 이름을 가진 모임에서 ‘공대탈출’이라는 이름으로 부스를 열고 운영도 해보며 친구들과 함께 무언가를 진행했다는 것에 뿌듯함을 느끼기도 했다. 11월, 다시:Django11월 초에는 장고걸스 서울에서 세미나를 열었다. AskDjango의 진석님이 Azure+크롤링을, Hannal님이 장고Admin을, 허신영님이 Kaggle에 대한 강의를 해주셔서 굉장히 높은 퀄리티의 세미나가 되었다. 그리고 처음으로 100명을 염두에 둔 행사이기도 했다. (보통 장고걸스 서울에서 연 행사는 워크숍을 제외하고는 30명 내 규모로 이뤄졌다.) 이때 장고걸스 서울이라는 커뮤니티가 굉장히 많이 성장했다는 생각이 들었다. 12월, 개발자로의 삶을 고민하다12월 첫날부터 Vuetiful Korea 세미나가 있었다. Vue를 많이 사용하지는 않지만 그래도 관심은 지속적으로 가지고 있기 때문에 이번에도 참석. 그리고 바로 다음주에 파이썬 연말 세미나가 있었다. ‘헛된꿈’이라는 간단한 투표 웹 사이트가 있었는데 재미로 ‘좋아요’수를 눌리는 코드를 짜보기도 했다.(재미있다) 그러다 굉장히 멋진 제의가 들어왔다. 넥슨에 계속 다니지 않겠냐는 제의로, 내년부터 정직원으로 다니지 않겠냐는 것. 사실 이 제의를 받고 ‘나는 과연 개발자로 살아갈 수 있는 사람일까?’에 대한 답을 조금은 찾은 듯한 느낌이 들었다. 어쩌면, 나는 개발자로 살아가도 괜찮지 않을까 하는 그런 작은 자신감. 코딩 테스트와 면접을 거쳐 내년에도 지금 있는 팀에 계속 다니기로 이야기가 되었다. 정말 좋은 기회들이 다가왔다. 나는 운이 좋다. 정말로 좋은 편이다. 지금까지 한 선택, 멀게는 교대 진학을 선택한 것부터 올해의 중요한 선택들에서 굉장히 좋은 선택지들을 골라왔다고 생각한다. 물론 아직 모르는 것이 한참 많은 늅늅이지만, 그래도 이제 어디 가서 ‘개발해요’라고 말은 하고 다닐 수 있을 것 같다. 올 한해는 정말 1년이 아니라 3년을 보낸 것 같은 기분이다. 수많은 멋진 기회들이 주어지고 그 기회들을 통해 성장했다. 작년보다 성장한 올해, 올 한해는 지금까지 내 삶에서 최고점을 주고싶다. 내년에도 올해와 같이 즐기며 성장할 수 있길 꿈꾼다. 어제보다 나은 오늘의 내가 되기를. 올해의 후기를 마무리짓고 이제 블로그 이야기를 조금 더 해보려 한다. 블로그 이야기 어떻게 블로그를 시작하게 되었나?기술과 관련된 블로그를 시작한 것은 wordpress.com에서 만든 블로그였다. 2014년 7월부터 사용했고, 이때가 Pogoplug에 ArchLinux/Debian을 설치해가며 커스텀 NAS를 만들고 사용했던 시기였다. 그래서 Nginx가 무엇이고, 웹서버가 무엇이며 FTP가 뭔지, 그리고 웹 상에서 동작하는게 무엇인지 보고 php라는것도 설치해 사용하고 뭔지 모르겠지만 Wordpress도 받아 설치해보고 MySql와 phpMyAdmin등도 사용해보았다. 이때도 “블로그를 쓰면 좋다더라..”라는 막연히 ‘카더라’ 식의 블로그 찬양설을 듣다 그냥 하나씩 해본 것을 정리해보는 식으로 블로그를 작성했다. 윈도에서 RDP를 어떻게 쓰는게 좋나, iptime NAS에 커스텀 리눅스를 어떻게 까나 등등… 이런 개발적인데 비개발적인, 마치 “코딩이랑 무관합니다만,”에 올라올 것 같은 글들을 하나씩 정리해갔다. 왜 Github Pages에 블로그를 옮겼나?그러다가 2016년 6월 즈음 구글 검색을 하다가 보게 된 것이 바로 아래 사진의 Syntax Highlighter였다. 블로그에 코드를 적을 때 단순하게 흰색에 고정폭 글씨만 쓰는 것이 아니라 더 다양하고 보기 좋은(개발할 맛이 나는) 코드 하이라이팅을 적용하는 것을 보았던 것! 이걸 보고 바로 뽐뿌가 와버려 내 블로그에 적용하려 했지만… CSS 수정은 유료 플랜에서만 사용할 수 있었다. 그래서 어떤 것을 사용해야하나, 티스토리를 사용해야하나 등 고민을 했다. 그러다가 Jekyll + Github Pages의 조합으로 블로그를 만들 수 있다는 사실을 보고, 마크다운 문법만 조금 공부하면 되겠다는 생각으로 jekyllthemes.org라는 Jekyll 테마 모음 사이트에서 적당히 예뻐보이는 사이트 하나를 골라 zip파일을 받은 뒤 기초적인 사이트 정보 수정만 하고 beomi.github.io 레포에 커밋을 하고 올렸더니 블로그가 완성이 되었다! 이때 내가 뭔가를 많이 하지 않았는데도(심지어 Jekyll을 설치도 하지 않았음) 블로그를 수정하고 올릴 수 있어서 Jekyll에 거부감 없이 시작할 수 있었다. 만세! 참고로 예전에 사용한 테마는 hagura라는 테마. 이때 단순히 글 목록만 보이는 테마였지만 그래도 꽤 예뻐 보여서 글쓰는 맛을 즐겼다. 왜 테마를 바꿨나?지금 사용하고 있는 테마인 trophy라는 테마를 선택하게 된 것은 구글링을 하다 이 테마가 ‘카테고리’를 지원한다는 사실에 선택을 한 것이 컸다. ‘나만의 웹 크롤러 만들기’ 시리즈를 연재하다가 연재본이 하나로 엮여있는 공간을 마련하기 귀찮아 그냥 테마가 카테고리를 지원하면 좋겠다는 생각에 이것을 선택하게 되었고, 이 테마는 지금도 만족도가 매우 높은 상태다. 물론 테마 색깔이나 레이아웃 등등 일부 CSS/SCSS파일을 수정해 사용하고 있고, 이 테마를 쓰면서 가장 큰 문제는 글 쓰기 전에 글의 메인 이미지인 고해상도이미지를 만드는 것이다. (지금은 보통 1920x1080px로 만들고, 100kb내외로 만든다.) 사실 이 작업이 생각보다 귀찮아서 이 블로그를 보는 분들이 무작정 예쁘다고 이 테마를 선택하지는 않기를 바라는 사소한 마음… 만들다 보면 ‘아 내가 메인 이미지를 만드는 센스가 없구나’ 라는걸 깨닫게 됩니다. 올해 블로그 성과는 어땠나?사실 깃헙에 블로그를 만들고 시작한 것이 2016년 5월이고 본격적으로 방문자 유입이 된 것은 올해라고 볼 수 있다.(구글이 그렇다고 한다.) 우선 올해 사용자부터 보면.. 약 6만명이 이용한 것으로 보인다. Adblock등을 이용해 GA를 차단한 경우는 수집하지 못하기 때문에 GA가 차단된 경우는 제외된 수치니 이것보다는 약간 더 많지 않을까 싶다. 그리고 올해 페이지 뷰는 약 17만 8천뷰정도가 나온 것으로 보인다.(사용자 수가 많을 수록 당연히 페이지 뷰수도 높다.) 방문자 유입은 주로 Google 검색을 통해 들어오는 것으로 보인다. 혹은 어딘가에서 링크로 들어오거나, 페이스북에 글을 올린 날은 페이스북에서 유입이 급격히 늘어나기도 한다. 소스별 이탈율과 평균 세션 시간을 살펴보면 구글 검색등으로 들어온 경우가 가장 긴 세션시간(글 읽는 시간)을 유지하고 있다는 것을 볼 수 있다. 소셜공유, 즉 페이스북을 통해 들어온 경우에는 1분내외의 짧은 시간에 글을 훑어보고 바로 나가버리는 형태의 사용 패턴이 나타나는 것을 볼 수 있다. 전체적으로 고정 방문자수/페이지 뷰수에 큰 영향을 미치는 것은 검색엔진을 통해서 유입된 것임을 확인할 수 있다. 그렇다면 구글 검색결과에는 얼마나 많이 떴을까?구글에는 Search Console을 통해 웹 사이트에 구글 검색을 통해 얼마나 유입이 이뤄졌는지 볼 수 있다. 다만 1년치를 보지는 못하고 최근 90일만 조회 가능하기 때문에 최근 90일을 조회해보면 검색 화면에 노출된 수는 약 20만회, 그리고 클릭으로 이어진 경우는 약 2만9천회임을 볼 수 있다. 당연히 노출이 많이 될수록 클릭 수도 올라간다. 그렇다면 사람들이 어떤 키워드로 검색해 들어왔을까? 역시 크롤링에 관련한 단어가 최상위권을 모두 차지하고 있다. 평균 게재 순위가 높을수록 상위에 노출되고 5위 안으로 들어갈 경우 해당 검색 결과에서 유의미한 유입이 이뤄지는 것을 알 수 있다. 맺으며..2017년은 본격적으로 블로그를 페이스북에 올리는 등의 방법으로 홍보를 시작했습니다. 사실 블로그를 작성하며 가장 많이 드는 고민이 “세상에 이미 이 자료들이 있고, 검색하면 나오는데 굳이 내가 작성할 필요가 있을까?” 라는 고민입니다. 하지만 저는 블로그를 단순한 코드 조각이 아니라 내가 아는 지식들을 꿰어진 구슬처럼 유의미한 가이드로 사용할 수 있도록 재가공해 제공한다는 측면에서 의미가 있다고 생각하고 지속적으로 글을 작성합니다. 지식이 많은 것도 중요하지만, 누군가에게 내 글이 답답한 부분을 뚫어주는 글이 될 수 있기를 바라며 글을 씁니다. 블로그 글에서 이번 가이드는 이라고 표현하는 이유가 단순히 ‘글’ 이 아니라 ‘가이드’의 역할을 할 수 있기를 바라며 작성하기 때문입니다. 이 글을 읽으신 분들도 블로그를 시작하시고, 커뮤니티를 시작하시면 좋겠습니다. 앞으로도 커뮤니티에서 많은 분들을 뵙고 더 성장할 수 있으면 좋겠습니다. 긴 글 읽어주셔서 감사합니다.","link":"/2017/12/30/Blog-1Year-2017/"},{"title":"ubuntu16에 pyldap 설치하기","text":"Problempyldap라이브러리를 이용해 AD Proxy/LDAP서버에 연결하기 위해서는 단순히 pip로만 설치하는 것 외에 사전으로 설치해야 하는 항목이 있다. 만약 설치가 되어있지 않으면 아래와 같이 에러가 난다. 1234In file included from Modules/LDAPObject.c:8:0:Modules/errors.h:7:18: fatal error: lber.h: No such file or directorycompilation terminated.error: command 'x86_64-linux-gnu-gcc' failed with exit status 1 Solution123456sudo apt install python3-dev # python3sudo apt install python3-pip # python3 pip3sudo apt install build-essential # for c/cpp buildsudo apt install libsasl2-devsudo apt install libldap2-devsudo apt install libssl-dev apt로 위 라이브러리 설치 후 아래와 같이 pip3으로 pyldap을 설치하면 된다. 1pip3 install pyldap","link":"/2018/02/07/pyldap-on-ubuntu16/"},{"title":"user mode로 설치한 pip 패키지 PATH에 등록하기","text":"이번 글은 macOS 기준입니다. pip 유저모드?파이썬 패키지 매니저인 pip를 사용할 때 종종 이용하는 옵션이 --user, 즉 사용자 디렉토리에 패키지 패키지를 설치하는 방법을 통해 sudo처럼 권한 상승 없이 패키지들을 설치해 사용할 수 있습니다. 이때 차이가 나는 부분은 저 패키지들이 어떤 디렉토리(폴더)에 설치되는지입니다. 여러분이 brew를 통해 python3을 설치했다면 아래와 같이 파이썬이 /usr/local/bin에 설치되어있는 것을 볼 수 있습니다. 일반적으로 pip3 install ...와 같은 방식을 통해 패키지를 설치한다면 패키지들의 바로가기들이 저 폴더에 자리잡게 됩니다. 그리고 /usr/local/bin은 시스템 환경변수 PATH에 기본적으로 등록되어있기 때문에 추가적인 설정 없이도 명령어들, 예를들어 fabric3을 설치했다면 fab와 같은 명령어들을 사용할 수 있습니다. 뭐가 문제인가요?하지만 --user 옵션을 통해 설치할 경우 패키지가 설치되는 경로는 위 경로 대신 ~/Library/Python/3.6/bin에 설치됩니다. (python3.6기준) 하지만 해당 경로는 시스템 환경변수 PATH에 등록되어있지 않아 아래와 같이 fabric3을 설치했지만 fab명령어를 사용할 수 없습니다. 어떻게 해결하나요?해결 방법은 간단합니다. ~/Library/Python/3.6/bin를 시스템 PATH 환경 변수에 추가해주면 됩니다. zsh를 사용하신다면zsh를 사용한다면 .zshrc파일에서 아래와 같이 입력해주면 됩니다. Python3.5나 3.4를 사용한다면 숫자 3.6을 3.5,3.4로 버전에 맞게 바꿔 사용하세요. 1echo 'export PATH=\"/Users/$(whoami)/Library/Python/3.6/bin:$PATH\"' &gt;&gt; .zshrc bash를 사용하신다면zsh를 사용한다면 .bashrc파일에서 아래와 같이 입력해주면 됩니다. 1echo 'export PATH=\"/Users/$(whoami)/Library/Python/3.6/bin:$PATH\"' &gt;&gt; .bashrc 이제 터미널을 종료한 뒤 다시 켜면 fab등 명령어가 잘 실행되는 것을 볼 수 있습니다.","link":"/2018/02/12/Add-packages-installed-with-pip-usermode/"},{"title":"django에 MSSQL 연결하기","text":"이번 글은 macOS에서 개발하는 경우입니다. Django와 MSSQL, 그리고 개발 환경이 macOS라면 상당히 연결해 사용하기 어려운 조합입니다. Django에서 MSSQL을 지원하는 라이브러리는 몇가지 있지만 Django 공식 문서에서 MSSQL을 지원하는 ORM 라이브러리로 소개하는 django-mssql의 경우 django 1.8까지만 지원하는 문제가 있습니다. 하지만 현재(2018-02-02 기준) 가장 최신 장고 버전은 무려 2.0.2입니다. 상당히 오래된 버전만을 지원한다는 문제가 있습니다. 따라서 다른 라이브러리를 사용할 필요가 있습니다. 이번에는 Python3와 Django2.0을 모두 지원하는 django-pyodbc-azure를 사용합니다. django-pyodbc-azure 설치하기django-pyodbc-azure는 pip를 통해 아래와 같이 설치할 수 있습니다. 1pip install django-pyodbc-azure django-pyodbc-azure는 pyodbc라이브러리를 기반으로 장고 ORM을 이용할 수 있도록 만들어주는데, ODBC는 Native 드라이버를 필요로 하기 때문에 다음과 같이 여러 라이브러리를 설치해줘야 합니다. 이번 설치는 HomeBrew를 사용합니다. 12345678brew install unixodbcbrew install freetds --with-unixodbcbrew tap microsoft/msodbcsql https://github.com/Microsoft/homebrew-mssql-releasebrew updatebrew install --no-sandbox msodbcsqlbrew install mssql-toolsbrew install autoconf 이제 Django 프로젝트에 MSSQL을 연결해 사용할 수 있습니다. django settings.py 파일에 DB 설정하기위에서 설치해준 django-pyodbc-azure는 sql_server.pyodbc라는 엔진 이름으로 django와 연동할 수 있습니다. 아래처럼 settings.py 파일 내 DATABASE부분을 수정해주세요. 1234567891011# settings.py 파일# 앞뒤 코드 생략DATABASES = { 'default': { 'NAME': 'DataBase이름', 'ENGINE': 'sql_server.pyodbc', 'HOST': 'DB의 IP', 'USER': 'DB접근 ID', 'PASSWORD': 'DB접근 PW', }} 해당 DB에 정상적으로 액세스 할 수 있다면 migrate, runserver등 장고 명령어가 성공적으로 실행됩니다.","link":"/2018/02/02/Connect-django-to-MSSQL/"},{"title":"DjangoORM에서 SQL Driver 지정해 Query & Pandas DataFrame 얻어내기","text":"들어가며장고의 매력적인 기능 중 하나는 ORM을 통해 SQL을 직접 작성하지 않아도 된다는 점입니다. 즉, 우리가 파이썬 코드를 작성하면 모델 매니저와 SQL Driver를 거쳐 실제로 SQL문으로 만들어주는 일을 장고가 대신해줍니다. 그리고 장고가 DB를 바라보는 방법은 settings.py파일 내 DATABASE설정 통합니다. 그리고 default로 설정된 데이터베이스를 참고해 ORM을 제공합니다. 하지만 이점은 장고 프로젝트 하나에서 여러 데이터베이스를 바라보며 사용할 경우 문제가 발생합니다. 만약 단순하게 모델의 특정 클래스만을 특정 데이터베이스를 바라보게 하려면 다음과 같이 settings.py를 작성할 수 있습니다. 장고의 데이터베이스가 아래와 같이 default와 anotherdb로 두개가 있다고 가정해 봅시다. default는 MySQL을, anotherdb는 MSSQL을 사용합니다. (SQL 문법이 비슷하지만 약간 다릅니다.) 123456789101112131415161718DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', # MYSQL 'NAME': 'MYSQLDB', 'HOST': 'localhost', 'USER': 'dbuser', 'PASSWORD': 'dbpassword', 'OPTIONS': { 'sql_mode': 'STRICT_TRANS_TABLES', }, }, 'anotherdb': { 'ENGINE': 'sql_server.pyodbc', # MSSQL 'HOST': '1.23.4.56', 'USER': 'anotheruser', 'PASSWORD': 'anotherpassword', }} 이와 같은 경우 모델 클래스별로 다른 DB를 사용하도록 커스텀 데이터베이스 라우터를 만들어 줄 수 있습니다. 123456789101112131415161718192021# settings.py 내class DatabaseRouter: def db_for_read(self, model, **hints): return getattr(model, \"_DATABASE\", None) def db_for_write(self, model, **hints): return getattr(model, \"_DATABASE\", None) def allow_relation(self, obj1, obj2, **hints): db_list = ('default') return obj1._state.db in db_list and obj2._state.db in db_list def allow_migrate(self, db, app_label, model_name=None, **hints): if db == 'default': return True else: return FalseDATABASE_ROUTERS = [ DatabaseRouter(),] 그리고 models.py파일에서는 아래와 같이 _DATABASE속성을 넣어주는 방법으로 라우터를 이용할 수 있습니다. 123456# someapp/models.pyclass Post(models.Model): _DATABASE = 'anotherdb' # 없으면 자동으로 'default' Fallback title = models.CharField(max_length=200) content = models.TextField() 이후 아래와 같이 Post 모델의 모델매니저를 통해 액세스 할 경우 anotherdb를 이용하게 됩니다. 1queryset = Post.objects.all() # anotherdb로 연결 문제하지만 문제가 발생하는 부분이 있습니다. 만약 Queryset을 통해 실제 동작하는 query와 params를 알아내 pandas에서 SQL Query를 읽어 DataFrame 객체로 바꾸는 경우에는 아래와 같이 queryset.query로 쿼리에 접근하게 됩니다. 1234567import pandas as pdfrom django.db import connectionsqueryset = Post.objects.all() # QuerySetquery, params = queryset.query.sql_with_params()# df는 Pandas의 DataFrame가 된다.df = pd.read_sql_query(query, connections['anotherdb'], params=params) 위 코드에서 Post 모델의 속성중 _DATABASE를 통해 커스텀 데이터베이스 라우터로 anotherdb를 바라보도록 만들어주었지만 실제 쿼리를 출력해 볼 경우 MSSQL 쿼리 대신 MySQL 쿼리로 나오는 것을 볼 수 있습니다. 따라서 동작시 에러가 발생합니다. 해결법쿼리셋을 만들고 query 객체에 접근한 뒤 .sql_with_params() 대신 .as_sql()메소드를 이용해 compiler옵션에 해당 데이터베이스의 SQLCompiler 클래스를 직접 전달해주거나 혹은 문자열로 경로를 지정해준 뒤, connection에 실제 사용할 데이터베이스 명칭(DATABASES의 키값, 없으면 default가 됩니다.)을 넣어줍니다. 12query, params = queryset.query.as_sql(compiler='sql_server.pyodbc.compiler.SQLCompiler', connection=connections['anotherdb'])df = pd.read_sql_query(query, connections['anotherdb'], params=params) 이제 df에 정상적으로 데이터가 들어온 DataFrame 객체가 만들어집니다. 편하게 씁시다, 함수 만들기매번 저렇게 커넥션을 처리해주는 것도 사실 귀찮은 일입니다. 그래서 간단하게 to_df라는 함수를 만들어 세가지 인자를 넣으면 처리할 수 있도록 해줍시다. queryset: 장고의 QuerySet 혹은 raw SQL(str) using: settings.py에 등록한 DB이름(‘default’, ‘anotherdb’등) compiler: 해당 DB의 SQLCompiler, import를 통해 가져온 실제 SQLCompiler 클래스 혹은 해당 경로의 문자열 12345678910111213141516171819202122232425import pandas as dffrom django.db import connectionsfrom django.core.exceptions import EmptyResultSetdef to_df(queryset, using=None, compiler=None): try: if type(queryset) == str: # SQL이 문자열로 그대로 들어올 경우 query = queryset params = None else: if using: # 어떤 DB를 사용할지 지정한다면.. con = connections[using] else: con = connections['default'] if compiler: # 어떤 SQLCompiler를 사용할지 지정한다면.. query, params = queryset.query.as_sql(compiler=compiler, connection=con) else: query, params = queryset.query.sql_with_params() except EmptyResultSet: # 만약 쿼리셋의 결과가 비어있다면 빈 DataFrame 반환 return pd.DataFrame() if using: # 어떤 DB를 사용할지 지정했다면 해당 DB connection 이용 df = pd.read_sql_query(query, connections[using], params=params) else: df = pd.read_sql_query(query, connection, params=params) return df","link":"/2018/02/19/django_orm_for_multiple_db_with_sqldriver/"},{"title":"Django: Truncated or oversized response headers received from daemon process 에러 해결법","text":"문제 발생 환경 OS: Ubuntu 16.04 LTS Python 3.5.2 Django 2.0.2 Apache HTTPd 2.4 numpy / Pandas / pymssql 등 사용중 문제의 발생장고 배포를 마친 뒤 배포 서버에 접속시 화면이 뜨지 않고 500에러가 났던 상황. 1Timeout when reading response headers from daemon process 'djangoproject': /home/ubuntu/djangoproject/djangoproject/wsgi.py 에러 로그로 살펴보면 위와 같이 “Timeout when reading response headers from daemon process”이라는 문제가 발생했다. 문제 원인Numpy나 Pandas와 같은 C 의존 라이브러리들은 파이썬 인터프리터 중 메인 인터프리터에서 사용해야 한다. 만약 mod_wsgi등을 통해 생성된 서브 인터프리터를 사용할 경우 GIL로 인한 Deadlock이 발생하거나 정확하지 않은 결과, 혹은 파이썬 인터프리터의 예기치 못한 종료를 유발할 수 있다. 해결법따라서 WSGI Application에서 사용할 파이썬 인터프리터에다 시스템의 메인 인터프리퍼를 지정해주면 된다. /etc/apache2/apache2.conf 경로의 파일 제일 아래에 아래 코드를 추가해준다. 1WSGIApplicationGroup %{GLOBAL} 코드를 추가해 준 뒤 Apache2를 재시작(service apache2 restart)한다. Refs (Serverfault) WSGI : Truncated or oversized response headers received from daemon process (Stackoverflow) Django Webfaction ‘Timeout when reading response headers from daemon process’ (Serverfault) Non-responsive apache + mod_wsgi after installing scipy (Google Code) summary Common problems with WSGI applications","link":"/2018/03/09/Truncated_or_oversized_response_headers_received_from_daemon_process_django_wsgi/"},{"title":"PySpark: 손상된 parquet파일 무시하기","text":"문제PySpark를 이용해 파일을 읽어와 DataFrame 객체로 만드는 경우 읽어오는 파일이 parquet 파일이라면 이 파일이 어떤 형식으로 되어있는지(어떤 Column/Type으로 이루어져있는지)에 대한 정보를 필요로 합니다. 보통 parquet파일에 이 파일에 대한 스키마가 저장되어있어 파일을 읽고 쓰는데 지장이 없습니다. 하지만 간혹 parquet파일이 깨져있는 경우가 있습니다. 12345678910111213141516171819202122232425# spark 는 SparkSession 객체path = [ 's3a://some-bucket/brokenfile.parquet', # Broken!]df = spark.read.parquet(*path) # SparkException!``` 위와 같은 코드를 실행할 경우 아래와 같이 깨진 파일이 속한 parquet파일들을 읽으려 할 경우 아래와 같이 `org.apache.spark.SparkException`이 발생합니다.![SparkException](https://d1sr4ybm5bj1wl.cloudfront.net/img/dropbox/2018-02-26 PM 2.54.19.png)로그를 살펴보면 \"Could not read footer for file\" 이라는 문구가 보입니다. 즉, parquet파일의 footer가 손상되어 파일을 읽어오지 못합니다. 하지만 이 파일 하나만 문제가 있다 하더라도 전체 과정이 멈춰버립니다. 더 심각한 문제는 만약 `*path`중 첫 번째 파일의 footer가 정상적이었다면 저 `path` 리스트 중 한 파일이 문제가 있다 하더라도 Spark의 lazy loading, lazy computing으로 인해 `.show()`나 `.count()`와 같이 실제 데이터가 필요한 코드를 실행하기 전까지는 데이터를 불러오지 않고 메타게이터만 연결된 DataFrame 객체를 사용하기 때문에 파이썬 코드들이 정상적으로 작동하더라도 실제 parquet파일이 깨졌다는 사실을 알 수가 없다는 것입니다.```pythonpath = [ 's3a://some-bucket/normal1.parquet', # 정상 's3a://some-bucket/normal2.parquet', 's3a://some-bucket/normal3.parquet', 's3a://some-bucket/brokenfile.parquet', # Broken! 's3a://some-bucket/normal4.parquet', # ...]df = spark.read.parquet(*path) # 정상적으로 실행된다. 해결 방법우선 손상된 parquet파일을 무시하고 나머지 정상적인 파일이라도 불러와 DataFrame을 만들어봅시다. 아래 설정은 스파크 세션을 생성할 때 설정값으로 넣거나, 혹은 세션을 만든 뒤 만들어진 spark와 같은 SparkSession객체에 설정으로 진행해도 됩니다. 이번에는 이미 생성된 spark 객체에 설정값을 바꿔 사용해봅니다. .read.parquet(*path)를 실행하기 전에 아래와 같이 설정을 넣어줍시다. 1spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\",\"true\") 아래와 같이 코드를 만들어 줍시다. 1234567891011path = [ 's3a://some-bucket/normal1.parquet', # 정상 's3a://some-bucket/normal2.parquet', 's3a://some-bucket/normal3.parquet', 's3a://some-bucket/brokenfile.parquet', # Broken! 's3a://some-bucket/normal4.parquet', # ...]spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\",\"true\")df = spark.read.parquet(*path) # 정상적으로 실행된다. 이제 무시된 파일의 데이터는 제외하고 나머지 파일의 데이터로 이루어진 정상적인 DataFrame객체가 생성됩니다. 남은 문제만약 parquet파일의 리스트인 path가 모두 손상된 파일로 이루어졌다면 아래와 같은 AnalysisException 에러가 발생합니다. ![AnalysisException](https://d1sr4ybm5bj1wl.cloudfront.net/img/dropbox/2018-02-26 PM 2.51.24.png) ignoreCorruptFiles 옵션을 true로 설정하고 작업을 진행할 경우 에러가 있는 파일 부분은 읽지 않아 만약 위와 같이 단 하나의 파일만 읽을 경우 빈 Spark DataFrame객체가 생성되는데, 이때 DataFrame의 Scheme이 없기 때문에(읽은 파일이 없으니까!) ‘Unable to infer schema for Parquet. It must be specified manually.;’ 라는 에러가 발생하게 됩니다.","link":"/2018/02/26/PySpark-Read-Parquet-ignoreCorruptedFiles/"},{"title":"GPU EC2 스팟 인스턴스에 Cuda/cuDNN와 Tensorflow/PyTorch/Jupyter Notebook 세팅하기","text":"들어가며Tensorflow나 PyTorch등을 사용하며 딥러닝 모델을 만들고 학습을 시킬 때 GPU를 사용하면 CPU만 사용하는 것에 비해 몇배~몇십배에 달하는 속도향상을 얻을 수 있다는 것은 누구나 알고 있습니다. 그래서 비싼 GPU를 사용하고 낯선 리눅스 환경을 이용하기도 합니다. 하지만 실제로 GPU, 특히 Cuda를 이용한 GPU가속을 세팅하고 cuDNN등을 통해 각 머신러닝 라이브러리에서 속도를 향상시키려고 할 때는 항상 무언가 문제가 발생합니다. 물론 Floydhub혹은 AWS SageMaker와 같이 이미 GPU 가속 환경이 마련되어있는 경우는 필요가 없지만, GPU 인스턴스의 시간당 요금 자체가 상당히 높습니다. k80 GPU를 제공하는 경우 시간당 약 1~2달러의 비용이 발생합니다. 조금이라도 저렴하게 GPU를 사용하고, 한번 설정된 GPU 인스턴스를 그대로 유지하기 위해 스팟 인스턴스를 사용해 봅시다. 오늘자(2018.03.18)기준 p2.xlarge(CPU 4 Core / RAM 60GB / GPU k80) 스팟 인스턴스 가격은 시간당 0.4395달러입니다. (원래 1.4650달러로, 70% 저렴하게 사용 가능합니다.) 만들기!이번 글에서는 Ubuntu 16.04 LTS 위에 아래 패키지와 라이브러리들을 설치하는 내용을 다룹니다. Python 3.5 Tensorflow 1.6.0 (GPU) PyTorch 0.3.1 (GPU) CUDA 9.0 cuDNN 7.0.5 (for CUDA 9.0) 우분투 업데이트우선 EC2를 처음 띄웠으니 패키지들을 모두 최신버전으로 업데이트 해 줍시다. 만약 작업 중 Dependencty 패키지의 버전을 업데이트 할 것이냐는 질문이 나오면 ‘로컬 버전 사용하기’를 눌러줍시다. 위 스크린샷과 같이 아래 명령어를 입력하고 잠시 기다리면 우분투 패키지가 모두 업데이트됩니다. 1sudo apt-get update &amp;&amp; sudo apt-get upgrade -y pip3 설치하기Ubuntu16.04에는 Python3이 기본적으로 설치되어있지만 pip3은 설치되어있지 않습니다. 아래 명령어로 설치해 사용해봅시다. 1sudo apt-get install -y python3-pip CUDA GPU 확인하기 (Optional)현재 ubuntu에 붙어있는 GPU가 있는지 확인하려면 아래 명령어를 이용해 확인해 볼 수 있습니다. 물론 AWS p2 인스턴스로 띄우셨다면 당연히 CUDA를 지원하는 그래픽 카드가 붙어 있습니다 :) 1lspci | grep -i nvidia 만약 아래 스크린샷과 같이 GPU가 나온다면 이 환경에서는 Cuda 가속을 이용할 수 있습니다! CUDA Toolkit 9.0 설치하기CUDA를 사용하기 위해서 CUDA Toolkit 9.0을 설치해야 합니다. 아래 명령어를 하나씩 입력해 실행해주세요. 12345678910111213141516171819202122232425262728293031323334353637383940# Nvidia Debian package Repo 등록 패키지 다운로드wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.0.176-1_amd64.deb# Nvidia APT 키 등록하기sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub# Nvidia Repo APT 등록하기sudo dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb# Cuda 9.0 설치하기sudo apt-get updatesudo apt-get install cuda-9-0# Cuda ToolKit 설치하기 (nvcc)sudo apt install nvidia-cuda-toolkit``` 한줄씩 입력하면 약간의 시간이 지난 뒤 CUDA 9.0 설치가 끝납니다. 그리고 작업 중 추가적으로 그래픽카드 드라이버 최신버전도 함께 설치되기 때문에 그래픽 드라이버는 따로 설치하지 않아도 됩니다.&gt; 만약 무언가 문제가 발생한다면 [Nvidia CUDA Toolkit 9.0 Downloads](https://developer.nvidia.com/cuda-90-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1604&amp;target_type=debnetwork)를 참고하세요.### cuDNN 7.0.5 설치하기cuDNN을 사용하기 위해서는 Nvidia Developer Membership에 가입해야 합니다. 가입은 nvidia 개발자 사이트에서 진행할 수 있으며, [cuDNN Download Page](https://developer.nvidia.com/rdp/cudnn-download)에서 바로 가입하실 수 있습니다.이미 계정이 있다면 [cuDNN Download Page](https://developer.nvidia.com/rdp/cudnn-download)에서 cuDNN 7.0.5 for CUDA 9.0을 클릭해 주세요.![cuDNN 7.0.5 클릭](https://d1sr4ybm5bj1wl.cloudfront.net/img/dropbox/2018-03-18%2017.45.59.png)그리고 아래 스크린샷처럼 cuDNN v7.0.5 Library for Linux를 클릭해 주시면 파일이 다운로드 됩니다.![클릭 후](https://d1sr4ybm5bj1wl.cloudfront.net/img/dropbox/2018-03-18%2017.47.46.png)하지만 우리는 cuDNN을 서버에서 사용할 것이기 떄문에 해당 링크 주소를 복사해 사용해야 합니다.&gt; 만약 단순히 링크를 복사해 사용하면 403 Forbidden 에러가 뜹니다.![cuDNN download url 복사하기](https://d1sr4ybm5bj1wl.cloudfront.net/img/dropbox/2018-03-18%2017.52.11.png)따라서 위와 같이 파일의 다운로드 경로를 복사해옵시다. 경로를 복사하면 아래와 같이 복잡한 문자열이 붙은 URL이 됩니다.```sh# 예시 URLhttp://developer.download.nvidia.com/compute/machine-learning/cudnn/secure/v7.0.5/prod/9.0_20171129/cudnn-9.0-linux-x64-v7.tgz?t8V0cLo2oAM-UT86ONPbFAF6Gae61AEK5a9KdkSzG9M5slquBxMffldmWEC8cNHOKiCpQWJx9WXgt6mKaFnDpq_zGVxVGTNyajaGQv4nQef2W0CBpe8Y9NKRycBGUF8k 이제 얻어온 URL을 이용해 cuDNN을 서버에 다운로드 받아줍시다. 아래 명령어로 위 링크를 다운로드받아줍시다. 1wget 위에서_받아온_URL 다운로드가 완료되면 아래 명령어를 차례대로 입력해 주세요. 12345mv cudnn* cudnn.tgztar -xzvf cudnn.tgzsudo cp cuda/include/cudnn.h /usr/local/cuda/includesudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 이제 Tensorflow와 PyTorch를 설치해줍시다. Tensorflow-GPU 설치하기Tensorflow의 GPU버전도 pip3으로 쉽게 설치할 수 있습니다. 아래 명령어를 입력해 Tensorflow를 설치해주세요. 1pip3 install tensorflow-gpu PyTorch-GPU 설치하기PyTorch 역시 pip3으로 설치할 수 있습니다. 아래 명령어를 통해 PyTorch를 설치해주세요. 12pip3 install http://download.pytorch.org/whl/cu90/torch-0.3.1-cp35-cp35m-linux_x86_64.whl pip3 install torchvision --user Jupyter Notebook 설치 및 설정SSH만으로 작업하는 대신 Jupyter Notebook 서버를 띄워 이용해봅시다. 설치아래 명령어로 Jupyter Notebook을 설치해주세요. 1pip3 install jupyter --user 서버 설정Jupyter를 띄우고 패스워드로 접속하기 위해서는 아래 스크린샷처럼 설정파일을 만든 뒤 패스워드를 생성해야 합니다. 설정 파일 생성은 다음 명령어로 쉽게 만들 수 있습니다. 1jupyter notebook --generate-config 그리고 원격 서버에서 접속할 때는 Jupyter Notebook의 토큰을 확인하기 어렵기 때문에 토큰 대신 지정한 패스워드를 이용하도록 바꿔줍시다. 먼저 패스워드를 생성해줍니다. 1jupyter notebook password Jupyter Notebook은 기본적으로 localhost에서의 요청만을 받습니다. 즉, 원격 브라우저에서의 접속이 기본적으로 되어있습니다. 따라서 이 설정값을 바꿔줍시다. 1234# localhost가 아닌 모든 ip를 듣기 sed -i 's/#c.NotebookApp.ip = '\"'\"'localhost'\"'\"'/c.NotebookApp.ip = '\"'\"'*'\"'\"'/' ~/.jupyter/jupyter_notebook_config.py# 자동으로 브라우저 켜는 기능 끄기sed -i 's/#c.NotebookApp.open_browser = True/c.NotebookApp.open_browser = False/' ~/.jupyter/jupyter_notebook_config.py Jupyter Notebook 데몬 서비스화이제 Jupyter Notebook 설정이 끝났습니다. 하지만 매번 서버를 켤 때마다 터미널에서 Jupyter Notebook을 켜고 작업하기는 귀찮으니 Deamon화를 해 봅시다. 12sudo mkdir /usr/lib/systemd/systemsudo touch /usr/lib/systemd/system/jupyter.service 그리고 vi 등 편집기를 이용해 아래 내용을 넣어줍시다. 123456789101112131415[Unit]Description=Jupyter Notebook[Service]Type=simplePIDFile=/run/jupyter.pidExecStart=/home/ubuntu/.local/bin/jupyter-notebook --config=/home/ubuntu/.jupyter/jupyter_notebook_config.pyUser=ubuntuGroup=ubuntuWorkingDirectory=/home/ubuntu/Restart=alwaysRestartSec=10[Install]WantedBy=multi-user.target 그리고 아래 네가지 명령어를 입력해주시면 Jupyter Notebook이 서비스로 띄워진 것을 확인할 수 있습니다. 1234sudo systemctl enable jupyter.servicesudo systemctl daemon-reloadsudo systemctl restart jupyter.servicesystemctl -a | grep jupyter EC2 접속해 확인하기이제 해당 EC2로 들어가봅시다. 처음에 비밀번호를 입력하라고 뜨면 위에서 Jupyter Notebook 패스워드로 설정해준 값을 넣어 들어가봅시다. GPU 가속까지 설정이 잘 된 것을 볼 수 있습니다. Yeah! 이렇게 만든 EC2에 8888 포트를 Security Group에서 열어줘야 접근이 가능합니다. 혹시 접근이 되지 않는다면 Security Group을 확인하세요! 기본적으로 부여되는 Security Group은 default입니다. 커스텀 AMI 만들기EC2를 새로 생성할 때는 커스텀 AMI를 사용해 띄울 수 있습니다. 커스텀 AMI는 EC2 볼륨 스냅샷을 기반으로 생성됩니다. 우리가 사용할 GPU 가속된 딥러닝 환경이 모두 세팅되었으니 이제 이 인스턴스의 볼륨을 스냅샷으로 찍어 새로 만드는 볼륨은 항상 이 스냅샷에서 시작하도록 만들어줍시다. 아래와 같이 스냅샷을 생성해 줍시다. 스냅샷 생성이 끝나면 AMI를 만들어줘야 합니다. 만들어진 스냅샷에 우측클릭을 하고 ‘이미지 생성’을 눌러주세요. 다음과 같이 ‘이름’을 적어주고, ‘가상화 유형’은 하드웨어 보조 가상화 혹은 hvm을 선택하신 뒤 ‘생성’을 눌러주세요. 주의: 가상화 유형에서 반가상화 (PV)를 선택하시면 EC2 인스턴스를 띄우실 수 없습니다. 시간이 조금 소요된 후 AMI가 성공적으로 생성되면 아래와 같이 ‘내 AMI’ 목록에 방금 만들어준 이미지가 나타납니다. 이렇게 생성된 AMI는 다음과 같이 새로운 온디맨드 EC2를 실행하거나 혹은 스팟 인스턴스를 요청하는데 사용할 수 있습니다. 맺으며이번 글에서는 AWS의 스팟 인스턴스를 통해 저렴한 (1/3도 안되는) 가격에 딥러닝을 위한 GPU 인스턴스를 띄우고 CUDA와 cuDNN, 그리고 Tensorflow와 PyTorch를 GPU 가속이 가능한 상태로 만드는 과정을 진행했습니다. 마지막 AMI를 만드는 과정까지 진행하시면 필요할 때 마다 온디맨드 혹은 스팟 요청을 통해 새로운 EC2를 켜더라도 이미 모델 개발을 위한 환경이 구축된 상태로 작업을 진행할 수 있습니다. 다만 스팟 요청은 EC2가 생성될 경우 매번 새로운 EBS(스토리지)를 생성하기 때문에 저 상태에서 사용했던 데이터가 유실됩니다. 따라서 재사용하고자 하는 데이터의 종류에 따라 다른 선택을 해야 합니다. 자주 사용하는 패키지를 모두 깔아두고 싶으신 경우:앞서 진행했던 ‘스냅샷 생성’ =&gt; ‘AMI 생성’ 과정을 진행하기 전, 미리 패키지를 모두 깔아 두신 뒤 스냅샷과 AMI를 생성하시면 됩니다.사용한 데이터셋과 모델 파일 등은 유실되지만 패키지를 재설치할필요는 없습니다. 데이터도 보존하고 싶은 경우:현재 EC2를 만들 때 기존의 볼륨을 루트 디바이스로 붙이지는 못합니다. 또한 스팟 인스턴스에는 남는 볼륨(EBS)도 붙이지 못하기 때문에 이런 경우에는 스팟 인스턴스 대신 다른 전략을 사용해야 합니다.즉, 일반 온디맨드 EC2로 켜야 합니다. 다만 온디맨드 인스턴스에는 CloudWatch를 이용해 n분 이상 idle 상태인 경우 인스턴스를 중지시켜 요금을 줄이는 방법이 있습니다. 데이터도 보존하고싶고 스팟인스턴스도 사용하고 싶은 경우:매 종료 전 AMI를 새로 생성하고 종료하면 되지만, s3 공간을 낭비하고 이 시간 자체가 비용이 되기 때문에 추천하지 않습니다 :(","link":"/2018/03/18/Create_GPU_spot_EC2_for_ML/"},{"title":"AWS Lambda Layers로 함수 공통용 Python 패키지 재사용하기","text":"들어가며올해 AWS Re:Invent에서 새로 발표된 기능 중 AWS Lambda에 새로운 전환점을 가져오는 기능이 발표되었습니다. 바로 Custom Runtime 지원과 Layers 지원이 추가된 것인데요,이번 글에서는 두가지 기능 중 “Layers” 기능에 대해 알아봅니다. Lambda Layers가 무엇인가요?사실 아직까지 많은 정보가 나오지는 않았는데요, Lambda Layers 추가 소개 문서를 살펴보면어떤 방식으로 동작하는지 대략적인 감을 잡을 수 있습니다. 아래 글은 위 링크 내용 중 Lambda Layers에 대한 간략한 소개 부분입니다. Lambda Layers are a new type of artifact that can contain arbitrary code and data, and may be referenced by zero, one, or more functions at the same time. Lambda functions in a serverless application typically share common dependencies such as SDKs, frameworks, and now runtimes. With layers, you can centrally manage common components across multiple functions enabling better code reuse. To use layers, you simply put your common code in a zip file, and upload it to Lambda as a layer. You then configure your functions to reference it. When a function is invoked, the layer contents become available to your function code. We are also providing a layer which includes the popular NumPy and SciPy scientific libraries for Python. … Read more about Lambda Layers in the AWS Lambda documentation. 핵심적인 부분을 bold 처리 해 두었는데요, 위 내용은 다음과 같이 요약할 수 있습니다. “Lambda에 코드 만들어 올릴 때 매번 패키지(pip 패키지 등) 세트 만들어 올리는거 귀찮았지? 같은 Dependency 가지는 함수라면 코드만 따로 빼고 의존성 패키지는 Layers라는 곳으로 빼서 사용해!” 즉, 굉장히 편리해진 요소가 추가된 것이죠. 그렇다면 Layers는 어떻게 동작할까요? 그래서 뭐가 바뀐건가요?기존 Lambda Packaging의 한계AWS에 익숙한 분이시라면 이미 아시겠지만 AWS Lambda는 굉장히 많은 제약을 가지고 있습니다. 서버리스라는 인프라 구조적 한계부터 시작해, 코드 용량(현재 max 250MB)의 압박, I/O의 제약(/tmp에만 500MB), 그리고 Ram용량(3G)의 한계와 실행시간(15분)의 한계까지 굉장히 많은 한계가 있습니다. 방금 언급한 부분 중 코드 용량(현재 max 250MB)의 압박, I/O의 제약(/tmp에만 500MB)으로 인해 pip를 이용한 패키지 설치 등이 불가능하고, 동시에 운영체제에 의존해 빌드가 필요한 패키지 등의 경우는 사용이 굉장히 까다롭기까지 합니다. 그렇다면 지금까지는 어떻게 이 문제를 회피해 왔을까요? 지금까지는 아래 과정을 통해 문제를 피해왔습니다. 한 폴더를 지정하고, 해당 폴더 내에 모든 패키지를 넣어본다. (pip로 설치한 site-packages폴더를 통으로 프로젝트에 넣는다.) 단, 250MB 이내여야 한다. 만약 실패할 경우(C의존 라이브러리 등) Docker나 EC2를 이용해 AmazonLinux 운영체제에서 빌드한 뒤 해당 의존성 패키지들을 한 폴더에 같이 넣는다. 위 상황에서 압축 해제시 250MB가 넘는다면 strip등을 이용해 필요없는 파일을 제거하거나 파일 용량을 압축하는 등 용량을 줄인다. 그래도 용량이 넘친다면 패키지를 반으로 쪼개고, Lambda 함수가 실행될 때 s3에서 두번째 패키지를 다운받아 /tmp에 압축을 풀어 사용한다. 사실 AWS Lambda Layers가 추가된 지금도 여전히 위 제약들은 그대로 살아있습니다. ㅠㅠ 하지만 위 과정에서 필요했던 여러 과정을 줄일 수 있게 됩니다. 예를들어 한 함수에서 requests라는 라이브러리를 사용하고 있었고, 다른 함수에서도 해당 라이브러리를 사용하려고 한다고 가정해 봅시다.기존에는 Lambda 함수를 만들 때 마다 해당 라이브러리 코드와 의존성 모듈들을 소스코드와 함께 묶어 업로드를 진행해야 했습니다. 상당히 귀찮은 일이죠. 그런데 Lambda Layers가 나오면서 이런 이슈가 엄청나게 줄어들었습니다. 그렇다면 Lambda Layers가 대체 어떤 일을 해주기에 일이 줄어든 걸까요? Lambda Layers는 어떻게 작동하나요?공식 문서: Lambda Layers 설정하기에서는 Lambda Layers가 아래와 같이 작동한다고 말합니다. Layers are extracted to the /opt directory in the function execution environment. Each runtime looks for libraries in a different location under /opt, depending on the language. Structure your layer so that function code can access libraries without additional configuration. 가장 중요한 부분은 “Layers가 하나의 ‘압축 파일’이며, /opt 폴더에 압축해제되는 것”입니다. 즉, Layers가 뭔가 특별한 것이 아니라 압축파일 하나를 /opt 폴더에 풀어준다는 것 뿐입니다. 이렇게 말만 들으면 기존 방식가 뭐가 다르지? 하는 의문이 생길 수 있습니다. 하지만 앞서 나온 소개와 맞물리며 이야기가 조금 달라집니다. Layers let you keep your deployment package small, which makes development easier. You can avoid errors that can occur when you install and package dependencies with your function code. For Node.js, Python, and Ruby functions, you can develop your function code in the Lambda console as long as you keep your deployment package under 3 MB. 작년 Amazon이 c9.io 서비스를 인수하며 Lambda 서비스 업데이트에 추가되었던 기능 중 하나가 바로 Lambda 콘솔에서 곧바로 코드 수정이 가능해졌다는 것입니다.하지만 이 방식으로 코드를 수정하려면 해당 Lambda 함수의 패키지 크기가 3MB 이하여야 했다는 점인데요,기존 방식으로 모든 의존성 패키지들을 압축해서 사용한다면 3MB는 정말 작고도 작은 크기입니다.단순히 typo 하나 수정을 위해서 패키지를 빌드하는 과정을 다시 반복해야 하는 것은 개발자에게 굉장한 고통으로 다가오는 것인데,용량을 조금 많이 사용하기 위해서 패키징을 했더니 Lambda console에서 코드 수정이 불가능해진 것이죠. 심지어 기존에는 코드와 모듈들을 합쳐서 압축한 zip파일 크기가 50MB가 넘어가는 경우에는 AWS Console상에서 업로드 하는 것도 불가능해서 S3에 올린 뒤 해당 S3의 경로를 Lambda 콘솔에 붙여넣기 해주어야 했습니다.(심지어 자동완성도 불가능해서 매번 해당 s3://~~~하는 주소를 복사-붙여넣기 해야 했죠!) 그렇다면 Lambda Layers가 등장하면 어떻게 바뀌는 것일까요? 우선 의존성 패키지를 압축하는 것은 동일합니다. 단, 기존에는 소스코드를 함께 패키징했다면 이제는 의존성 모듈만 패키징하게 된다는 것이 가장 달라지는 점입니다. 이렇게 되면 소스코드는 처음 업로드 할 때만 zip파일로 압축해 업로드하고 이후 수정시에는 AWS Lambda Console에서 곧바로 수정 가능합니다. Lambda Layers는 어떻게 사용하나요?Lambda 함수로 만들 코드 작성하기아주아주 간단하고 심플한 크롤링 코드를 Lambda에 올려 사용한다고 가정해봅시다. 해당 코드는 requests와 bs4라는 모듈을 사용합니다.이 블로그를 긁어 h1태그 하나의 글자를 가져와봅시다. 12345678910111213import jsonimport requestsfrom bs4 import BeautifulSoup as bsdef lambda_handler(event, context): # TODO implement res = requests.get('https://beomi.github.io') soup = bs(res.text, 'html.parser') blog_title = soup.select_one('h1').text return { 'statusCode': 200, 'body': json.dumps(blog_title) } 로컬에서 requests, bs4가 설치된 상태에서 lambda_handler 함수를 실행시 결과는 다음과 같습니다. 1234{ \"statusCode\": 200, \"body\": \"\\\"Beomi's Tech Blog\\\"\"} 하지만 아무런 준비 없이 AWS Lambda 콘솔에서 위 코드를 저장하고 실행하면 아래와 같은 No module named 'requests' 에러가 납니다. 12345678Response:{ \"errorMessage\": \"Unable to import module 'lambda_function'\"}...Unable to import module 'lambda_function': No module named 'requests' 위 에러 메시지는 requests라는 모듈을 찾을 수 없다는 파이썬 에러입니다. 당연히 설치되어있지 않기 때문에 에러가 발생합니다. Lambda Layers를 이용해 이 이슈를 해결해봅시다. 크롤링 의존 패키지들 Lambda Layers로 만들기이제 AWS Lambda Console을 켜 줍시다. Lambda 서비스 항목 중 “계층” 혹은 Layers를 누르고 “계층 생성”을 눌러줍시다. 아래와 같이 새로운 Lambda Layer를 생성하는 창이 뜹니다. 이제 requests와 bs4가 들어있는 zip 압축파일을 업로드해야 하는데요, 크롤링을 위한 패키지가 아래 Github Repo에 준비되어 있습니다. 아래 Direct Download 링크를 통해 pack.zip파일을 받아 업로드 해주세요. Github Repo: https://github.com/Beomi/aws-lambda-py3 requests + bs4 + lxml ; Direct Download 그리고 Runtime으로 python3.6/python3.7를 선택해 줍시다. (여러분이 Layer를 만들때는 해당 Layer가 사용될 환경을 모두 선택해주세요.) 업로드가 성공하면 아래와 같이 새로운 Lambda Layer가 생성됩니다. 참고로 각각의 Layer는 버전별로 수정이 불가능하고 만약 수정이 필요하다면 zip파일을 다시 올리고 새로운 리비전이 생성됩니다. Lambda Function 생성 + Layers 붙이기아래와 같은 방식으로 함수를 만들었다고 가정해 봅시다. 함수 생성이 성공하면 다음과 같은 화면이 나옵니다. 아래와 같이 Layers를 누르고 계층 추가를 눌러줍시다. 계층 추가를 진행시 다음과 같이 ‘런타임 호환’에서 선택한 뒤 방금 만들어준 Layer의 이름 + 버전(첫 버전이라 1)을 선택하고 연결을 눌러줍시다. Layer를 추가한 뒤에는 Console의 우측 상단의 저장 버튼을 눌러야만 Lambda Function이 저장됩니다. 이제 Lambda 함수 코드를 수정해봅시다. 기존에는 아래와 같은 샘플 코드가 들어있습니다. 12345678import jsondef lambda_handler(event, context): # TODO implement return { 'statusCode': 200, 'body': json.dumps('Hello from Lambda!') } 당연히 실행은 잘 되지만, 우리가 원하는 코드는 위에서 사용한 크롤링 코드입니다. 하지만 아래 버전은 제대로 동작하지 않습니다. No module ~~이라고 하는 에러가 발생하게 되죠. 파이썬 모듈들을 import 해줍시다. 12345678910111213import jsonimport requestsfrom bs4 import BeautifulSoup as bsdef lambda_handler(event, context): # TODO implement res = requests.get('https://beomi.github.io') soup = bs(res.text, 'html.parser') blog_title = soup.select_one('h1').text return { 'statusCode': 200, 'body': json.dumps(blog_title) } 파이썬은 기본적으로 현재 폴더, 그리고 실행하는 파이썬이 참고하는 PYTHON PATH들을 참고해 여러 패키지와 라이브러리를 import합니다.Lambda Layers가 압축 해제된 /opt폴더는 해당 PATH에 들어있지 않아 import할 때 Python이 탐색하는 대상에 포함되지 않습니다.대신, 우리가 방금 다운받은 패키지지 안의 python 폴더가 /opt/python에 압축이 해제되고 해당 폴더는 PYTHONPATH 환경변수 내에 포함되게 됩니다. 이제 다시 더미 테스트를 실행해보면 다음과 같이 결과가 잘 나오는 것을 볼 수 있습니다. 맺으며이번 글에서는 굉장히 가벼운 패키지들만 사용했지만 당장 selenium을 이용하기 위해 PhantomJS 바이너리를 포함하는 경우 총 패키지 크기가 13MB를 넘어가게 됩니다.또한 AWS Lambda의 ‘250MB’ 크기 제약은 여전히 “한 함수의 소스코드 크기 + Layers 크기 합”으로 되어있기 때문에 Layer를 쪼개더라도 총 합이 ‘250MB’로 걸린다는 점은 아쉽습니다. (얼른 용량을 늘려라 AWS 일해라 AWS) 다만 일상적인 수정이 필요한 경우, 그리고 Proof of concept 같은 상황에서 Lambda 환경을 테스트하기 위해서는 이 글에서 소개한 AWS Lambda Layers를 적극 활용해 보는 것이 좋을 것 같습니다. :)","link":"/2018/11/30/using-aws-lambda-layers-on-python3/"},{"title":"뉴스 댓글 분석 프로젝트[0]: 프로젝트를 시작하며","text":"온라인 뉴스 댓글은 정말 사람들의 목소리일까? “여기 오신 모든 분들 손을 들어주세요.” (모두 손을 든다) “나는 인터넷 뉴스를 보지 않는다, 하시는 분 손 내려주세요.” (조금 손을 내린다) “나는 인터넷 뉴스를 보고 댓글은 보지 않는다, 하시는 분 손 내려주세요.” (좀 더 손을 내린다) “아직도 반이 넘는 많은 분들이 손을 들어주시고 계시네요. 이제 손 내려주셔도 됩니다. 감사합니다.” – 2018 PyConKR 발표를 시작하며 작년(2018) PyCon에서 “온라인 뉴스 댓글은 정말 사람들의 목소리일까?“ 라는 주제로 비공개 세션을 진행했다. 해당 발표의 CFP를 제출했을 때 사실 ‘딥러닝 일베 탐지기’와 ‘네이버 뉴스 댓글 분석’ 이라는 두 가지 주제 중 한 가지를 선택해 진행할 것이라고 생각했지만, CFP에 붙었던 주제가 네이버 댓글 분석이어서 급하게 해당 프로젝트로 선회하게 되었다. 발표가 나고 2~3개월동안 해도 괜찮은걸까, 할 수 있을까 하는 수많은 고민을 하다 결국은 진행하기로 했다. 무엇을 했나? - 데이터 수집편 1) 데이터 수집하기처음 분석시에는 네이버에 있는 뉴스를 가져와 모든 것을 분석해 보기로 했지만, 현실의 장벽에 부딪혔다. 총 기사의 개수가 억단위를 넘어가는 상황에서 해당 기사들을 모두 긁어오는 것은 현실적인 무리가 있다고 판단해 기간별 샘플링을 하는 방법을 통해 개수를 많이 줄여서 데이터 수집 부담을 줄였다. 2) 서버리스 크롤링실제로 데이터를 가져오는 과정에서 AWS Lambda 서비스를 이용한 서버리스 크롤링을 통해 동시에 수백개+ 정도의 요청을 보내는 방식을 통해 보다 빠르게 데이터를 수집했다. 데이터를 저장하는 소스는 AWS S3를 이용해 Lambda에서 boto3 패키지를 이용해 보다 쉽게 업로드 할 수 있도록 구성했다. 그리고 실제로 람다 함수를 activate하는 boto3를 이용한 로컬 async 기반 코드를 실행해 아래와 같이 서버리스 크롤링이 오토스케일링으로 자동으로 다량으로 올라갈 수 있도록 환경을 구성해보았다. 3) 데이터 용량 줄이기하지만 데이터를 모으는 것 만이 끝이 아니다. 원본 데이터는 *.json 파일로, 용량이 상대적으로 큰 상태였다. 따라서 이 용량을 줄이기 위해 parquet 파일 형식으로 변환해 저장하는 방식을 사용하면 용량과 파일 개수를 획기적으로 줄일 수 있다. 이때 파일 개수는 PySpark Dataframe의 partition 개수에 의존한다. 즉 Repartitioning을 통해 파티션 개수를 1개로 줄이면 parquet파일도 1개로 나오고, 100개로 설정하면 파일이 100개로 쪼개진다. 파일 개수가 적으면 Sequential I/O가 이루어지기 때문에 속도면에서 조금 유리하지만, 실제로 PySpark가 클러스터에 작업을 분배할 때 파티션 개수에 따라 Job distribution이 이뤄지기 때문에 이후 작업시 몇 개의 Worker Node를 이용할지에 대한 생각을 한 뒤 적정한 수로 나누는 것이 좋다. 또한 연도/월/일과 같이 기간별 탐색이 자주 있는 경우 파일 단계에서 필터링을 걸어줄 수 있기 때문에 기간별로 다른 파일을 만들어 저장하는 것도 좋은 방법이다. 실제로 파일을 변환한 경우 아래와 같이 용량의 87% 감량이 이뤄진 것을 볼 수 있다. Parquet 파일 자체가 유리하다기 보다는 압축되지 않는 json파일 vs Snappy 압축이 이뤄진 parquet파일 간 비교이기 때문에 압축의 유무가 좀 더 큰 영향을 미친다. 다만 json파일 각각을 gzip등을 이용해 저장할 경우 이후 수천~수만개의 파일을 읽을때 해당 파일을 list/get하는 비용만으로도 AWS S3 비용이 상당히 올라가게 된다. (S3는 파일 리스팅/파일 GET요청 횟수별로 비용이 매겨진다.) 4) 데이터 분석환경위와 같이 parquet 파일로 정제한 뒤에는 데이터 분석을 하기가 좀 더 쉬워진다. 그리고 해당 파일을 읽어서 데이터 분석을 할 수 있는 환경은 크게 Pandas를 이용한 로컬 환경과 PySpark를 올린 AWS EMR환경을 이용했다. 이후 어떻게 진행되고 있나?2018년 파이콘 발표를 끝낸 뒤, 데이터 전체를 찾는 방식에서 매일 데이터 분석 모델을 돌려보고 있다. 이에 따라 다음 글에서는 무슨 데이터를 어떻게 모으고 있는지, 그리고 어떻게 데이터를 적재하고 ETL작업을 하는지 등에 대해 알아본다.","link":"/2019/06/24/alba300-0/"},{"title":"DevGround 2019, 뭣이 중헌디? and 데이터야 흘러라","text":"DevGround 2019를 다녀오다오늘 DevGround 2019 행사에 다녀왔다. 다양한 머신러닝과 관련된 연사분들의 고충(!)을 듣는, 일부는 공감되기도 하는 시간이었다. 이번 행사를 두 문장으로 요약하면, “뭣이 중헌디?” &amp; “데이터야 흘러라!” 였다. 회사의 ML Project가 지향해야하는 방향성이 회사의 핵심과 연결되어야 한다는 것이 첫번째. 그리고 우리가 머신러닝 혹은 딥러닝을 이용하고 보다 높은 성능의 모델을 만들기 위해 노력하지만 실제 현실의 회사들에서 겪는 문제는 모델의 성능이 아니라 데이터가 흐르는 환경을 만드는 것이 더 큰 문제라는 것을 다시한번 느꼈다. 이하 내용은 세션 들으며 노트테이킹한 내용들. Session 01. 데이터와 머신러닝이 비즈니스와 만날 때 발생할 수 있는 비극들 (by 하용호님)Data =&gt; Pattern =&gt; Money! 개발을 하고 =&gt; 통계를 낸 뒤 =&gt; 비즈니스를 하자! 2012년에 DataScience Boom이 일어났지만 2018년에는 이걸로 무엇을 할 수 있나?에 대한 의문이 제기되고 있다. 한편 회사들에서는 AI니 ML이니 빅데이터니 하는 이야기들이 나오고 있어 어라, 이걸 회사에 도입해보면 어떨까? 하는 생각을 하게되지만 실제로 Data를 이용해 일을 해 본 경험이 없기 때문에 어떻게 해야하는지 자체에 대한 인식이 낮은 상태다. C급이든 실무자든 모두 기술적 미신에만 빠져있는 상황이라고 볼 수도 있는 상태인 것. 따라서 ‘빅 데이터’ 혹은 ‘데이터 비즈니스’ 와 같은 공허한 키워드들만을 사용하게 되는 것. “구슬이 서말이어도 꿰어야 보배다.”1) 구슬이 “서말” =&gt; “데이터 량이 많아야…” “우리 회사에는 데이터가 많아요!” == ‘데이터 없어요!’ or ‘(쓸 수 없는) 데이터가 많아요!’ 하지만 실제 현실의 데이터는 1. 없거나 2. 쓰레기더미거나. 사장들의 환상을 이루기 위해서는 (엄청난) 데이터량과/컴퓨팅서버와/엔지니어가 모두 필요하지만,정작 그걸 가지고 “뭘 할거야?” 라고 묻는 경우 ‘추천? 광고?’ 라는 이야기를 한다. 한편 추천/광고 등으로 재미를 보려면 MAU가 20만 이상이거나 앱 기준 Download 1천만 이상이 되어야 하고, 무엇보다 실제로 해당 분야의 업종(커머스-쿠팡/아마존 등 or 광고수입-구글 등)이거나 혹은 유저가 엄청 많거나(Netflix)의 조건이 충족되어야 유의미한 결과물을 도출해 낼 수 있다. 따라서 보다 현재(재직중인) 회사의 본질과 연결해서 일을 해야 유의미한 결과가 나온다. 예를들어 광고/추천 서비스의 경우 유저의 결정을 빠르게 해 빠른 Transaction으로 이어지게 하는 것이 목표이지만, 정말 이게 목표라면 서비스 초창기에는 그저 인기순위 Top10만으로도 충분히 위 목표를 성취할 수 있다. 오히려 결제 UX Flow를 원활히 만들어 결제 과정에서 이탈이 발생하지 않게 만드는 것이 더 중요한 것. 결국 ROI를 따질 때 들어가는 노력 대비 결과가 더 높고, ‘쉬운 방법’을 모두 선택한 뒤 그 다음에 복잡한 기법(ML등)으로 넘어가는 것이 효율적이라는 이야기. 2) 구슬을 “꿰어야” =&gt; “인력이 필요하다…”문제는 관련 일을 하는 인력이 ‘비싸다’ 는 점. 일을 잘 하는 ML Engineer는 6k ~ 10k+의 연봉을 받고 다니고, 또 동시에 분야가 데이터와 관련이 적은 현재의 대기업들에서는 정작 들어가서는 메리트가 없기 때문에 잘 가지 않으려고 하는 편이라는 이야기. ‘좋은 엔지니어’ 들은 회사의 비즈니스 축에 속하지 않으면 재미없어하고 흥미와 동기가 DOWN. 한편 회사에서 기술을 사용하는 이유는 이익을 만들어 내기 위해서인데 이러한 기본적인 목적을 잃으면 안된다는 이야기. 3) 원하는 “보배”가 뭔지 모른다 “ML을 쓰고싶다. 어디에 쓰지?” (X) “회사에 지금 이런 문제가 있는데, 여기에 ML을 써보면 효율적으로 바뀌지 않을까?” (O) 전자의 경우는 새로운 기술을 이용해 새로운 비즈니스를 만들려고 한다는 점에서 문제가 생긴다. 회사의 전체 Value의 상승에 도움이 되지 않는다. 공들여서 새로운 비즈니스를 만들어도 오래걸리고 impact도 작기 때문. (회사가 현재 100이라면 신사업은 5~10인데 이걸 2,3배 올려도 10-30에 불과해 %는 높지만 실제 영향은 작음) 따라서 제대로 된 목표는 회사의 메인 비즈니스 체인 중 ‘느린 부분’을 찾고, 이를 ML등을 이용해 자동화 하는 등의 방법을 통해 효율을 높이는 것이 가장 유의미한 목표가 된다. Q. 그렇다면 메인 비즈니스 체인 중 ‘느린 부분’은 어떻게 찾나? 어떤 일을 한다고 할 때 진행되는 작업을 개별적인 Transaction이라고 가정하고 시간단위로 재어서 실제로 시간이 오래 걸리는 병목을 찾는 것. (느린 부분은 실제로 시간이 오래 걸리고, 이 부분은 사람의 손이 가는 부분일 가능성이 매우 높다.) 복잡해 보이는 것만이 답이 아니다 “If all you have is a hammer, everything looks like a nail” – Law of the instrument 현실에 있는 문제들은 복잡하고 고도화된 기법을 통해서만 답을 해결할 수 있는 것이 아니다. 오히려 기업에서는 ROI 계획이 필요하고, 기회비용을 고려해야 한다. 즉, 간단한 휴리스틱 혹은 룰베이스 기반으로 하는 처리방법이 비록 효율이 낮더라도 도입이 빠르고 어느정도의 성과가 나오는 것이 더 낫다는 이야기. 그러면 언제 ML등이 필요한가? 상품 100개를 정렬해보자. 어떻게 하지? (사람 손으로 OK) 그러면 상품 100만개를 정렬해보자. 어떻게 하지? (사람 손으로 X) 휴먼리소스로 커버가 되지 않는 순간이 오면 그때는 이런 기법들을 적극적으로 도입해야 한다! 카카오 플러스친구 이야기 시안 1, 2, 3이 있는데요, 어떤게 가장 좋을까요? AS-IS: “음… 시안 3이 좋을 것 같으니 이것으로 발송하자.” TO-BE: “1,2,3 모두 보내보고 AB테스트를 한 뒤 CTR이 가장 높은 것으로 발송하자.” 10000만명에게 보낸다면 앞선 1천명에게는 섞어서 보낸 뒤 CTR을 확인해 보고 가장 CTR이 높은 시안을 자동으로 선택해 나머지 9천명에게 발송하는 아이디어. 실제 카카오의 Value chain으로부터 실제 Viable product까지 이어진 프로젝트. 따라서 쓸 기술을 찾는 것보다 쓸 분야를 찾는 것이 더 중요하다. Session 02. AI 프로젝트 간지나게 잘 진행하는 법 (by 백정상님) “AI Project를 간지나게. 무엇이 프로젝트를 간지나게 만들까요?” 좋은 팀 풀려는 문제의 사이즈와 임팩트가 큰 것 한편 실패하는 머신러닝 프로젝트는? 회사의 코어 비즈니스가 아닌 딴 사업 낮은 데이터 품질 (로그를 찍는데 시간을 안써요) 딥러닝만이 답이 아니다! 통계적 ML도 유의미함 확증편향: 이거로 하면 잘 될거야! (행복회로 화르륵) 부족한 인프라 - GPU 500대만 주세요 등등.. 수많은 실패하는 원인. 그렇다면 어떤 프로젝트를 시작해야 할까? “투자 대비 10배가 나오는 프로젝트” 나쁜 계획: ​ 팀 = 1인 + 1PC + 3개월 ​ -&gt; 현실적으로 불가능함! 현실적 계획: ​ 팀 = PM + BM + DS + ML 이렇게 최소 4명 여기서 저 4명의 파티를 어떻게 잘 모으느냐가 가장 큰 관건 그런데- S급 인재 4명 월급은 4k, AWS비용은..? + 기간은 1년 =&gt; 8억! x10배 원칙 하면 80억! “어떤 ML 프로젝트를 해야 80억의 가치를 가져올 수 있을까?” 어떤 프로젝트를 해야 80억의 가치를 가져올 수 있나 + 회사의 메인 비즈니스 체인에서 ML을 이용해 어떤 개선을 만들 것인가? 맥킨지 등의 Importance factor Business factor =&gt; 현재 회사의 상태에 맞게 적용할 수 있는 방법론 또한 현재 회사들에서 다루는 대부분의 데이터는 Structured Data(1위) 그리고 데이터를 EDA하는 과정과 Corelation, 그리고 통계적 검증(빈도/타당도/z스코어/t스코어 등)을 하는 것 자체만으로도 얻는 결과가 크다. 그리고 만약 ML Project를 시작한다면 AutoML 혹은 Managed ML 서비스에서 돌려보는 것을 추천한다. 도메인 지식에 따라 Feature selection이 이루어질 수 있는데, 해당 부분은 우선 전체를 넣어보는 것이 일부만 골라 넣는 것보다 낫다. 그리고 딥러닝 모델을 만드는 데 있어 평가를 하는 Metric을 공유하는 것이 매우 중요함. “어떤 것을 높여야 우리가 원하는 모델인가?” 그럼에도 비즈니스 impact를 세상이 주려면… 어려움 ㅠㅠ Session 03. 온라인 게임 데이터 분석 사례와 향후 과제 (by 이은조님) “무엇이 (ML모델 개발의) 불쾌한 골짜기를 만드는가?” ML모델을 만드는 과정 중, 간단한 모델을 만들어 어느정도 성과가 나오다가 일정 시기가 지나면 들이는 노력에 비해 굉장히 낮은 성과가 나오기 시작한다. 이 구간을 불쾌한 골짜기에 비유. 특히 Data가 시간이 지나면서 자체적으로 변화하기 때문에(게임의 경우 게임 업데이트 등) 기존의 데이터로 학습한 ML 모델이 제대로 동작하지 않는 경우가 많다. =&gt; “Concept Drift” Academic issue를 연구할 때는 위와 같은 Concept drift를 경험하는 경우가 적으나, 실제 현실에서는 굉장히 많이 자주 접하게 되는 부분이고, Train data와 Live data와의 간극이 커지면서 Feature 자체가 바뀌게 된다. 따라서 이에 대한 해결책으로 Robust modeling을 선택해 불변하는 Feature를 지정해 모델을 만들 수도 있다. 하지만 심각한 문제는 그런 Feature로 만든 모델은 굉장히 Naive할 수 밖에 없다는 한계가 있다. 즉 모델을 실제로 배포할 때는 LiveData의 성능을 주기적으로 측정하는 것을 통해 모델의 성능을 감시하는 것이 필요하다. 물론 Online Learning같은 방식을 도입하는 것도 방법. 그리고 데이터와 모델 성능을 측정하는 것에 있어 Citizen data scientist와 같은 사람들의 참여가 있으면 좋다! 또한 정확도를 높인다 라기 보다는… ‘비용 지출’의 관점에서 볼 수도 있다. 전체 고객을 대상으로 이탈 예측 모델링을 돌리면 이미 이탈한 유저가 포함되어있기 때문에 마케팅 비용이 오히려 클 수 있지만, 충성고객만을 대상으로 한 경우는 기대이익을 높일 수 있다. 여기서 질문: “기대 이익 자체를 Loss Function으로 만들 수는 없을까?” Session 04. 한국어 인공지능 콜센터 (by 맹윤호님)IBM 왓슨 이용한 방식. 한국어는 원래 지원하지는 않지만 Custom으로 추가한 MVP(Minimal Viable Product)수준의 개발! 이슈: 옛날에는 이미지 or 버튼 선택의 챗봇을 했지만 이제는 ‘문장’단위의 자연어 처리하는 챗봇. 새로운 챗봇은 이제 사람이 말하는 걸 듣고 답해주는 콜센터가 되어야 한다. 한편 STT 데이터들은 8Khz(Narrow band)와 16Khz(Broad Band)가 있는데 현실에서는 데이터가 뒤죽박죽이고 양도 많지도 않음. 따라서 새로운 데이터를 만들어야 하는데… 음성을 듣고 텍스트로 받아적기: 가장 좋은 접근방법이고 현실적 데이터를 얻을 수 있다. 텍스트를 ‘읽어’ 음성 만들기: 빠른 데이터 생성이 가능함. 하지만 비현실적임 데이터가 없으면 QnA기반에서 만들어주거나 혹은 텍스트기반 고객센터 내용을 가져와 사용하기도 한다. 한편 STT 학습은 Lightly Supervisored Learning을 사용해 TimeStamp 없이도 학습! (LanguageModel 메뉴에서 지원함) 하지만 학습한 모델을 평가할 때는 TimeStamp된 데이터가 있어야 평가가 가능하다. Chatbot이라는 측면에서 분기를 탄 뒤 Multilayer custom model을 만들어서 상황에 최적화된 Utterance별로 다른 모델을 사용하도록 만들게 된다. 전화를 사용하는 Chatbot이니 twilio등의 서비스를 이용해 가상 전화번호를 받아와 사용할 수 있다. +사람들은 AI에 존댓말을 사용하지 않는다. 한편 train data는 존댓말 기반으로 이뤄지기 때문에 Train data와 live data와의 괴리가 생기게 된다. +짧은 단어만을 사용하는 것도 이슈. 적은 양의 정보만으로 이야기 하는 상담 대화의 데이터가 필요하나 현재는… (ㅠㅠ) Session 05. 딥러닝과 자동차 (by 조국현님)말 그대로 “최적화” 이야기. SIMD(AVX 등), SIMT(CUDA등)…. (사실 잘 모르는 내용이라 이해를 많이 못했다 ㅠㅠ 다만 최적화를 엄청 빡세게 한다는 것은 느낌.) ##Session 06. MOBILITY X DATA : 모빌리티 산업의 도전 과제 (by 변성윤님) Google OR Tools DeepST -&gt; CNN 기반으로 수요예측하는 모델 SimPy: 시뮬레이터! Route Planning: 어떤 길로 가지? Map Matching: GPS를 도로 위로! (차가 바다 위에 떠있지는 않을테니까) 그 외에도 모빌리티에서 사용하는 다양한 사례. 그 주에 타다를 타고 배차를 하는 이야기를 들었는데 실제 현실에서 적용되는 케이스가 굉장히 합리적이고 잘 설계되었다는 느낌을 받았다. Session 07. 린하게 구축하는 스타트업 데이터 파이프라인 (by 박재영님)DataLake &amp; Data ETL &amp; Data WarehouseDataLakeKInesis(Firehorse) -&gt; S3 &amp; ElasticSearch Hot Data -&gt; Redis 실시간 데이터 분석은 RDS Replica DB로 Custom Endpoint (메인 DB에 영향 거의 주지 않음) Data ETLBefore: Single EC2 + Jupyter Now: AWS EMR Task관리는 Zeppelin + EMR AutoScaling // DAG관리에서 Airflow를 사용중은 아님. Data Warehouse &amp; BIRedShift + 태블루(좋지만 비쌈…) =&gt; 대신 AWS QuickSight! (but 개발자를 위한 서비스) =&gt; MS Power BI (엑셀과 비슷해 비개발자도 쓰기 편함) Session 08. AI의 스타크래프트 도전기 (by 배창현님)분명히 빡세고 어렵다. 하지만 RL로 해보자는 도전! -&gt; Facebook쪽 팀도 이김! 1등! 벌쳐의 아주 강력한 카이팅(신컨)","link":"/2019/06/27/DevGround/"},{"title":"DLCAT#2 참석 후기 & 정리","text":"DLCAT#2를 다녀오다.대전 UST와 ETRI에서 열린 DLCAT행사를 다녀왔다. 모두 ML/DL에 관련한 이야기로 한가득…(굉장히 듣는 즐거움이 있었다.) (아래 이미지는 DLCAT에서 들은 세션들) 제목만 보고 잘못 들어갔지만 새로운 인사이트를 얻은 세션도 있었고, 현실적인 이야기를 하는 경우와 사업적인 이야기를 하는 경우, 그리고 논문을 수십편을 40분만에 살펴보는 어마무시한 세션도 있었다. 하지만 어느쪽이든 대부분 굉장히 재미있었던 편. Session 01. 딥러닝과 최적설계첫번째 세션이 사실 ‘딥러닝을 최적화하는 것’으로 오해해 들어갔던 세션이다. 실제로는 최적 설계(Topology Optization)을 하는 딥러닝을 이용해 만들어내는 과정을 담은 세션이었다. 차체 설계 등 기계를 어떻게 해야 최적으로, 즉 적은 양의 자원을 사용해 최고의 효율을 뽑아내는 가에 대한 연구. 그리고 이러한 연구는 수식을 통한 최적화가 있고 Global Optima값이 존재하기 때문에 컴퓨팅 리소스와 시간만 충분하다면 최적해를 찾을 수 있는 문제라고 한다. (정답이 있는 문제) 하지만 여러가지 문제가… 시간이 오래걸린다. (2D는 빠르게 되지만, 3D는 정말 느리다고 함) 최적화를 하려면 일단 수식을 만들어야 하는데 수식 formulation자체가 어려움 10년전에는 최적 설계가 나와도 만들수가 없었음(CNC로 깎는 것도 한계가 있음) 사람이 보기에 안예쁨 이와 같이 굉장히 다양한 문제가 있었지만, 최근의 발전으로는 여러가지를 해결할 수가 있다고 한다. 특히 3번의 경우에는 3DPrint등을 통해 웬만큼 이상한 모양은 구현이 가능해졌다고 한다. 최적 설계 = Input -&gt; f(x) -&gt; output이곳에서 f(x)를 수식Iteration이 아니라 DeepLearning을 써보자! 처음으로 꺼내본 방법은 AutoEncoder 방식을 이용한 Feature 탐색. 설계의 요소 여러가지를 이미지로 변환한 뒤 CNN &amp; GAN등의 기법을 이용해 모델링. 결론은 “잘 된다!” 하지만 잘 되기는 하지만… DeepLearning으로 하는 모델링의 단점도 있다. Near Solution을 찾기는 하지만 Global Optimal solution은 아님. Domain specific issue 1.을 해결하는 방법은 하이브리드식 접근법(혹은 GreyBox)을 통해 DL Model로 근접해를 찾고 이후에 기존의 수식 iter 방식을 통해 최적해를 찾는 방법. 실제로 Convergence까지 속도가 4배 빨라진다고 한다. (DL결과 후 수식 iter이고 수식은 언제나 최적해를 찾기 때문에 Optimal solution이라는 것이 보장됨) 그리고 Video 등에서 다음 frame을 예측하는 방법을 수식의 next iteration을 예측하는 것처럼 응용할 수도 있다고 한다. 결국 DL모델링은 Topology Optimization을 가속해주는 역할. 한편 이런 방식은 AutoDesk와 같은 모델링 프로그램에서 이미 지원하고 있다고 한다. 오토데스크에서 작업을 해나가면 그걸 기반으로 학습을 한다고 한다. (정확히 어떤 것인지 모르겠지만 일종의 FineTuning일까?) 그리고 위의 방법 외에도 RL등을 이용해 해를 찾는 방법도 기획중이라고 한다.(아직까지는 아름다운 결과로 나오지는 않았다고 함) 이것은 ‘과연 수학회가 Real Optima인가? 혹은 다른 새로운 방법은 없을까?’하는 의문에서 시작했다고 한다. 한편 이런 DL 방법들에서 나오는 결과물들은 고객들을 취향과 갬성적 측면을 고려하지 않고 있기 때문에 문제가 있다고 한다. (이것도 모 랩에서 연구중이라고..) 그리고 최적화 외에 다른 것에서 2D 모델링에서 유의미한 결과가 나온 것이 Super-Res등의 방법과 같이 X선 피폭량 제한때문에 Low-res로 나오지만 실제로는 더 높은 해상도가 필요한 골다공증 등에 해당 기법을 도입했다고 함(뼈는 최적설계로 생성되기 때문에, 단순한 Weifu같은 해상도 증폭이 아니라 해당 최적 설계를 예측하는 방식으로 진행할 수 있는 분야라고 한다. 이 외에 여러가지 분야에서 사용한다고 한다. 결론: ML은 귀납적 방법론 중 하나. || 기존의 Formula vs 사람의 직관 vs DataDriven 의 문제다. Session 02. 나도너도 모르는 GraphNN의 힘최신 SoTA를 경신해대는 Transformer등등이 있지만, 2019CVPR을 살펴보면 ‘Graph’라는 키워드의 비중이 커지고 있는 것을 볼 수 있다. “Attention, Transformer 같은거로 최고의 결과들을 다 보고 나니까 다른 분야로 살펴보기 시작하는거에요.” “졸업하기 위해서는 블루오션인 GraphNN을 팝시다!!” – 모 세션을 진행한 모 발표자님 GraphNN은 뭐지? 왜쓰지?Node Feature들을 통해 뉴럴넷을 학습시키는 방법. Graph형태 데이터를 classification하는 것에 사용할 수 있다고 한다. (그래서 Social Network Anslysis등에 많이 사용한다고 함) 해당 뉴럴넷에서 사용하는 행렬은 두개: Node Feature Matrix: 각 Vertex에 지정된 feature set의 vector들을 모은 행렬 Adjacency Matrix: Vertex간 연결을 보여주는 행렬 그래서 만들어지는 두 행렬을 이용해서 연산을 진행한다. GraphNN에서 사용하는 연산의 진행은 아래 세 가지 방법. Aggregate: 중심 Node로 인접 node들의 feature vector를 모으기 Combine: 가져온 벡터들을 가져와 중앙 node의 feature vector를 업데이트 ReadOut: Graph를 하나의 벡터로 뽑는 것 (Representative vector를 뽑는 것일까?) 한편 1. Aggregate와 2. Combine은 논문마다 진행하는 방법이 다른 상태라고 한다. GraphNN에서의 layer는 중심 Node에서 얼마나 많이 떨어진 node들에서까지 정보를 가져와 업데이트 하냐는 의미. 아래와 같이 바로 옆 Node들에서만 가져온다고 하면 layer = 1 이 된다. (이건 Agg/Combine 단계에 영향) 한편 가장 중앙인 Node 외 다른 Node들도 1 layer(바로 옆 노드)를 통해 자기자신의 Feature Vector를 업데이트하기 때문에, Iteration이 반복됨에 따라 가장 외부의 정보도 서로서로 멀리까지 퍼져나가게 된다고 한다. 이런 Iter가 끝나고 나서 Vector Mean Pooling을 통해 Conv Vector를 추출해 낼 수 있다고 한다. (이게 결국 해당 network의 representative vector가 되는 것이겠지?) Graph에서의 Convolution은 CNN의 이미지에서의 Convolution와는 다르게, 로컬한 정보를 본다 라는 컨셉과 weight를 공유한다 라는 컨셉에서의 conv라고 한다. (앞의 Conv vector가 이런 의미) 만약 node adjacency matrix에 0,1이 아니라 weight가 있다면 Attention coefficient 통해 각각 곱해 Linear projection을 하는 셈이 된다고 함. 이러한 GNN을 적용하는 분야중 하나가 VQA로 image classification + QA modeling을 하는 분야이고 이미지에 나와있는 Relation을 측정하는 방법에서 GNN을 사용한다고 한다. (ReGAT paper) Session 03. 딥러닝 모델 엑기스 추출 (Knowledge Distillation) 모델이 커지면 커질수록 성능이 좋아진다. 그래서 Big Model로 가고 Ensenble도 한다. 하지만 실제 Edge device 혹은 Realtime service에서는 latency도 훨씬 중요하다. 그렇다면 큰 모델을 어떻게 작고 빠르게 만들 수 있을까? 딥러닝 모델을 압축하는 방법에는 여러가지 방법이 있지만(사실 이후 EfficientNet세선에서 이부분을 좀 더 다뤘다.) 크게 두 가지를 사용한다. Pruning: 중요하지 않은 node들을 제거하기 Quantization: fp32 -&gt; fp16등으로 줄이기 bigLITTLE Net: 2개의 뉴럴넷을 이용하기 하지만 이 외에도 Knowledge Distillation이라는 방법도 있다. 이때 Distillation은 Teacher - Student 방식을 이용해 BigNN -&gt; SmallNN으로 학습한 것을 전달해주는 것이 SmallNN 혼자 (원천 데이터에서) 학습하는 것보다 더 높은 성능(더 높은 Generalization)을 갖게 된다. 이와 관련된 수많은 Papers논문의 제목에 논문 아카이브 혹은 PDF 링크를 붙여둠. 쿠팡 기사가 전화가 와서 중간에 일부 못들은 논문이 있다. 아쉽다 ㅠㅠ Do Deep Nets Really Need to be Deep? (NIPS 2014)TeacherNN인 거대한 BigNN을 학습한 뒤 Freezing을 거치고 나오는 logits(softmax취한 결과 아님. Logits 숫자임.)을 이용해 StudentNN인 smallNN을 학습시켜 해당 logits와의 L2 Loss를 줄이는 방식으로 학습을 진행. Softmax 대신 Logits 쓰는 이유: softmax는 Model의 과도한 확신이 들어가 BigNN의 Generalization을 학습하지 못함. 따라서 logits를 이용해 다른 것일 ‘가능성’, 즉 BigNN의 Generalize를 배우도록 만든다. Distilling the Knowledge in a Neural Network (NIPS 2014) 최초로 Knowledge Distillation이라는 용어를 다룬 논문 (Jeff Dean이 쓴 논문!) Softmax target function과 Soft target을 만들어서 기존의 softmax가 한 label에 peak를 치는 대신 아래 핑크색 그래프처럼 조금 Generalize된 그래프를 그리도록 완화한 것. 비슷한 방식으로 StudentNN이 TeacherNN을 배우도록 하지만 Loss로는 KL Divergence를 이용함. FitNets: Hints for Thin Deep Nets (ICLR 2015)모델의 결과값인 logits/softmax value만 사용하는 대신 중간 layer들의 값도 사용하자는 이야기. TeacherNN이 큰 filter를 가지고 있는 반면 StudentNN은 작은 filter를 가지고 있지만 더 깊은 뉴럴넷. Layer마다 output을 hint로 사용해 Teacher/Student NN을 비교하고, 이때 StudnetNN이 더 필터가 작으므로 TeacherNN의 필터 사이즈에 맞추기 위해 Regresser를 사용함 A Gift from Knowledge Distillation (CVPR 2017)Paying More Attention to AttentionTeacher-Student 학습을 AttentionMap을 전달해주는 방법으로 진행. Distillation loss를 같이 쓰면 더 성능이 높게 나온다고 한다. 이때부터 ImageNet을 이용한 Validation이 시작되었다고 함 Paraphrasing Complex Network (NIPS 2018)Born Again Neural Networks (ICML 2018)원천데이터 X에서 가져오는 것 뿐만 아니라 Teacher로 가르친 S1, 그리고 S1에서 배우는 S2, ….이런 방식으로 k번 반복해 Student model들을 ensenble해서 최종 모델로 만드는 방법. Teacher보다 Student Ensenble이 더 성능이 좋게 나온다고 한다. Network Recasting (AAAI 2019)TeacherNN와 StudnetNN의 Layer 별 MSE Loss를 통해 트레이닝 Relational Knowledge Distillation (CVPR 2019)단순히 distance만 계산하는 것 뿐만 아니라 Cosine값을 이용해 angle의 차이도 이용함 결론Teacher -&gt; Student NN 방식 학습은 강력한 Regularizer로서의 역할을 담당한다. Few Shot Learning, Meta Leraning등의 여러가지 방법이 있지만 Knowledge Distillation은 충분히 유의미한 성능을 뽑아내는 중. QnAQ1. 이러한 Knowledge Distillation 방법론이 성능 향상을 위한 것인지 아니면 모델 압축을 위한 것인지? A1. Use case마다 다를 듯 Q2. FitNets는 TeacherNN &lt; StudentNN으로 성능이 더 좋음. 검증이 되나? A2. 이때까지는 테스트데이터가 CIFAR100이라 ImageNet으로 한 후속논문과 비교는 어려움 Q3. Relational Knowldege Distillation논문에서 Feature를 128차원으로 하고 Distance는 euclidian distance를 이용했는데 차원의 저주 이슈는 없나? A3. 아마 TradeOff일 듯. Q4. paper들이 CV만 다루는데, Sequential Data(ex: NLP)등에서는 안쓰나? A4. 있는것으로 알지만 정확한 논문 이름은 바로 알지는 못함 Q5. StudentNN같이 Knowledge Distillation을 통해 학습하면 학습 속도(아마도 Convergence속도?)가 더 빠른가? A5. 당연히 더 느리지만 Knowledge Distillation에서 학습 속도는 코어 이슈가 아님. Session 04. 딥레마, 딥러닝을 서비스로 할때의 딜레마사실 이 세션은 점심 직후라 피로가 쌓여 완전 집중해서 듣지는 못했다 ㅠㅠ 대체로 DevGround에서 들었던 이야기과 비슷한 내용을 좀 더 ‘일반인’ 대상으로 이야기 하던 세션. Session 05. EfficientNet 톺아보기(라고 쓰고 논문읽기) “핵심만 보자!” CNN은 이제 성능 올리기보다는 효율을 높이는 방향으로 나아가고 있는 중. 최근에 나온 BERT와 XLNet의 경우 340million의 파라미터를 학습, 512개의 TPUv3 x 500k steps로 학습해 2.5일 걸려서 대충 2억정도 걸려서 학습했다고 한다. (진짜로 ‘억’소리나는 금액) NAS의 초기모델도 800GPU x 1month 걸렸다고 하는 등 ‘요즘 모델’들은 너무 크고 무거워지는 중. 그래서 최신의 이슈는 OnDevice &amp; Offline등 Edge에서 이뤄지는 딥러닝 학습/추론의 이슈가 커지고 있다고 한다. 물론 작은 모델을 만들기 위해 Pruning을 하거나 Knowledge Distillation을 할수도 있지만, 모델 자체를 최적화된 설계로 하는 것도 중요하다는 이야기. CNN 등 딥러닝 모델에서 속도의 이슈는 크게 3가지. Memory Access Cost Parameters Computing Cost 이렇게 나눌 수 있는데, 각각을 줄여나가는 것이 이슈라는 것. 속도를 높이고 &amp; 가볍게 만드는 방법BottleNeck Layer &amp; Global Avg PoolingBottleNeck Layer는 1x1 conv layer로 GoogleNet에서 사용했던 방식. 그리고 Global Avg Pooling은 FC Layer의 연산을 줄여주는 방식. Filter factorization거대한 5x5 필터 1개를 -&gt; 3x3 필터 2개로 쪼개기 (행렬을 작게 쪼개기) Depthwise Seperable ConvolutionAxis따라 가로 계산 세로 계산을 따로 filter해서 계산하는 방식. MobileNet(v1-v3)에서는 inverted residual network를 이용한다.(ResNet에서는 뚱뚱하고 거대한 Layer간 Skip connection을 연결하지만, MobileNet에서는 압축한 layer를 Skip connection으로 연결해 이걸 ‘inverted’ residual이라고 부른다.) SENet에서는 중간에 Global Avg Pooling처리릍 통해 FeatureMap에 곱해주는 방식으로 Channel Attention을 통해 채널 중요도를 계산했다. Group ConvConvolution 연산의 핵심은 Local을 본다는 점이지만, 실제 Convolution연산시 모든 채널을 다 보고 있기 때문에 “왜 모든 채널을 봐야해?” 라는 질문. 따라서 Channel을 여러개로 쪼개서 Conv계산을 따로 한 뒤 합치는 방식으로 사용. ShuffleNet: 1x1 G Conv로 변경 + Channel Shuffle을 통해 학습량 감소 ResNext &amp; AlexNet에서도 사용 CondenseNet: GConv할때 어떤 채널을 가져올지 자체도 학습 FLGC: 업그레이드 된 버전 ShiftFilter 할 때, “왜 모두 ‘같은 채널’에서만 데이터를 가져와야 하나?” 채널을 상하좌우 등 1칸씩 밀고 + 채널을 1x1 conv를 통해 합친다. “사실은 Shift는 1x1로 밀고 합치는 대신 메모리 액세스 주소만 잘 바꿔주면 연산 없이 쓸수 있어요” ShiftNet All you need is a few shift (CVPR 2019)-&gt; 모든 filter 아니라 일부만 shift해도 성능 오른다 Use Direct Metric “Parameter 개수가 작아진다고 정말 속도가 빨라질까요?” 연산량이 작아진다고 해서 실제 속도가 빨라지는 것은 아니다. 물론 당연히 영향을 주지만, 실제로는 연산 자체 외에 다른 부분도 영향을 끼친다고 보아야 한다. M-Nas Net(?? 이름이 맞나..?)-&gt; 모델의 Latency를 Reward에 추가해 속도도 빠르게 하는 것 한편 앞서 말한 것과 같이 Conv를 줄인다고 속도가 무작정 빨라지지는 않는다. 오히려 Memory Access Cost가 정말 비싼 경우가 있다. ShuffleNet v2 Balanced Conv를 쓰자! G conv가 정말 답인것 같니…? Fragmentation 하지마… Element-wise 연산(덧셈 등) 하지마… 이까지가 CNN 속도 이야기! 에너지를 신경쓰는 NeuralNetAlexNet의 Parameter 수는 SqueezeNet의 50배!! 하지만 실제로 SqaueezeNet이 AlexNet보다 에너지 소모 1.2배 이상 많이 먹는다. 이유: Layer가 깊어서 Memory Access Cost가 높아서 전력 소모가 매우 높음 메모리 이동 1번 = 1000번의 곱하기더하기 연산 비용 = 엄청 비싸다! EfficientNet최신의 XLNet등 트렌드는 ‘너무 커지는’ 중. 작고 성능 좋은 NAS로 찾은 Net에서 시작해 효율적으로 성능을 높이는 좀더 큰 모델을 만드는 방법. 실제로 내가 ResNet50 -&gt; EfficientNet으로만 바꾸었는데도 성능 훨씬 좋아짐. (Convergence도 빠르다!) Session 06. 하스스톤 with 강화학습NDC에서 발표해주셨던 것과 유사한 내용! 아래 링크에서 기여를 하도록 하자! https://github.com/utilForever/RosettaStone Session 07. 이제-발표도-딥러닝이-알아서-할거에요분명 제목은 하이퍼파라미터 튜닝을 케라스 튜너로 하는거지만 부제와 제목이 뒤바뀐 것은 기분탓입니다. 아쉽게도 책 당첨은 안되었지만 그래도 ㅎ.ㅎ (혹시 케라스튜너로 PyTorch도 쓸 수 있는지 확인해 봐야겠다.) 맺으며여러 세션들이 모두 재미있었다! 비록 시간이 짧아 막 파고들지는 못했지만 이렇게 약간 메타적인 느낌으로 훑으면 나중에 논문들 챙겨보면 되는거니까. 세미나에서 들었던 논문만 읽어도 한달은 훅 지나갈 거서 같은 기분이다.","link":"/2019/07/04/DLCAT2nd/"},{"title":"V Lang 톺아보기[1]: 첫 만남","text":"V lang? V 언어? V lang의 Hello World 예제 모습 오늘 페이스북 진중님의 타임라인 게시글에 아래와 같이 V 언어에 대한 홍보(?)가 올라왔다. 컴파일도 엄청 빠르고, V 언어로 V 언어를 컴파일하는 (마치 pypy같은..!?), 게다가 Go보다 간단하고, 웹 프레임웍/ORM 내장에 동시성 처리와 패키지도 있다! -라는 말에 낚에서 우와 신기하다! 하고나서 대체 어떤 언어인지 살펴보러 가보았다. 와, 스타가 벌써 12.5k+ (1만2천5백개+)라고? 참고: Python의 (사실상)표준 구현체인 CPython의 스타가 27.2k+, React가 138k+, Vue가 151k+(언제 이렇게..), Django가 44.7k+ 등으로 12.5k면 무척무척(!!) 많은 숫자다. 이정도면 스타트업으로 따지면 유니콘급 아닐까? 기왕 본 김에 Hello World 부터 한번 시작해 봐야겠다. - 라고 생각해서 기본 문법부터 간단 웹서버까지 한번 띄워보기로 생각했다. 이것이 바로 HDD, Hype Driven Dev…. 공식 페이지: https://vlang.io Docs: https://vlang.io/docs 당연히 시작은 공식 문서를 봐야지. 모든 시작은 설치부터사실 Golang 관련해서 처음에 어려움을 겪은 적이 있었다. GO_PATH 관련이나 혹은 설치 패키지 등 드물게(거의 없지만) 발생하는 케이스에서 접근성이 약간 낮나? 하는 생각을 하기도 했다. (사실, 파이썬도 어떤어떤 경우(윈도우 계정명이 한글이라거나..)에는 설치가 꼬이기도 한다. 아주 가끔.) 그래서 ‘과연 설치가 잘 되기는 할까…?’하는 의문을 품었다. 게다가 아래 사진처럼 Linux는 바이너리 파일을 지원하지만, macOS와 Windows는 지원하지 않는 것을 보았다. 즉, 직접 빌드를 해야한다는 것! 직접 뭔가 빌드를 해보신 분들은 아시겠지만 빌드라는건 빌드 시간이 정말 빠를까? 하는 막연한 두려움이 생긴다. 그래도 어쩔수 없으니, 해보기로 한걸 해보기로 했다. 반전: 설치는 Git clone이 제일 오래 걸리더라 공식 설치방법: Installing V from source 설치는 아주 단순하게 Git clone 이후 make 명령어만 치면 끝난다. 아래 세줄로 끝! 123git clone https://github.com/vlang/vcd vmake 실제로 걸리는 시간은 1분도 걸리지 않더라(!!) 실행되는 코드는 vc, V 컴파일러를 C로 변환한 레포를 받은 뒤 V를 빌드(!)해 바이너리 파일을 만든다. 성공한 빌드 환경: macOS 10.14.6(18G87) Apple clang version 11.0.0 (clang-1100.0.33.8) XCode 11.1 아무 폴더에서나 진행한 뒤 해당 폴더로 접근 가능하도록 시스템의 PATH 에 등록해주면 완료. 저는 Home폴더 내 .v 폴더를 만들고 그 안에 설치를 진행했습니다. 사실 어디에 두던 상관없고, sudo ./v symlink 명령어를 통해 /usr/local/bin 에 심볼릭 링크를 걸어줄수도 있습니다. (이 설명을 뒤늦게 읽었어요….) 그리고 가장 먼저하는 Hello world! v는 그냥 실행하면 마치 파이썬처럼 REPL 환경을 제공한다. (&gt;&gt;&gt; 표시 보고 순간 파이썬인줄 알았다!) 분명히 컴파일 하는 언어인데 이렇게 잘 지원을 해준다. 그래도 기본적인 사용법은 파일을 만들고 👉 컴파일/빌드 해주고 👉 실행! 이니까, 그렇게 해보기로 했다. 파일로 시작하는 V langCode Editor?막상 파일로 시작하려니 엇, 이건 지원하는 코드 에디터나 IDE가 있나? 하는 의문이 들었다. 코드 에디터나 IDE의 지원에 따라 생산성이 달라지는건 확실하기 때문에, 없다면 상당히 회의적이게 될 것 같다는 생각을 했지만.. 그런데 짜잔! VS Code에 V 라는 이름의 패키지로 V lang을 위한 하이라이팅, 테마, 코드 스니펫 등을 지원하고 있었다. 글 쓰는 중 0.0.7 👉 0.0.8로 버전업이 있었다. 빠른 버전업 와우! 설치는 VS Code 내 Extensions 마켓 플레이스에 ‘v’ 라고만 검색하면 최상위에 뜬다. 검색이 어려우면 👉 V for VSCode(설치링크) 에서 Install 클릭! 그러면 잘 될까? 가장 제일 먼저 나오는 Hello World를 만들어보았다. 여타 다른 언어와 동일한 수준으로 하이라이팅이 잘 된다! 괄호 레벨에 따라 컬러링 붙는건 다른 익스텐션으로, Bracket Pair Colorizer 2(설치링크) 를 설치하시면 됩니다. 꽤 좋아요. 추천! 첫 파일, 첫 빌드01_hello.v 라는, .v 확장자를 가진 첫 파일을 만들었다. (바로 위 파일이다!) V를 통해 위 코드를 실행하는 것에는 두가지 방식이 있다. V로 빌드 &amp; Binary 실행 v run 명령어로 1번 통합실행 간단하게 v run으로 테스트를 해봤다. 실제로 기존 같은 이름의 바이너리 파일이 있으면 덮어쓰기를 하고 실행하는 것을 볼 수 있다. 만들어진 바이너리 파일은 실행 권한도 있어서 곧바로 실행할 수도 있다. 여타 다른 C언어나 Go처럼, 파일 내부의 main() 함수를 찾아서 실행하는 것은 동일하다. 하지만 main 함수가 없더라도 해당 파일 내부는 모두 실행이 된다. 아래 파일은 main 없이 만든 v 파일이다. 실제 실행을 해보면 아래와 같이 파일 내부에 있는 것을 잘 실행한다. 정리크게 다른 부분은 없다. 하지만 몇가지 부분에서 놀랐다. 설치에서 에러가 나지 않았다! 사실 이거 굉장히 중요한 부분이다. 사용자들의 접근성에 있어서 매우 중요함. Code Editor가 지원된다 이것 역시 처음 접할때 &amp; 이후 개발시에도 편리함을 얼마나 지원하느냐의 이슈라 매우 중요함! 불편하지 않고 ‘깔끔한데?’ 라는 생각이 드는 문법 분명히 빌드/컴파일을 해야하는 언어지만 마치 “타입 힌팅해서 쓰는 파이썬 + JS 조금..” 같다는 느낌적 느낌을 받았다. 좀 더 알아보고 느낌이 달라질수도 있겠지만, 현재 느낌은 마치 노션을 접했을때 느낀 산뜻한 느낌과 비슷한 기분. (물론 이걸 어디다 써먹을 수 있냐는 질문은 별도의 문제라는 것…) 간단하게 배우고 - 실제로 공식 문서에서 1시간이면 다 배운다고 한다는 말이 Fact - 빠르게 쓰는 것, 부담이 없게 느껴지는게 신기했다. 조금 더 알아봐야지.","link":"/2019/10/20/Learning-VLang/"},{"title":"한국어 NLP와 딥러닝을 위한 도커이미지 만들기","text":"딥러닝 + 도커?딥러닝 프로젝트를 진행할 때 귀찮은 것 중 하나는 여러 라이브러리를 관리하고 어떤 버전을 설치했는지를 매번 체크하는 것이다. Tensorflow나 PyTorch의 경우 매 시즌별로 버전 업데이트가 이뤄지며 동시에 api가 이전 버전과 달라져 어떤 것을 사용해야 하는지 선택이 곤란해지는 때가 있다. 한편 위 문제는 양반일 정도로 귀찮은 것이 하나 더 있다. 바로 CUDA와 cuDNN, APEX등을 버전을 맞춰 설치하고 PATH를 잡아서 진행하는 부분은 정말 끔찍하다. 다만 딥러닝만이 아닌 웹 개발을 진행한다 하더라도 버전 관리와 재현성이 제공되는 개발환경은 필수이기 때문에 도커를 사용해서 관리하는 것은 사실상 기본이 되어가고 있다. 딥러닝 도커 이미지딥러닝에 도커를 사용하는 이유는 또 다른 이유도 있다. 딥러닝 라이브러리들에서 GPU 가속을 사용하기 위해서는 앞서 말했던 것과 같이 CUDA와 여러 GPU 관련 라이브러리를 잘 잡아줘야 한다. 하지만 이 작업이 생각보다 귀찮은 것은 차치하더라도, Tensorflow나 PyTorch가 (특히 TF가..!!!) 요구하는 CUDA/cuDNN을 버전 세트를 맞춰주는 것도 한세월. 하지만 최근 몇년 사이 딥러닝을 위한 도커 이미지가 넘쳐나고 있다. 당장 Tensorflow나 PyTorch의 공식 이미지부터 시작해 deepo, h2o.ai등 굉장히 다양한 이미지들이 넘쳐나고 있다. 이들을 이용해 추가적인 라이브러리를 설치해 입맛에 맞는 도커 이미지를 만들면 가장 편안하게 연구 개발을 할 수 있다. Deepo 이미지로 띄우기여러가지를 사용해 보았지만 현재(2019.11.27)시점 체감상 가장 안정적이고 잘 동작하고 나름 설정이 잘 되어있는 - 혹은 개인적 취향에 맞는 - 라이브러리는 바로 deepo 였다. Deepo 바로가기: https://github.com/ufoym/deepo Deepo는 처음부터 도커기반 딥러닝 이미지 프로젝트였고, CPU와 GPU 모든 경우이고 동시에 현재 자주 사용중인 딥러닝 라이브러리 대부분을 지원한다. https://github.com/ufoym/deepo#available-tags 에서 볼 수 있는 Deepo에서 제공하는 태그들. 웬만한 것들은 다 지원하는 셈이다. 따라서 이 도커 이미지 위에 우리가 원하는 추가 패키지들을 설치하고 세팅을 진행하면 보다 편안-한 마음으로 개발을 할 수 있다. Deepo 이미지 중 어떤 것을 사용하나?개인적으로는 PyTorch를 가장 많이 사용하고 종종 Tensorflow도 사용하기 때문에 위 이미지 중 전체가 다 설치되어있는 all 시리즈를 사용한다. 그 중에서도 CUDA버전을 명시적으로 지정하는 all-jupyter-py36-cu100 (혹은 all-jupyter-py36-cu101)을 이용해서 진행하면 좋다. 최신버전을 사용하려면 all-jupyter 을 사용하면 된다. 이 버전에 따라서 Pre installed 라이브러리들의 버전이 달라진다. 도커 이미지 만들기도커 이미지는 보통 아래 요소들로 만들어진다. FROM: 어떤 이미지에서 받아서 진행할지 RUN: 어떤 명령어를 실행해서 이미지로 만들지 WORKDIR: 현재 있는 폴더에서 명령을 실행할지 각 요소 하나하나가 실행될 때 마다 도커 이미지의 layer가 된다. = 이미지가 무거워진다! 따라서 apt-get 와 같이 한번만 실행하면 되는 경우에는 한 세트로 넣어서 돌리는 것이 유리하다. Ubuntu 기반인 deepodeepo는 ubuntu 기반으로 이미지를 제작하기 때문에 apt 등을 보다 편안하게 사용할 수 있다. RUN 을 사용할 경우, 상위 이미지(deepo)에서 지정한 root 유저로 실행되기 때문에 apt 와 같은 명령어를 sudo 권한 없이도 실행할 수 있다. 1) Ubuntu 기반으로 불러오기 FROM 을 이용해 Deepo 이미지를 불러오자. 1FROM ufoym/deepo:all-jupyter 2) KoNLPy를 사용하기 위해 JVM을 설치해준다. 이와 함께 python, curl, wget등을 같이 설치해주자. 123456# Install JVM for KonlpyRUN apt-get update &amp;&amp; \\ apt-get upgrade -y &amp;&amp; \\ apt-get install -y \\ openjdk-8-jdk wget curl git python3-dev \\ language-pack-ko 3) 언어팩 설정을 하기 위해 가장 많이 지원되는 en_US.UTF-8 을 시스템 언어로 지정해주자. 12RUN locale-gen en_US.UTF-8 &amp;&amp; \\ update-locale LANG=en_US.UTF-8 4) zsh를 설치한다. (Optional, 하지만 나중에 작업 편함) 123# Install zshRUN apt-get install -y zsh &amp;&amp; \\ sh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" 5) Kakao에서 발표한 CNN기반 토크나이저인 Khaiii를 설치한다. 빌드 시간이 조금 오래걸린다. 123456789101112131415# Install KhaiiiWORKDIR /depsRUN git clone https://github.com/kakao/khaiii.gitWORKDIR /deps/khaiiiRUN pip install cython &amp;&amp; \\ pip install --upgrade pip &amp;&amp; \\ pip install -r requirements.txt &amp;&amp; \\ mkdir buildWORKDIR /deps/khaiii/buildRUN cmake .. &amp;&amp; make all &amp;&amp; make resource &amp;&amp; make install &amp;&amp; make package_pythonWORKDIR /deps/khaiii/build/package_pythonRUN pip install . 6) Jupyter Notebook 환경을 좀더 편리하게 사용하기 위해 Extension들을 설치해준다. 추가적으로 필요한 Extension이 있다면 이 단계에서 설치하는 것을 추천한다. 1234567# Setup Jupyter extensionsRUN pip install jupyter_nbextensions_configurator jupyter_contrib_nbextensions &amp;&amp; \\ jupyter nbextensions_configurator enable &amp;&amp; \\ jupyter contrib nbextension installRUN pip install jupyter_http_over_ws &amp;&amp; \\ jupyter serverextension enable --py jupyter_http_over_ws 7) 기타 Python라이브러리와 mecab을 설치한다. 기타 라이브러리를 설치해준다. 간단히 소개하자면… Pandas_explode는 pandas Dataframe에 .explode('column') 등을 사용 가능하게 만들어준다. (속도가 빠르진 않다.) autopep8은 PEP8을 자동으로 맞춰준다. (ipynb파일도 지원함) s3fs - s3:// 주소를 사용 가능하도록 만들어준다. (pd.read_csv 등에서 유용) fastparquet - parquet 파일 형식을 읽고 쓸 수 있도록 해준다. soynlp / konlpy - 한국어 토크나이저 dask - Pandas와 비슷하지만 Distributed Computing을 지원하는 라이브러리 python-snappy - parquet 파일 사용시 snappy 알고리즘을 사용하는데 이때 필요함 mecab-ko: KoNLPy에서 Mecab 클래스 사용을 위해서 필요 mecab은 초당 3~5000개 처리 / 1core 다른것은 초당 100-300개 처리 / 1core (라이젠 1800x 기준) 1234567891011# Install another packagesRUN pip install -e git+https://github.com/kanth989/pandas_explode#egg=pandas_explodeRUN pip install \\ autopep8 pytorch_pretrained_bert \\ s3fs fastparquet soynlp konlpy \\ randomcolor pynamodb plotlyRUN pip install \"dask[complete]\"RUN pip install python-snappy# Add Mecab-KoRUN curl -L https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash 8) Docker 이미지를 root아닌 유저로 쓰도록 지정하는 유저 생성하기 (Optional) user 라는 이름을 가진 유저가 생성된다. 12345# Add non-root userRUN adduser --disabled-password --gecos \"\" user# Reset WorkdirWORKDIR /code 유저를 root 가 아닌 유저로 생성하는 경우의 이점과 단점 장점 이후 Volume 마운트 시 권한이 꼬이지 않을 수 있고, 조금 더 안전하다. 단점 이와 같이 유저를 생성할 경우, sudo 권한을 명시적으로 주지 않는 한 sudo 명령을 쓰지 못한다. apt-get 등으로 설치가 필요한 경우 Dockerfile을 새로 빌드해야 한다. 하지만 간단한 pip install 등은 User 디렉토리에 설치하는 방법을 통해 조금 환경 제약을 줄일 수도 있다. 최종본 Dockerfile최종본 파일은 https://github.com/Beomi/deepo-nlp/blob/master/Dockerfile 에서 받을 수 있다. 위와 같이 Dockerfile을 작성한 뒤, 아래 명령어를 통해 도커 이미지를 빌드하면 된다. 1docker build -t beomi/deepo:all-jupyter-py36-cu100-konlp . 도커 띄우기 RUN!위와 같이 도커이미지를 만든 뒤, Jupyter Notebook 환경을 원활하게 사용하기 위해서는 여러 세팅이 필요하지만, 그 중에서 몇가지 지원을 하는 방식을 보자. 아래 파일은 내가 위 도커 이미지를 실행할때 쓰는 Shell Script다. 12345678910111213141516docker run --gpus=all -it --rm --ipc=host \\ -p 28888:8888 \\ -p 28787:8787 \\ -u $(id -u ${USER}):$(id -g ${USER}) \\ -v $(pwd)/.dockervm/tmp:/tmp \\ -v $(pwd)/.dockervm:/home/user \\ -v $(pwd)/code:/code \\ -v /home/${USER}:/home/${USER}:ro \\ beomi/deepo:all-jupyter-py36-cu100-konlp \\ zsh -c ' export SHELL=zsh; export PATH=$PATH:/root/.local/bin:/root/.local/lib/python3.6/site-packages; jupyter notebook \\ --no-browser \\ --ip=0.0.0.0 \\ --notebook-dir=/code' 하나씩 뜯어보자. 1) GPU설정 &amp; ipc=host 1docker run --gpus=all -it --rm --ipc=host \\ nvidia-docker 를 사용해야 도커 환경 내에서 CUDA가속을 사용할 수 있다. Nvidia Docker 공식 레포: https://github.com/NVIDIA/nvidia-docker 도커 이미지 실행시 어떤 GPU를 사용할지 지정해야 하는데, --gpus=all 로 사용할 경우 해당 시스템에 있는 모든 GPU를 사용하도록 지정하고, -gpus '&quot;device=1,2&quot;' 와 같이 지정시 PCIe Bus기준 1,2번에 해당하는 것만 사용하도록 지정할 수 있다. 2) 도커 이미지로 연결할 포트 지정 12-p 18888:8888 \\-p 18787:8787 \\ -p 명령어를 통해 어떤 외부포트 를 어떤 내부포트 로 연결할지 지정할 수 있다. 위와같이 -p 18888:8888 로 지정할 경우 해당컴퓨터IP:18888 로 접속시 Jupyter Notebook 포트인 내부 8888에 접근할 수 있게 된다. 3) 유저명으로 접근 1-u $(id -u ${USER}):$(id -g ${USER}) \\ 위와 같이 사용시 현재컴퓨터에 로그인한 유저의 ID와 Group ID를 알 수 있다. 만약 첫 유저라면 보통은 1000:1000 속성을 가지게 된다. 앞서 Dockerfile에서 지정했던 유저는 1000번이기 때문에 만약 혼자 사용하는 컴퓨터라면 유저 권한을 맞춰줄 수 있다. 만약 여럿이서 사용하는 서버라면 Dockerfile 빌드시 유저 id를 다르게 생성하면 된다. 4) 로컬의 임의의 폴더(.dockervm)을 연결 123-v $(pwd)/.dockervm/tmp:/tmp \\-v $(pwd)/.dockervm:/home/user \\ -v $(pwd)/code:/code \\ 위와 같이 임의의 폴더에 도커 내부의 폴더와 연결하면, pip install --user 와 같은 명령어를 통해 설치한 패키지는 도커 이미지를 재시작 할 경우에도 여전히 설치된 상태를 유지할 수 있고, JupyterNotebook의 패스워드 역시 매번 새로운 토큰 대신 고정된 패스워드를 사용할 수 있다. 또한 로컬의 code 폴더와 내부의 code 폴더를 맞춰 쉽게 액세스 할 수 있도록 실제 작업환경 공간을 맞춰준다. 5) 로컬의 유저 폴더를 내부에 ReadOnly로 연결 1-v /home/${USER}:/home/${USER}:ro \\ Docker의 Volume Mount시 :ro 옵션을 붙이면 ReadOnly 모드로 동작한다. 특정 파일들을 접근할 때 매번 도커 이미지 내에 업로드 하는 대신, 로컬의 파일을 손쉽게 가져다 쓸 수 있도록 만들어 준다. 6) ZSH를 이용한 JupyterNotebook 실행 12345678beomi/deepo:all-jupyter-py36-cu100-konlp \\zsh -c 'export SHELL=zsh;export PATH=$PATH:/home/user/.local/bin:/home/user/.local/lib/python3.6/site-packages;jupyter notebook \\--no-browser \\--ip=0.0.0.0 \\--notebook-dir=/code' 마지막 단계인 도커 이미지 실행 단계이다. 단순하게 jupyter notebook 이라고 실행하면 문제가 발생한다. PATH 지정이 되지 않아 앞서 설치한 패키지가 인식되지 않을 수 있기 때문이다. 따라서 PATH 를 오버라이딩해줘 Python이 인식하도록 만들어줄 수 있다. 이후 No browser(CLI인 경우) 옵션, 그리고 모든 hostname을 통해 접근 가능하도록 0.0.0.0 으로 지정하고, JupyterNotebook이 실행될 기본 폴더를 /code 로 지정해주면 끝이 난다. 정리딥러닝, 혹은 자연어 처리 등을 위해 Docker 이미지를 직접 만들어서 쓰는건 사실 당연한 일이다. 아무리 기존의 딥러닝 이미지들이 잘 되어있다고 하더라도 특히 한국어를 위한 이미지는 많이 부족한 것이 사실이다. 시스템을 많이 건드리지 않으면서도 커스터마이징을 좀 더 쉽고 빠른 연구가 가능하도록 자신만의 환경을 쌓아가는 것도 중요한 부분이지 않을까 생각해 본다.","link":"/2019/12/20/DockerImage_for_KoreanNLP/"},{"title":"트위터에서 많은 팔로워를 크롤링하는 방법 [1]: 어떤 점들을 고려해야 할까?","text":"트위터에는 수많은 유저가 있다. 그 중에서는 단 1명의 팔로워도 모으지 못한 유저도 있지만 반대로 수백 수천만명의 팔로워를 가진 유저도 있다. Twitter의 트위터 공식 계정에는 약 5천6백만명의 팔로워가 있다. 당장 우리가 ‘팔로워’ 를 클릭해 내용을 살펴보면 한번에 20명씩 정보를 보여준다. 스크롤을 해서 5천만명의 유저 데이터를 가져올 수는 없다. 그렇다면 어떤 방법을 써야 저 데이터를 가져올 수 있을까? 어떤 상황에 처해있나? 어떤 데이터가 필요한가? 데이터가 얼마나 필요한가?여러가지 방법을 생각해 볼 수 있지만, 각각의 방법에는 ‘조건’이 필요하다. 예를들어, 아래와 같은 질문에 대한 응답이 데이터 수집 방법을 쓸 수 있는 조건이 된다. Q. “트위터 API를 유료로 구매해서 쓸 수 있는가?” 위 질문을 통해 트위터 API를 회피해야 하는지, 아니면 정직하게 지불을 하고서 사용을 해야하는지가 갈리게 된다. Q. “얼마나 많은 팔로워를 수집해야 하는가?” 위 질문은 두가지로 나누어서 볼 수 있다. 1) 수집할 계정 수가 많은가 2) 수집할 각 계정의 팔로워 수가 많은가 1)의 경우는 한번에 여러명의 계정을 수집해야 한다. 한편, 2)의 경우는 조금 더 어렵다. 위 조건들을 따지고, 어떤 방식으로 데이터 수집을 진행할지 선택해야 한다. 참고) 이후부터는 팔로잉/팔로워를 구분하지 않고 언급합니다. “API 살 돈 없고, 수집할 계정 수도 많고, 엄청 많은 팔로워있는 계정도 있어요…”위 가정들에서 어려운 상황들을 골라 가정해보자. API를 유료로 구매할 지원금도 없고, 수집해야하는 계정 수는 수백수천을 넘어 수십만을 향해가고, 전체적으로 팔로워가 엄청 많지는 않지만 수백 수천명의 팔로워는 흔하고 수십만명 팔로워를 가진 셀럽의 계정들을 수집해야한다고 가정해보자. 문제를 하나씩 나눠서 해결해보자. 무료 API라도 쓸 것인가? vs HTML을 긁을 것인가?API의 무료 Quota라도 사용해보자API를 검색해보면 다음과 같이 Follower의 목록을 가져오는 API가 보인다. 공식 링크: https://developer.twitter.com/en/docs/accounts-and-users/follow-search-get-users/api-reference/get-followers-list 하지만 잘 읽어보면 문제가 있다. “Rate Limited? YES.” 15분에 겨우 15개, 즉 1분에 1개밖에 요청을 하지 못하는 것을 볼 수 있다. 한번에 최대로 요청 가능한 수가 200개이니 1시간 = 60분에 12,000명의 정보를 가져올 수 있다. 하지만 만약 팔로워 수가 120만명인 유저 1사람이 있다? = 10시간을 투자해야 한다… 당연히, 이건 좀 아니다! 하지만 만약 수집하는 유저 수가 수백명 안쪽이고 평균 팔로워수도 수십-수백명이면 여전히 유효하고 가장 안전하고 엄밀한 방식이다. Q. 여러 계정을 만들어서 API Key를 많이많이 발급받으면 되지 않나요? A. 충분히 가능하지만, 최근 트위터는 API Key 발급을 위해 상당히 많은 항목(연구 계획, 필요한 데이터 범위 등등)을 요구하고 심사 기간도 굉장히 오래 걸립니다.(1주+ 소요) 그렇다면… 인증이 필요해서 ➡️ 느린 속도(Quota limit)가 걸린다면, 인증 없이 수집할 수 있다면 ➡️ 빠른 속도로 수집할 수 있겠다! 그렇다면, 어떻게 해야 ‘인증없이’ 수집할 수 있을까? 트위터에서 많은 팔로워를 크롤링하는 방법 2편: HTML 웹 크롤링을 해보자으로 이어집니다.","link":"/2019/12/21/Crawling_Twitter_Following/"},{"title":"pypapago 개발기","text":"TL;DR아래 내용을 통해 개발한 pypapago 는 현재 pypi에 올라가 있어 아래 명령어로 설치해 바로 사용할 수 있습니다. 1pip install -U pypapago 2019.07.09일자 기준 최신버전은 0.1.1.1 입니다. Github Repo: https://github.com/Beomi/pypapago 파파고 번역? 네이버에서는 Papago 라는 이름으로 ML 기반과 사전 기반 두가지 방식의 번역과 언어 감지 등 여러가지 서비스를 제공한다. 한편 해당 서비스는 파파고 공식 사이트에서 실제로 사용할 수 있지만 API를 이용해서 사용할 수도 있기 때문에, 개발자들이 API Key만 신청하면 한 번에 5천자, 그리고 하루에 1만자 이내로만 요청할 수 있다는 제한이 있다. (무료로 사용하는 경우) 한편 papago.naver.com에서 제공하는 웹 페이지 상에서의 번역은 추가적인 제한이 없기 때문에, 해당 웹 페이지를 파싱해서 어떤 API Call을 하고 있는지 뜯어보면 보다 많은 요청을 자유롭게 할 수 있지 않을까 싶었다. AJAX(XHR) Request 뜯어보기파파고에 번역할 문장을 집어넣고 ‘번역’ 버튼을 누를 때 이뤄지는 HTTP 요청을 크롬 개발자 도구로 살펴보면 다음과 같다. 실제로 translatedText 라는 키에 결과값이 잘 들어오는 것을 볼 수 있다. 한편 그렇다면 HTTP 요청을 보내는 방식이 어떻게 이뤄지는지 확인이 필요하다. 가장 먼저 살펴보는 것은 API Call이 어떤 URL(HOST)와 어떤 Method로 요청이 이뤄지는지 보는 것. 위 스샷을 살펴보면 https://papago.naver.com/apis/n2mt/translate 주소에 POST 방식으로 HTTP 요청을 보내고 있다는 것을 볼 수 있다. POST 방식으로 보내는 HTTP 요청은 주로 &lt;form&gt; 내부의 FormData와 함께 보내는 경우가 많다. 따라서 아래쪽의 Form Data 항목을 살펴보면 다음과 같다. 하지만 우리가 입력한 “I am GROOT”라는 문장은 위 FormData 어디에서도 살펴볼 수가 없다. 어떻게 된 것일까? Base64 Encode &amp; Decode위와 같이 암호화된 것 처럼 보이는 데이터를 base64 로 디코딩을 해 보았다. 결과는 위와 같이 “I am GROOT”라는 부분이 잘 나오는 것을 볼 수 있다. 한편 앞쪽에 나와있는 이상한 문자열은 정체를 알 수 없었다. 따라서 우리가 실제로 조정하는 것이 필요한 source, target, text를 남기고 앞쪽은 base64로 인코딩 된 값을 그대로 가져왔다. (패키지 소스코드의 SECRET_KEY 부분. 사실은 secret이 아니다.) 코드로 만들어보자Basic setup가장 기초적인 방법은 크롬에서 실제로 요청하는 HTTP를 그대로 따라하는 방법이다. Translator Class를 생성할 때 기초적인 헤더들을 설정해 주고, 앞서 살펴보았던 요청에 필요한 base64로 인코딩 된 값들을 넣어준다. Github: https://github.com/Beomi/pypapago/blob/0.1.1.1/pypapago/translator.py#L14-L33 12345678910111213141516171819202122232425class Translator: \"\"\" Main Translator Class \"\"\" def __init__(self, regex_pattern=None, headers=None): self.regex_pattern = re.compile(regex_pattern or '[가-힣]+') self.headers = headers or { 'device-type': 'pc', 'origin': 'https://papago.naver.com', 'accept-encoding': 'gzip, deflate, br', 'accept-language': 'ko', 'authority': 'papago.naver.com', 'pragma': 'no-cache', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko)\\ Chrome/75.0.3770.100 Safari/537.36', 'content-type': 'application/x-www-form-urlencoded; charset=UTF-8', 'accept': 'application/json', 'cache-control': 'no-cache', 'x-apigw-partnerid': 'papago', 'referer': 'https://papago.naver.com/', 'dnt': '1', } self.SECRET_KEY = 'rlWxMKMcL2IWMPV6ImUwMWMwZWFkLWMyNDUtNDg2YS05ZTdiLWExZTZmNzc2OTc0MyIsImRpY3QiOnRydWUsImRpY3REaXNwbGF5Ijoz' self.QUERY_KEY = '0,\"honorific\":false,\"instant\":false,\"source\":\"{source}\",\"target\":\"{target}\",\"text\":\"{query}\"}}' String Type의 base64 값 만들기앞서 사용한 self.QUERY_KEY 의 경우는 아직 UTF-8이기 때문에 base64로 인코딩 된 str 결과물이 필요하다. 이때 단순히 encode를 하면 type이 str이 아니라 byte타입이기 때문에 문제가 생긴다. (String와 Byte를 합칠 수 없다고 TypeError가 발생한다.) Github: https://github.com/Beomi/pypapago/blob/0.1.1.1/pypapago/translator.py#L35-L42 12345678@staticmethoddef string_to_base64(s): \"\"\" Generate Base64 Encoded string :param s: Origin Text (UTF-8) :return: B64 encoded text (B64, still UTF-8 string) \"\"\" return base64.b64encode(s.encode('utf-8')).decode('utf-8') 따라서 위와 같이 .decode('utf-8') 으로 다시한번 바꾸어주는 과정이 필요하다. HTTP 요청 보내기위와 같이 준비가 끝나면 실제 HTTP 요청으로 보내는 것이 필요하다. API Host도 알고, Method와 내용물도 알고 있으니 간단하게 보내기만 하면 된다. Github: https://github.com/Beomi/pypapago/blob/0.1.1.1/pypapago/translator.py#L44-L61 123456789101112131415161718def translate(self, query, source='en', target='ko', verbose=False): \"\"\" Main Translate function :param query: Original Text to translate :param source: Source(Original) text language [en, ko] :param target: Target text language [en, ko] :param verbose: Return verbose json data. Default: False :return: Translated text \"\"\" data = { 'data': self.SECRET_KEY + self.string_to_base64( self.QUERY_KEY.format(source=source, target=target, query=query) ) } response = requests.post('https://papago.naver.com/apis/n2mt/translate', headers=self.headers, data=data) if not verbose: return response.json()['translatedText'] return response.json() 실제로 query, source, target 을 우리가 바꾸어 사용하기 때문에 해당하는 값들을 넣어주고, API 요청이 이뤄질때 실제로 넘어오는 결과값은 굉장히 길고 디테일한 정보를 담고있다. 이런 정보도 사용할 수 있도록 verbose 옵션을 통해 Raw json을 받을 수 있는 옵션도 넣어준다. Bulk/Parallel로 실행하기한편 번역기를 사용할 때 한번에 하나가 아니라 여러개를 실행해야 하는 경우도 있다. 이런 경우를 위해 multiprocessing 을 사용해 기본적으로는 cpu코어 수(하이퍼스레딩은 2배)만큼 Worker를 띄워 사용하도록 설정해두고, 커스텀으로 worker 수를 지정할 수 있도록 옵션을 넣어준다. 실제로 worker를 30개를 넣으면 25배+로 빨라진다. WOW. Github: https://github.com/Beomi/pypapago/blob/0.1.1.1/pypapago/translator.py#L63-L80 123456789101112131415161718def bulk_translate(self, queries, source='en', target='ko', workers=None, verbose=False): \"\"\" Call Translate function in parallel :param queries: List of query texts :param source: Source(Original) text language [en, ko] :param target: Target text language [en, ko] :param workers: Python multiprocessing workers. Default: vCPU cores :param verbose: Return verbose json data. Default: False :return: List of translated texts \"\"\" with Pool(workers or cpu_count()) as pool: result = list(tqdm(pool.imap( func=partial(self.translate, source=source, target=target, verbose=verbose), iterable=queries ), total=len(queries))) pool.close() pool.join() return result 그리고 Bulk로 작업을 하는 경우에는 보통의 경우 얼마나 진행되었는지 알고싶은 것이 당연하기 때문에 imap 와 tqdm 을 사용해 Progress bar를 화면(Jupyter Notebook도 지원함!)에 나타나도록 만들어주었다. (아래는 Google colab에서 pypapago를 이용해 몇천개 번역을 worker 30개로 돌릴때 화면에 나타나는 모습) 맺으며간단하게 써보려고 하다가 파파고 API의 요청건수가 가볍게 넘어버려서 (ㅠㅠ) + Google번역기 패키지가 자꾸 에러를 뿜어서 라는 두가지 이유로 인해 만들어보았다. 오랫만에 작업한 pypi 패키징이라 살짝 헷갈리기도 해서 0.1.0 배포 후 0.1.1 로 긴급 패치(의존성을 setup.py 에 넣는 것 잊음)를 했지만 정작 오타가 있어서 0.1.1.1이라는 이상한 버전으로 올리게 되었다는 이야기. pypi 패키징 하는 부분도 간단하게 정리가 필요할 것 같다.","link":"/2019/07/08/Papago-API-with-Python/"},{"title":"트위터에서 많은 팔로워를 크롤링하는 방법 [2]: HTML 웹 크롤링을 해보자","text":"앞서 쓴 글(1편)에서는 몇가지 조건들과 나쁜 상황을 걸어 단순한 수집이 아니라 뭔가 꼼수가 필요한 상황을 가정해 보았다. API를 유료로 못쓴다? ➡️ 못쓸정도로 느리다! 😨 따라서 다른 방법인 직접 데이터 크롤링이 필요했고, 이번에는 Twint와 HTML 웹 크롤링을 통해서 수집을 해 본 과정을 적어보았다. HTML 웹 크롤링을 해보자!인증이 필요해서 ➡️ 느린 속도(Quota limit)가 걸린다면, 인증 없이 수집할 수 있다면 ➡️ 빠른 속도로 수집할 수 있겠다! 1) Twint: 내가 짠 것보다는 3k 스타 오픈소스가 낫겠지? 처음에는 기존에 존재하는 트위터 수집 라이브러리를 사용해보자!는 생각을 했다. 내가 생각했다면, 분명 다른 누군가는 만들어뒀을거라는 나름 타당한 생각에 검색을 했고, 가장 적합한 라이브러리라고 생각했던 것이 바로 Twint였다. Spoiler: Twint는 쓸만하지 않았다. Twint를 이용해 로컬에서 적당한 사이즈(1k 팔로잉 이하)를 크롤링 하는 것에 성공했지만 MultiProcess를 통해 돌릴 경우 생각보다 굉장히 낮은 성능을 보이는 면을 보여, AWS Lambda에 올려 병렬로 수집하고자 했다. 수집에 성공시 Boto3를 통해 DynamoDB에 적재하는 방법을 취했다. (아래 코드가 AWS Lambda에 올린 코드.) Sqlite를 사용하기까지 해서 이상한 코드(8-9번째 줄)까지 추가로 지정해줘야 했다. 123456789101112131415161718192021222324252627282930313233343536373839404142import impimport sysimport jsonimport boto3db = boto3.client('dynamodb')sys.modules[\"sqlite\"] = imp.new_module(\"sqlite\")sys.modules[\"sqlite3.dbapi2\"] = imp.new_module(\"sqlite.dbapi2\")def get_following(username): import twint twint.output.follows_list = [] c = twint.Config() c.Username = username c.Store_object = True twint.run.Following(c) following = twint.output.follows_list return list(set(following))def lambda_handler(event, context): if event.get('body'): event = json.loads(event['body']) print(\"HTTPAPI /\", event['username']) username = event['username'] followings = get_following(username) if not followings == ['message']: db.put_item( TableName='tweet-followings', Item={ 'username': {'S': username}, 'following': {'SS': followings}, } ) return { 'statusCode': 200, 'body': json.dumps(followings), 'headers': {'Content-Type': 'application/json'}, } 한편, Twint 자체가 Async하게 동작하고있음에도 굉장히 느린 속도를 보이기도 하고 동시에 일부 대형 유저에서는 데이터 유실이 발생하기도 해, 해당 부분에 대한 신뢰도가 굉장히 낮아지게 되어, 다른 방법을 찾아보기로 했다. 2) 직접 짠 크롤링 코드: 차라리 내가 짜고 말지!앞서 말한 것과 같이, Twint에 대한 신뢰도가 팍팍 떨어지고 나서 데이터를 싹 버리고 새로 수집하는데, 기왕 이렇게 된 것 내가 직접 코드를 짜고 말지! 하는 생각을 했다. 간단하게, requests 그리고 beautifulsoup4 로 어떻게 수집, 안될까? 1st try: 어라? 생각보다 잘 되는데?정말로 심플하게 웹 페이지에 들어가서 수집하는 코드를 돌렸다. 유저의 아이디를 입력하면 해당 유저의 팔로잉을 모아 s3에 저장하는 코드를 구성했다. 1234567891011121314151617181920212223242526272829303132333435import jsonimport requests as rfrom bs4 import BeautifulSoup as bsimport boto3BASE_URL = 'https://mobile.twitter.com's3 = boto3.client('s3')def lambda_handler(event, context): if event.get('Records'): event = json.loads(event['Records'][0]['Sns']['Message']) username = event['username'] users = [] url = f'/{username}/following?lang=en' while True: try: s = bs(r.get(BASE_URL + url).text, 'html.parser') f_count = int(s.select_one('span.count').text.replace(',', '')) users += [i['name'] for i in s.select('div.user-list td.screenname a[name]')] url = f\"{s.select_one('div.w-button-more &gt; a')['href']}&amp;lang=en\" except TypeError as e: print(\"Cursor Ended\") print(len(set(users))) s3.put_object( Bucket='datas3asdfasdf', Key=f'twitter/following/{username}.json', Body=json.dumps({ 'username': username, 'following': users, 'count': f_count, 'len': len(users), }) ) 한번에 20개의 데이터를 보여주고, 각 항목에 나오는 HTML을 파싱해 유저의 screen_name (트위터의 @id)을 수집했다. 그리고 ‘더 보기’ 버튼이 있으면 해당 Cursor가 존재하기 때문에 해당 커서로 넘어가고, 만약 커서가 없다 = 종료되었다고 보고 크롤링을 종료했다. 하지만, 문제가 발생했다. 2nd try: 그럼, 한번에 잘 될리가 없지. 😅유저들 중에서 ‘정상적 페이지’가 나오지 않는 유저가 자주 발생했다. NOTE : 크롤링 진행시 Edge case를 미리 목록으로 만들고 진행하는 것이 낫다. 또한, 크롤링한 데이터는 메모리가 아닌 Json 등의 파일로 적재하고 수집한 것은 다시 수집하지 않도록 만드는 것이 필수다! 이유는 다양했다. Protected 계정: 유저가 자신의 정보를 보지 못하도록 비공개로 돌린 계정으로, 팔로우/팔로워 숫자는 볼 수 있지만 누구인지는 알 수 없다. “Sorry, that page doesn’t exist” 계정: 유저가 계정명을 변경했거나 탈퇴한 경우. ‘suspended’ 계정: 유저가 트위터 약관을 위반해 트위터에서 차단한 경우. (Spam 등) 특히 3번의 경우 정말로 예상치 못한 경우여서 데이터 수집을 중간에 놓치기도 했다. 그래서 몇번의 시행착오 끝에(이틀 넘게 걸렸다) 코드를 완성했다. 아래 코드는 위의 모든 엣지 케이스를 통과하는 방향으로 진행되었고, 결과를 확인했을때 99%정도의 확실한 결과를 보여주었다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115import jsonimport requests as rfrom bs4 import BeautifulSoup as bsimport boto3BASE_URL = 'https://mobile.twitter.com's3 = boto3.client('s3')def lambda_handler(event, context): if event.get('Records'): event = json.loads(event['Records'][0]['Sns']['Message']) username = event['username'] user_html = r.get(f'{BASE_URL}/{username}?lang=en').text # Proteced User 제거하기 if 'protected' in user_html: print(f\"User [{username}] is protected\") s3.put_object( Bucket='datas3asdfasdf', Key=f'twitter/following-protected/{username}.json', Body=json.dumps({ 'username': username, }) ) return False elif \"Sorry, that page doesn't exist\" in user_html: print(f\"User [{username}] does not exist(deleted)\") s3.put_object( Bucket='datas3asdfasdf', Key=f'twitter/following-deleted/{username}.json', Body=json.dumps({ 'username': username, }) ) return False users = [] url = f'/{username}/following?lang=en' while True: try: s = bs(r.get(BASE_URL + url).text, 'html.parser') try: f_count = int(s.select_one('span.count').text.replace(',', '')) except AttributeError: try: f_count = int(s.select('div.statnum')[-1].text.replace(',', '')) except IndexError as e: if \"Sorry, that page doesn't exist\" in s.select_one('body').text: print(f\"User [{username}] is deleted\") s3.put_object( Bucket='datas3asdfasdf', Key=f'twitter/following-deleted/{username}.json', Body=json.dumps({ 'username': username, }) ) return False if 'suspended' in r.get(f'{BASE_URL}/{username}?lang=en').text: print(f\"User [{username}] is suspended\") s3.put_object( Bucket='datas3asdfasdf', Key=f'twitter/following-suspended/{username}.json', Body=json.dumps({ 'username': username, }) ) return False users += [i['name'] for i in s.select('div.user-list td.screenname a[name]')] url = f\"{s.select_one('div.w-button-more &gt; a')['href']}&amp;lang=en\" except TypeError as e: print(\"Cursor Ended\") print(len(set(users))) try: is_complete = f_count == len(users) except UnboundLocalError as e: print(e) print(f\"[UnboundLocalError] username: {username}\") s3.put_object( Bucket='datas3asdfasdf', Key=f'twitter/following-no-fcount/{username}.json', Body=json.dumps({ 'username': username, 'following': users, 'count': -1, 'len': len(users), }) ) return False if is_complete: s3.put_object( Bucket='datas3asdfasdf', Key=f'twitter/following/{username}.json', Body=json.dumps({ 'username': username, 'following': users, 'count': f_count, 'len': len(users), }) ) else: print(f\"Unexpected End on crawling, counnt:{f_count}, len:{len(users)}\") s3.put_object( Bucket='datas3asdfasdf', Key=f'twitter/following-needmore/{username}.json', Body=json.dumps({ 'username': username, 'following': users, 'count': f_count, 'len': len(users), }) ) return True 와! 잘 된다! 그렇다면, 전부 끝난걸까? ➡️ 그럴리가요…앞서 작성한 코드를 AWS Lambda에 태워 S3에 적재하는 방향으로 진행했다. 해당 Lambda 함수는 AWS SNS를 통해 트리거되도록 만들어, Rate Limit exceed (Boto3을 통해 Lambda를 많이 호출시 발생하는 오류)를 회피하도록 구성했다. 한편, 여전히 문제는 남아있었다. AWS Lambda는 15분이 최대 😢항상 Lambda를 쓸 때 마다 투덜거리는 투로 올리는 말이지만, 서버리스가 정말 좋긴 하지만 Lambda에는 여전히 15분 max 제약이 걸려있다. 실제로 수천개 (2k~5k) 이내의 유저는 손쉽게 &amp; 안정적으로 데이터 수집을 완료했지만, 10k+ 계정들에서는 시간 부족의 문제로 수집되지 않는 등 이슈가 생겼다. 대체 어떻게 해야 다 모을 수 있을까? 트위터에서 많은 팔로워를 크롤링하는 방법 3편: 1초에 5천개 데이터 가져오기으로 이어집니다.","link":"/2019/12/22/Crawling_Twitter_Following_2/"},{"title":"Django CBV: queryset vs get_queryset() 삽질기","text":"요약: queryset은 request 발생시 한번만 쿼리셋이 동작하고, get_queryset()은 매 request마다 쿼리를 발생시킨다. 조건이 걸린 쿼리셋을 쓸때는 get_queryset()을 오버라이딩하자. 사건의 발단ListView안에서 체크박스로 ForeignKey로 연결된 장고 모델 인스턴스를 저장(.save()를 호출)하는데 저장 후 모델 인스턴스의 값을 확인하는 뷰에서는 결과값이 저장 전의 데이터로 나타났었다. 123456789# 문제의 코드..class OrderMatchingList(ListView): class Meta: model = Order queryset_list = Order.objects.filter(status__gte=5) \\ .select_related('education', 'region') \\ .prefetch_related('orderdetail_set') queryset = sorted(queryset_list, key=lambda x: x.start_date()) 사실 지금은 코드를 보면 queryset에서 sorted된 값을 반환하고, 이경우에는 쿼리셋 자체가 저 변수로 할당되어버려 다음 request에서 쿼리가 돌지 않는다는 것을 쉽게 찾을 수 있다. 하지만 원래 한번 안보이면 잘 안보이는 법.. 심지어 이 경우에는 Exception이 나는 것도 아니기 때문에 더 찾기 어려웠다.. (ㅠㅠ) 삽질의 시작여러가지 가정을 할 수 있는 상황이었다. 혹시 브라우저가 리스트를 캐싱하고 있던건 아닐까? (브라우저 캐시) 장고가 View의 Response를 캐싱하고 있는걸까? (장고 캐시) 혹시 DB에 save()가 안된(아예 DB가 업데이트가 되지 않은) 것은 아닐까? 장고 queryset에 캐싱이 되어있었을까? AJAX call이 비정상적으로 이루어진 것은 아닐까? 아니면, 아예 내 View 로직이 잘못된 것은 아닐까? (CBV인데?) select_related나 prefetch_related에서 캐싱이 발생하는걸까? … 이런저런 가정을 하고 하나씩 체크를 해보기로 했다. 아래부분에서는 django 로직과 관련된 삽질만 다뤘습니다. JS쪽은 문제가 없었거든요. widgets:첫번째 삽질: “브라우저가 캐싱을 하고 있는건 아닐까?”만약 브라우저가 HTML파일을 캐싱하고 있다면 캐시 삭제후 강력 새로고침을 하거나, 다른 브라우저로 접근하면 정상적인 화면이 나와야 했다. 그러나… “#망했어요” 브라우저가 캐싱하고 있는게 아니었고, 다른 브라우저에서도 기존(업데이트 전)값을 가져왔다. widgets:두번째 삽질: “장고가 template 렌더링 된것을 캐싱하는게 아닐까?”사실 장고에서 response는 따로 캐싱을 명시적으로 하지 않으면 쿼리가 새로 발생해야 하는 경우에는 캐싱을 하지 않는다. 하지만 일단 template을 재 렌더링 하지 않는게 아닐까… 하는 생각에 아래와 같은 부분을 추가해 보았다. 123{% raw %}{% for object in object_list %}{{ object }} 이건 object다 {% endfor %}{% endraw %} 역시 .. “응 아니야~ 장고 일 잘하고 있음” 템플릿은 렌더링이 충분히 잘 되고 있었다. 뭐가 문제일까? widgets:세번째 삽질: “.save() 메소드의 사용을 잘못한게 아닐까?”아예 다음번에는 DB에 저장이 되지 않고 있는게 아닌가.. 하는 생각에 save()와 update()의 사용법을 찾고, force_insert=True와 같은 옵션을 넣어보기도 했다. 123456789# view.py 파일에서... # ... for m_pay in mentor_payment_list: if str(m_pay.pk) in cleaned_keys: m_pay.status = 1 else: m_pay.status = 0 m_pay.save(update_fields=['status']) # ... .save()는 모델 인스턴스에 적용하는 케이스이고, .update()는 쿼리셋에 적용하는 방법이다. save()의 경우 모델 인스턴스를 가져오기 위해 SELECT 쿼리를 한번 날리고 값을 변경 후 UPDATE를 해주는 방법이라면, update()는 쿼리 자체를 SELECT쿼리로 날리는 방식이다. 따라서 만약의 경우 .update()를 실행 중 다른 요청에서 값이 변경되었다면 그 Transaction이 손실될 수 있고, 모델 인스턴스의 값 자체를 이용해 업데이트하는 방법은 사용하기 어렵다. (물론 사용은 가능하지만 SELECT쿼리같이 .get()으로 한번 가져와야 하기때문에 큰 의미는 없습니다. 여전히 중간에 값이 변경되었을 경우에 기존 값(get)에 대한 불가능하고요.) m_pay.save(update_fields=['status'])에서는 save()에 update_fields 리스트를 넣어주었다. 일반적인 save()함수가 인스턴스 전체를 변경하는 UPDATE문을 사용하지만 update_fields가 있는 경우에는 force_insert가 자동으로 True가 되며 동시에 해당되는 Column만 update가 일어난다. 게다가 update_fields를 넣기 전에도 이미 잘 작동하던 코드. 무엇이 문제일까? 문제는 미궁속으로.. widgets:네번째 삽질: “select_related나 prefetch_related에서 캐싱이 발생하는건 아닐까?”장고에서 select_related나 prefetch_related는 기본적으로 한번에 데이터를 가져와 queryset 자체에 캐싱을 하는 전략인데.. 혹시 여기에서 ‘과도한 캐싱’이 발생하고 있는건 아닐까? 그렇다면 장고의 캐싱을 강제로 없애는 never_cache 데코레이터를 사용하면 어떨까? 하지만 지금 뷰는 CBV니까.. @method_decorator로 never_cache를 전달해 주면 되겠다! 12345from django.views.decorators.cache import never_cache@method_decorator(never_cache, name='dispatch')class OrderMatchingList(SuperuserRequiredMixin, LoginRequiredMixin, ListView): # ... 물론, 당연히, 캐시 문제가 아니었기 때문에 안되는 것은 당연했다. widgets:다섯번째 삽질, 여섯번째, 일곱 … 그리고 더 많은 삽질 끝에서의 허무도대체 뭐가 문제인거지? ListView가 아예 문제인가? 이런 고민을 하다가 결국 django의 ListView자체를 뜯어보는데 눈에 들어오는 MultipleObjectMixin. 1234567class MultipleObjectMixin(ContextMixin): # ... queryset = None # ... def get_queryset(self): # ... 헐. queryset와 get_queryset은 다른데. widgets:해결 &amp; 평화사실 이 문제가 생긴건 DB에서 정렬하는 대신 파이썬 View에서 쿼리셋을 정렬하는 방식으로 사용하려다보니 생긴 문제였다. 모델 내부의 start_date()에 따라 정렬하는 방식을 쿼리셋 내부에서 구현이 어려워 파이썬의 sorted를 이용했는데, 이 sorted된 결과물이 queryset 변수에 담겨 새 request에도 같은 결과를 반환하게 된 것. 따라서 다음과 같이 get_queryset으로 변환해주어서 깔끔하게 해결되었다. 123456789class OrderMatchingList(ListView): class Meta: model = Order def get_queryset(self): queryset_list = Order.objects.filter(status__gte=5) \\ .select_related('education', 'region') \\ .prefetch_related('orderdetail_set') return sorted(queryset_list, key=lambda x: x.start_date()) 사실 DJDT(Django Debug Toolbar)를 사용하며 쿼리의 개수를 확인해보는데 첫 요청시에는 6개의 쿼리가 가는데 비해 두번째 요청부터는 3개의 쿼리만이 실행되고, 그마저도 데이터를 가져오는 쿼리는 없고 세션/로그인등의 비교만 쿼리를 실행하고 있다는 것을 발견해 쿼리셋쪽의 문제라는 것을 알 수 있었다. 여담문제의 코드 부분(아래)에서 select_related와 prefetch_related를 제거하면 쿼리수는 몇십개로 증가하지만 데이터 자체는 정상적으로 가져왔다. 이건 또 왜그랬을까? 123456789# 문제의 코드..class OrderMatchingList(ListView): class Meta: model = Order queryset_list = Order.objects.filter(status__gte=5) \\ .select_related('education', 'region') \\ .prefetch_related('orderdetail_set') queryset = sorted(queryset_list, key=lambda x: x.start_date())","link":"/2017/08/25/DjangoCBV-queryset-vs-get_queryset/"},{"title":"기존 DB를 Flask-SQLAlchemy ORM Model로 사용하기","text":"본 게시글에서는 MySQL/Sqlite을 예제로 하고있지만, Flask-SQLAlchemy가 지원하는 다른 DB에서도 사용 가능합니다. 들어가며Flask로 웹 개발 진행 시 SQLAlchemy(Flask-SQLAlchemy)를 사용해 ORM구조를 구성할 때 데이터를 저장할 DB의 구조를 직접 확인하며 진행하는 것은 상당히 귀찮고 어려운 일입니다. Django에는 내장된 inspectdb라는 명령어를 통해 Django와 일치하는 DB Model구조를 만들어주지만 SQLAlchemy 자체에 내장된 automap은 우리가 상상하는 모델 구조를 바로 만들어주지는 않습니다. 따라서 다른 패키지를 고려해볼 필요가 있습니다. flask-sqlacodegenflask-sqlacodegen은 기존 DB를 Flask-SQLAlchemy에서 사용하는 Model 형식으로 변환해 보여주는 패키지입니다. 기존 sqlacodegen에서 포크해 Flask-SQLAlchemy에 맞게 기본 설정이 갖추어져있어 편리합니다. 설치하기설치는 pip로 간단하게 진행해 주세요. 글쓰는 시점 최신버전은 1.1.6.1입니다. 글쓴것과 같은 버전으로 설치하려면 flask-sqlacodegen==1.1.6.1 로 설치해 주세요. 1234# 최신 버전 설치하기pip install flask-sqlacodegen# 글쓴 시점과 같게 설치하려면# pip install flask-sqlacodegen==1.1.6.1 설치가 완료되면 명령줄에서 flask-sqlacodegen라는 명령어를 사용할 수 있습니다. 주의: sqlacodegen이 이미 깔려있다면 다른 가상환경(virtuale / venv)를 만드시고 진행해 주세요. sqlacodegen이 깔려있으면 --flask이 동작하지 않습니다. DB 구조 뜯어내기flask-sqlacodegen는 sqlacodegen과 거의 동일한 문법을 사용합니다.(포크를 뜬 프로젝트니까요!) flask-sqlacodegen 명령어로 DB를 지정하면 구조를 알 수 있습니다. SQLite의 경우1flask-sqlacodegen \"sqlite:///db.sqlite3\" --flask &gt; models.py # 상대경로, 현재 위치의 db.sqlite3파일 SQLite는 로컬에 있는 DB의 위치를 지정하면 됩니다. 위 명령어를 실행하면 models.py파일 안에 db.sqlite3 DB의 모델이 정리됩니다. NOTE: Sqlite의 파일을 지정할 경우 “sqlite://“가 아닌 “sqlite:///“ 로 /를 3번 써주셔야 상대경로로 지정 가능하며, “sqlite:////“로 /를 4번 써주셔야 절대경로로 지정이 가능합니다. mysql 서버의 경우1flask-sqlacodegen \"mysql://username:password@DB_IP/DB_NAME\" --flask &gt; models.py MySQL의 경우 mysql에 접속하는 방식 그대로 사용자 이름, 비밀번호, IP(혹은 HOST도메인), DB이름을 넣어준 뒤 진행해주면 됩니다. NOTE: mysql은 “mydql://“ 로 /가 2번입니다. NOTE: mysql에 연결하려면 pip패키지 중 mysqlclient가 설치되어있어야 합니다.설치가 되어있지 않으면 아래와 같이 ModuleNotFoundError가 발생합니다. MAC에서 진행 중 혹시 mysqlclient설치 중 아래와 같은 에러가 발생한다면 아래 명령어를 실행해 xcode cli developer tool와 openssl을 설치해주신 후 mysqlclient를 설치해 주세요. 1234xcode-select --installbrew install opensslexport LIBRARY_PATH=$LIBRARY_PATH:/usr/local/opt/openssl/lib/pip install mysqlclient 실행결과아래 결과는 장고 프로젝트를 생성하고 첫 migrate를 진행할 때 생기는 예시 db.sqlite3파일을 flask-sqlacodegen을 사용한 결과입니다. Index, PK등을 잘 잡아주고 있는 모습을 볼 수 있습니다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137# models.py 파일# coding: utf-8from sqlalchemy import Boolean, Column, DateTime, ForeignKey, Index, Integer, String, Table, Textfrom sqlalchemy.orm import relationshipfrom sqlalchemy.sql.sqltypes import NullTypefrom flask_sqlalchemy import SQLAlchemydb = SQLAlchemy()class AuthGroup(db.Model): __tablename__ = 'auth_group' id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String(80), nullable=False)class AuthGroupPermission(db.Model): __tablename__ = 'auth_group_permissions' __table_args__ = ( db.Index('auth_group_permissions_group_id_permission_id_0cd325b0_uniq', 'group_id', 'permission_id'), ) id = db.Column(db.Integer, primary_key=True) group_id = db.Column(db.ForeignKey('auth_group.id'), nullable=False, index=True) permission_id = db.Column(db.ForeignKey('auth_permission.id'), nullable=False, index=True) group = db.relationship('AuthGroup', primaryjoin='AuthGroupPermission.group_id == AuthGroup.id', backref='auth_group_permissions') permission = db.relationship('AuthPermission', primaryjoin='AuthGroupPermission.permission_id == AuthPermission.id', backref='auth_group_permissions')class AuthPermission(db.Model): __tablename__ = 'auth_permission' __table_args__ = ( db.Index('auth_permission_content_type_id_codename_01ab375a_uniq', 'content_type_id', 'codename'), ) id = db.Column(db.Integer, primary_key=True) content_type_id = db.Column(db.ForeignKey('django_content_type.id'), nullable=False, index=True) codename = db.Column(db.String(100), nullable=False) name = db.Column(db.String(255), nullable=False) content_type = db.relationship('DjangoContentType', primaryjoin='AuthPermission.content_type_id == DjangoContentType.id', backref='auth_permissions')class AuthUser(db.Model): __tablename__ = 'auth_user' id = db.Column(db.Integer, primary_key=True) password = db.Column(db.String(128), nullable=False) last_login = db.Column(db.DateTime) is_superuser = db.Column(db.Boolean, nullable=False) first_name = db.Column(db.String(30), nullable=False) last_name = db.Column(db.String(30), nullable=False) email = db.Column(db.String(254), nullable=False) is_staff = db.Column(db.Boolean, nullable=False) is_active = db.Column(db.Boolean, nullable=False) date_joined = db.Column(db.DateTime, nullable=False) username = db.Column(db.String(150), nullable=False)class AuthUserGroup(db.Model): __tablename__ = 'auth_user_groups' __table_args__ = ( db.Index('auth_user_groups_user_id_group_id_94350c0c_uniq', 'user_id', 'group_id'), ) id = db.Column(db.Integer, primary_key=True) user_id = db.Column(db.ForeignKey('auth_user.id'), nullable=False, index=True) group_id = db.Column(db.ForeignKey('auth_group.id'), nullable=False, index=True) group = db.relationship('AuthGroup', primaryjoin='AuthUserGroup.group_id == AuthGroup.id', backref='auth_user_groups') user = db.relationship('AuthUser', primaryjoin='AuthUserGroup.user_id == AuthUser.id', backref='auth_user_groups')class AuthUserUserPermission(db.Model): __tablename__ = 'auth_user_user_permissions' __table_args__ = ( db.Index('auth_user_user_permissions_user_id_permission_id_14a6b632_uniq', 'user_id', 'permission_id'), ) id = db.Column(db.Integer, primary_key=True) user_id = db.Column(db.ForeignKey('auth_user.id'), nullable=False, index=True) permission_id = db.Column(db.ForeignKey('auth_permission.id'), nullable=False, index=True) permission = db.relationship('AuthPermission', primaryjoin='AuthUserUserPermission.permission_id == AuthPermission.id', backref='auth_user_user_permissions') user = db.relationship('AuthUser', primaryjoin='AuthUserUserPermission.user_id == AuthUser.id', backref='auth_user_user_permissions')class DjangoAdminLog(db.Model): __tablename__ = 'django_admin_log' id = db.Column(db.Integer, primary_key=True) object_id = db.Column(db.Text) object_repr = db.Column(db.String(200), nullable=False) action_flag = db.Column(db.Integer, nullable=False) change_message = db.Column(db.Text, nullable=False) content_type_id = db.Column(db.ForeignKey('django_content_type.id'), index=True) user_id = db.Column(db.ForeignKey('auth_user.id'), nullable=False, index=True) action_time = db.Column(db.DateTime, nullable=False) content_type = db.relationship('DjangoContentType', primaryjoin='DjangoAdminLog.content_type_id == DjangoContentType.id', backref='django_admin_logs') user = db.relationship('AuthUser', primaryjoin='DjangoAdminLog.user_id == AuthUser.id', backref='django_admin_logs')class DjangoContentType(db.Model): __tablename__ = 'django_content_type' __table_args__ = ( db.Index('django_content_type_app_label_model_76bd3d3b_uniq', 'app_label', 'model'), ) id = db.Column(db.Integer, primary_key=True) app_label = db.Column(db.String(100), nullable=False) model = db.Column(db.String(100), nullable=False)class DjangoMigration(db.Model): __tablename__ = 'django_migrations' id = db.Column(db.Integer, primary_key=True) app = db.Column(db.String(255), nullable=False) name = db.Column(db.String(255), nullable=False) applied = db.Column(db.DateTime, nullable=False)class DjangoSession(db.Model): __tablename__ = 'django_session' session_key = db.Column(db.String(40), primary_key=True) session_data = db.Column(db.Text, nullable=False) expire_date = db.Column(db.DateTime, nullable=False, index=True)t_sqlite_sequence = db.Table( 'sqlite_sequence', db.Column('name', db.NullType), db.Column('seq', db.NullType)) Flask의 app에 덧붙이기이렇게 만들어진 model은 다른 Extension과 동일하게 Flask app에 붙일 수 있습니다. app.py라는 파일을 하나 만들고 아래 내용으로 채워주세요. 1234567891011121314# app.py (models.py와 같은 위치)from flask import Flaskimport models # models.py파일을 가져옵시다.def create_app(): app = Flask(__name__) app.config['SQLALCHEMY_DATABASE_URI'] = \"mysql://username:password@DB_IP/DB_NAME\" models.db.init_app(app) return appif __name__=='__main__': app = create_app() app.run() 앞서 만들어준 models.py파일을 가져와 create_app 함수를 통해 app을 lazy_loading해주는 과정을 통해 진행해 줄 수 있습니다. 마치며기존에 사용하던 DB를 Flask와 SqlAlchemy를 통해 ORM으로 이용해 좀 더 빠른 개발이 가능하다는 것은 큰 이점입니다. ORM에서 DB 생성을 하지 않더라도 이미 있는 DB를 ORM으로 관리하고 Flask 프로젝트에 바로 가져다 쓸 수 있다는 점이 좀 더 빠른 프로젝트 진행에 도움이 될거랍니다.","link":"/2017/10/20/DB-To-SQLAlchemy-Model/"},{"title":"트위터에서 많은 팔로워를 크롤링하는 방법 [3]: 1초에 5천개 데이터 가져오기","text":"앞서 쓴 글(2편)에서는 Twint와 직접 만든 크롤러를 이용해 데이터를 수집하는 내용을 정리했다. 하지만 여전히 큰 문제가 남아있었다. “1만명까지는 어떻게든 모을 수 있다. 그런데 10만 이상의 팔로워/팔로잉을 가진 유저의 데이터는 대체 어떻게 가져와야 하나?” 여기서는 몇 가지의 이슈가 충돌했다. “여러 계정을 한번에 가져와야 한다.”: 수집해야하는 계정이 여전히 1k+개. “15분 이상의 긴 수집 시간이 필요하다.”: Lambda에서는 어렵다. “한 IP에서 너무 많은 요청을 하면 안된다.”: 한 IP에서 너무 많은 요청을 하면 걸릴 수 있다. “HTML 웹 크롤링은 간혹 일부만 수집후 종료되더라.”: 트위터에서 간혹 팔로잉/팔로워를 모두 수집하지 않았음에도 (5만명중 1만명만 수집 등) 이후 Cursor를 제공하지 않아 크롤링이 정지되는 경우가 있었다. 그러다, 이 이슈를 한번에 해결할 수 있는 방법(꼼수)를 찾게 되었다. 어떤 방법인가?꼼수아닌 꼼수는 아래와 같은 과정을 통해 발견할 수 있었다. “트위터에 로그인 한 상태로 ‘더보기’ 버튼을 누르면 무한히 내려간다. 근데 이건 ajax요청인데?” ⬇️ “어? 어떤 API를 사용하는거지?” ⬇️ “트위터 개발자 API와 같은 API네?” ⬇️ “개발자 등록을 해서 쓸 때는 15분에 15개 요청 제한인데, 지금은 100번 넘게 해도 잘 가져오네?” 개발자 등록을 해서 얻는 OAuth API Key로는 곧바로 제한이 걸려버리는데, 정작 일반 트위터 유저로 로그인한 상태에서 요청을 하는 것에는 제한이 없었던 것! Q. 그렇다면 제한/한계는 없나?A. 같은 API 엔드포인트를 사용하는 만큼, 요청 QueryString에 넣은 쿼리는 트위터 개발자용 API와 100% 호환된다. 이후 서술하겠지만, 아래의 조건을 지키면 문제가 생기지 않는다. 정상적으로 인증받은(이메일 &amp; 전화) 트위터 계정을 이용하기➡️ 인증받지 않은 계정을 하면 요청이 금방 막힌다. 한번에 하나의 요청만 진행할 것➡️ Multiprocess등을 이용해 동시 요청을 하는 순간 바로 막힌다. 즉, 1개의 PC에서 한번에 1번의 요청만 보내면 꾸준히 요청이 가능하다는 것이다. 상당히 쉬운 조건이다! 1st try: 유저들의 모든 정보를 가져와보자!1st-1) 웹 사이트를 뜯어보자 💥 Spoilers 💥 이 방법은 한번에 200개가 최대이지만, 아래 2nd try에서는 한번에 5000개를 가져옵니다. 컨셉은 “로그인이 유지된 상태의 HTTP 요청을 따라하기” 이기 때문에, 트위터에 로그인을 한 상태에서 비동기 요청을 보내는 것을 크롬으로 몰래 가져오면 된다. Followers 혹은 Following으로 들어가서 스크롤 몇 번을 해주면 스크린샷 우측과 같이 followers/list.json API로 요청하는 것을 볼 수 있다. 해당 내용을 살펴보면 아래 스크린샷과 같이 각 유저들의 디테일한 정보를 가져오는 것을 볼 수 있다. 위 정보 중에서 사실 필요한 것은 id, 그리고 screen_name 정보다. id: 트위터 각 계정의 고유한 No. (변경 불가능) screen_name: 트위터에 로그인할때 &amp; @Mention 기능을 사용할때 쓰는 아이디. (변경 가능) 그래도 다른 정보도 같이 있으면 좋긴 하니까, 모두 그대로 가져오기로 생각했다. 1st-2) HTTP Request 코드를 만들자크롬에서 HTTP 요청이 있다면 곧바로 Python의 requests 기반 요청으로 바꿀 수 있다. 위 과정을 입력하면 해당 HTTP 요청이 cURL 형식으로 복사된다. 복사한 값을 https://curl.trillworks.com/ 에 가서 그대로 붙여넣기를 해주자. 붙여넣기를 완료한 파이썬 코드는 아래와 같은 결과가 나온다. [Note] 아래 token들은 원래 각자 다른 값이 들어있다. 개인정보 보호를 위해 testToken이라는 이름으로 일괄 숨김을 진행한 것이라, 원래는 영문자와 숫자의 복잡한 값으로 이루어져있다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import requestscookies = { 'dnt': '1', 'kdt': 'testToken', 'csrf_same_site_set': '1', 'csrf_same_site': '1', 'syndication_guest_id': 'testToken', '_ga': 'testToken', 'personalization_id': 'testToken==', 'guest_id': 'v1%testToken', 'ads_prefs': 'testToken=', 'remember_checked_on': '1', 'u': 'testToken', 'auth_token': 'testToken', 'twid': 'u%testToken', 'rweb_optin': 'side_no_out', 'tfw_exp': '0', 'des_opt_in': 'N', 'night_mode': '0', 'ct0': 'testToken',}headers = { 'Connection': 'keep-alive', 'Pragma': 'no-cache', 'Cache-Control': 'no-cache', 'Origin': 'https://twitter.com', 'x-twitter-client-language': 'en', 'x-csrf-token': 'testtoken', 'authorization': 'Bearer testToken', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36', 'x-twitter-auth-type': 'OAuth2Session', 'x-twitter-active-user': 'yes', 'DNT': '1', 'Accept': '*/*', 'Sec-Fetch-Site': 'same-site', 'Sec-Fetch-Mode': 'cors', 'Referer': 'https://twitter.com/twitter/followers', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',}params = ( ('include_profile_interstitial_type', '1'), ('include_blocking', '1'), ('include_blocked_by', '1'), ('include_followed_by', '1'), ('include_want_retweets', '1'), ('include_mute_edge', '1'), ('include_can_dm', '1'), ('include_can_media_tag', '1'), ('skip_status', '1'), ('cursor', '1653632830146283795'), ('user_id', '783214'), ('count', '20'),)response = requests.get('https://api.twitter.com/1.1/followers/list.json', headers=headers, params=params, cookies=cookies)# 결과는 response.json()result = response.json()print(result) 위 코드에서 주목해야 하는 부분은 바로 params, 즉 44번째줄의 인자 부분이다. 해당 부분 값 중에서 아래의 3가지 항목이 우리가 변경해야 하는 값이다. 123('cursor', '1653632830146283795'),('user_id', '783214'),('count', '20'), cursor: 요청을 하는 요청 시점. -1 을 넣어주면 첫 요청이 되고, 1653632830146283795 처럼 숫자가 들어가있으면 DB 커서처럼 해당 부분부터 count 갯수만큼 반환된다. user_id: 해당 유저의 ID. count: 한번에 몇개의 결과를 반환할지. 이 부분은 max 200이다. 따라서, 아래와 같은 로직의 요청을 진행하면 된다. user_id 를 지정하고, count 를 200으로 설정한 뒤, cursor 를 -1 로 맞추고 요청을 시작한다. Response를 .json() 을 이용해 값을 얻어내고, 해당 값 중 cursor 정보를 얻어낸다. 결과값을 while True: 를 돌면서 cursor 값을 업데이트하면서 Response에서 cursor 가 더이상 등장하지 않을때까지 반복한다. 이정도면, 꽤 괜찮은 속도로 수집이 가능하다. 😄 1st-3) user_id 대신 트위터 ID인 screen_name 으로 가져오자하지만 위 요청에서는 한가지 한계가 있다. 바로 user_id 값을 알아내야 한다는 것. [Note] 만약 유저들의 user_id 를 알고있다면, 해당 값을 기준으로 수집하는 것이 가장 안전하고 정확하다. screen_name 은 유저들이 언제든지 변경 가능하기에, 안정적인 고유 식별자가 아니다. 우리가 보통 트위터에서 쉽게 눈으로 보고 얻을 수 있는 정보는 screen_name, 즉 @ 로 시작하는 유저의 로그인 아이디이기 때문에 해당 screen_name 값을 통해 가져온다면 좀더 편할 것 같았다. Q. 혹시 Parameter를 user_id 대신 screen_name 으로 바꿔도 동작할까? A. 잘 동작합니다! 아래와 같이 params 내용의 값을 바꾸어주면 잘 동작한다. 123('cursor', '1653632830146283795'),('screen_name', 'twitter'), # &lt;-- 이 부분이 바뀌었다!('count', '200'), 와우! 이제 수집만 하면 될 것 같았다. 하지만, 1만 팔로워 넘는 셀럽이 너무 많더라. 😨 2nd try: 팔로워 수집, ID만 가져오면 어떨까? 🎉1st try에서 진행했던 방식으로 열심히 크롤링을 하던 중, 한번의 요청에 약 1-1.5s가 걸리는 것을 지켜보면서 10만 Follow를 수집하려면 대략 500-750초가 걸리겠구나.. 하고 생각하고 있었다. 나쁘지 않은 속도, 하지만 크롤링 남은 시간이 800시간인 것을 보면 대략 정신이 멍해진다. 다른 방법을 찾아야 했다. 그러던 중, 이런 의문이 들었다. “음… 혹시 똑같은 Auth Token으로 다른 API를 쓸 수는 없을까?” 답은 놀랍게도, 가능하다! 트위터 웹에서 사용하는 /followers/list.json 뿐만 아니라 다른 API도 호출이 가능했던 것. 2nd-1) ids.json API가 있네..?새로 사용해보는 API는 Follower들의 id 만 목록으로 받아내는 API였다. 공식 문서는 ➡️ https://developer.twitter.com/en/docs/accounts-and-users/follow-search-get-users/api-reference/get-followers-ids 앞서 사용한 list.json API가 팔로워들의 모든 정보(팔로우 수, BIO, 계정명, ID, …)를 가져오는 것에 반해, ids.json API는 단순히 Id 하나만을 리스트로 툭 하고 반환할 뿐이다. 대신, 한번에 5000개의 값을 반환해준다. 🎉 하지만 나에게 필요했던 것은 단순히 팔로우-팔로잉 관계만 알아내면 되는 것이기 때문에, 다른 정보는 필요하지 않았다. 따라서 대박이다! 를 외치고 작업을 시작했다. 2nd-2) ids.json API 요청은 어떻게 하나?99% 동일하다. (사실상 같다.) 요청시 API 주소만 다르게 하고, Params만 아래와 같이 입력해주면 된다. 123456789101112131415161718# ... 생략 ...params = ( ('include_profile_interstitial_type', '1'), ('include_blocking', '1'), ('include_blocked_by', '1'), ('include_followed_by', '1'), ('include_want_retweets', '1'), ('include_mute_edge', '1'), ('include_can_dm', '1'), ('include_can_media_tag', '1'), ('skip_status', '1'), ('cursor', cursor), ('user_id', user_id), # &lt;-- screen_name으로 대체가능 ('count', '5000'), # &lt;-- max 5000)response = requests.get('https://api.twitter.com/1.1/folloewrs/ids.json', headers=headers, params=params) params의 count 를 무려 5000으로 올릴 수 있다는 것이다. 2nd-3) 모든 팔로워/팔로잉을 가져올때 까지 가져오는 함수를 만들자.앞서 진행한 로직과 같이 while True: 를 돌면서 cursor 값이 0 이 될 때까지 수집을 진행하면 된다. 아래 코드는 크게 두 함수로 만들어져있다. get_follow_ids: user_id와 username(screen_name), cursor를 넣으면 해당 요청의 결과(json)을 반환하는 함수. get_follow_all: user_id와 username(screen_name), user_follow_count(유저의 팔로우 수, Optional)를 넣으면 전체 결과를 반환하는 함수. 아래 코드 내 [TESTTOKEN] 으로 된 부분만 앞서 체크한 cURL 요청 변환에서 찾아 넣어주면, 성공적으로 요청을 진행할 수 있다. [Note] Cookie 부분은 앞서 나온 코드처럼 cookie=cookie 인자로 전달해줘도 된다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import requestsimport jsonfrom bs4 import BeautifulSoup as bsdef get_follow_ids(user_id, username, cursor=-1): headers = { 'authority': 'api.twitter.com', 'origin': 'https://twitter.com', 'x-twitter-client-language': 'ko', 'x-csrf-token': '[TESTTOKEN]', 'authorization': 'Bearer [TESTTOKEN]', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36', 'x-twitter-auth-type': 'OAuth2Session', 'x-twitter-active-user': 'yes', 'dnt': '1', 'accept': '*/*', 'sec-fetch-site': 'same-site', 'sec-fetch-mode': 'cors', 'referer': f'https://twitter.com/{username}/following', 'accept-encoding': 'gzip, deflate, br', 'accept-language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7', 'cookie': 'personalization_id=\"[TESTTOKEN]\"; guest_id=v1%[TESTTOKEN]; ct0=[TESTTOKEN]; ads_prefs=\"[TESTTOKEN]=\"; kdt=[TESTTOKEN]; remember_checked_on=1; _twitter_sess=[TESTTOKEN]; auth_token=[TESTTOKEN]; csrf_same_site_set=1; rweb_optin=side_no_out; csrf_same_site=1; twid=u%[TESTTOKEN]', } params = ( ('include_profile_interstitial_type', '1'), ('include_blocking', '1'), ('include_blocked_by', '1'), ('include_followed_by', '1'), ('include_want_retweets', '1'), ('include_mute_edge', '1'), ('include_can_dm', '1'), ('include_can_media_tag', '1'), ('skip_status', '1'), ('cursor', cursor), ('user_id', user_id), ('count', '5000'), ) response = requests.get('https://api.twitter.com/1.1/followers/ids.json', headers=headers, params=params) return response.json()def get_follow_all(user_id, username, user_follow_count=None): cursor = -1 dataset = [] while cursor != 0: # 커서가 0이면 다음페이지가 없다. data = get_follow_ids(user_id, username, cursor) try: cursor = data['next_cursor'] except KeyError as e: print(data) if data.get('error') and data['error'] == 'Not authorized.': open(f\"protected/{user_id}\", 'w').close() print(username, \"Protected\") return if data.get('errors') and data['errors'][0]['code'] == 34: open(f\"deleted/{user_id}\", 'w').close() print(username, \"Does Not exist\") return print('='*50) print('Username:', username, 'User_id', user_id) print(data) print('='*50) raise e dataset += data['ids'] print(f'{username}({user_id}): {len(set(dataset))} / {user_follow_count}', end='\\r') dataset = list(set(dataset)) json.dump(dataset, open(f'success/{user_id}.json', 'w+')) print(f'# Final for USER [{username}({user_id})]: {len(dataset)} / {user_follow_count}') 온전한 Following(팔로잉)을 가져오는 코드는 아래 Github Gist에서 찾을 수 있습니다. https://gist.github.com/Beomi/9d263bf9d1128180e1c17c1e94b0409b 이제 위 함수를 user_id 를 for loop를 돌며 실행하면 API의 응답에 따라 proetectd, deleted, success 등의 폴더에 자동으로 정리된다. (당연히 각 폴더는 미리 생성해두어야 에러가 나지 않는다.) 이번 요청 역시 하나에 1~1.5s가 걸리지만, 앞서 200개 대비 무려 25배 빨라지기 때문에 훨씬 편안한 수집이 가능하다. 맺으며트위터에서 RT가 어떻게 퍼지는지 Network Tree를 그리기 위해, 엄밀하고 정확한 수준의 팔로워-팔로잉 그래프가 필요했다. 앞선 시행착오를 통해 5k+ 유저들의 경우 이 방식으로 수집하고, 그 이하의 경우는 앞서 만든 병렬 처리 가능한 HTML 크롤러로 나머지 수집을 진행했다. 약 20만명+ 수준을 수집할 수 있었고, 실제로 5k+ 유저들을 모으는데 걸리는 시간이 그 이하를 AWS lambda를 통해 병렬수집하는 시간보다 훨씬 오래 걸렸다. 이후 id 와 screen_name 의 중복 사용 혹은 변경 등을 체크해 맞는지/아닌지를 검사해 팔로우/팔로워 수집 태스크를 마쳤다. 앞으로 트위터에서 어떻게 대응할지는 모르겠지만, API의 한계를 이런 식으로 넘을 수 있어서 정말 다행히도 프로젝트 진행이 가능했다. 😂 결론: 트위터 API는 돈주고 쓰는게 마음편하다….. 다음부터 가능하면 결제 펑펑 할 수 있기를 바라며, 글을 마친다.","link":"/2019/12/22/Crawling_Twitter_Following_3/"},{"title":"2019년 회고, 2년치 연말정산","text":"2018년에는 회고를 쓰지 않았다. 정확히 말하면.. 회고를 쓰기에 너무 많은 일이 있었기 때문이 아니었을까. 2019년의 회고글이지만, 2018년과 19년은 마치 1년처럼 끊김없이 이어진 기분이었다. 그래서 이번 회고는 2019년 회고이지만 2018년도 함께 정리해보려 한다. 👨🏻‍💻👩🏻‍💻 (대략 열심히 타이핑 중이라는 뜻) 2018년 훑어보기. 뭐했더라?2017년 말부터 2018년 초까지 패스트캠퍼스에서 웹 크롤링 강의를 진행했다. 어떻게 해야 좋은 수업을 만들 수 있을까, 어떻게 해야 보다 근본적인 이해를 하도록 만들 수 있을지 많은 고민을 하던 시기였다. 2017년부터 시작한 장고걸스 서울 운영진 활동도 만으로 1년을 넘어가며 ‘어떤 교육이 초심자를 위하는가’에 대해 수많은 고민을 하기도 했다. 한편, 2018년에는 열심히 회사를 다녔다! 넥슨 어뷰징탐지팀에서 늅늅 개발자로 AWS 서비스 이것저것 써보면서 블로그도 연초에는 상당히 많이 썼다. 그 중에서 2017년 말에 썼던 딥러닝 on Lambda 게시글: AWS Lambda에 Tensorflow/Keras 배포하기 글이 예상 외로 인기를 끌고, 이와 함께 AWS 윤석찬님의 메일을 받아 AWS Summit 2018년 4월 행사에서 커뮤니티 세션에서 발표를 진행했다. 생각보다 큰 행사였고(참가인원상), 많은 분들이 들으러와주시고 질문도 많이 해주셔서 즐거웠던 발표였다. 그리고 학교에 복학도 했고, 여러 일들이 있었다. ‘성장‘? 발표로 되돌아보자.2019년 연말인 지금에서야 돌아보는 것이지만, 생각보다 발표마다 시간이 그렇게 멀지는 않다는걸 새삼 느낀다. 파이썬 커뮤니티에서 ‘첫 발표’를 진행한 것이 2017년 5월인데, 그해 파이콘을 넘어 2018년 4월에 ‘핫한 주제’를 가지고서 발표를 했다는 것. 기분으로는 2년, 3년 지나서 발표한 듯한 엄청난 시간적 거리감을 느끼는데 실제로 살펴보니 겨우 1년도 채 안되는 시간이었다는 걸 새삼 느낀다. 정작 저 시기에는 성장이라는 키워드에 나 자신이 굉장히 집착했던 것 같다. (사실 지금도 그런 것 같기도 하다.) 굉장히 좋은 기회를 얻었지만, 내 블로그나 내 발표에서 다루는 내용을 100% 이해하고 해당수준을 훌쩍 넘은 상태로 발표하고 있는 것일까? 다른 사람들이 내가 대답할 수 없는 Deep한 질문을 던지면, 내가 대답을 못하면 어쩌지? 하는 (실제로는 아무도 그러지 않았지만) 두려움이 있었다. 사실, 2018년 4월 저때는 딥러닝의 D자도 제대로 이해하지 못하고, 굉장히 나이브하게 저런게 있다더라~하는 수준으로만 이해하고 있었다. 그럼에도 저렇게 만들 수 있다며 발표를 했고, 그때 딥러닝과 관련해서 굉장히 많은 공부를 했던 것으로 기억한다. 내가 아는 것을 넘어 발표를 했다면, 그 내용만큼 내가 공부를 하면 되니까. 아는 ‘척’이 아니라, 실제로 ‘아는 것’이 되면 되니까. (그렇게 지금은 딥딥러닝거리고 다니고 있다구) 그런 점에서 커뮤니티에서 한 발표는 날 성장시키는 것과 동시에 내가 어떻게 성장해왔는지를 아주 뚜렷하게 보여주는 역할도 맡았다. 내가 어떤 것에 관심이 있는지, 내가 무엇을 공부했는지, 그리고 무엇을 할 수 있게 되었는지. 그리고 그와 함께, 내가 어떤 사람이 되어가는지를 보여주는 창과 같은 역할을 해주었다. 완벽한 사람이 되고싶어하지만, 저런 마음이 꼭 장점만 있는 것은 아니었다. 위의 마음은 내가 굉장히 빠르게 성장할 수 있도록 미는 역할을 했다. 마치 절벽에서 밀고 살아봐아아아아아악(…)하는 수준의 부담을 주지만, 어찌되었든 익히고 학습 했기는 했다. 하지만 이건 정작 내가 가진 두려움, ‘못하면 사람들이 나를 떠나지 않을까?’ 하는 두려움을 마음 속에서 더욱 증폭시키는 역할도 함께 맡았다. 나의 이러한 면을 명시적으로 인식한 것은 리틀빅데이터#1 행사에서 발표 세션을 진행할 때였다. 내가 생각했던 것보다 내 준비가 많이 부족했고, 다른 사람들의 퀄리티와 행사 전반에 비해 내가 너무 못하고, 여기에 내가 있어도 되는걸까, 속으로 나를 비웃거나 욕하고있지는 않을까 하는 두려움이 계속 생겼다. 당장 같은 주에 다른 발표 하나를 내 생각보다 훨씬 멋지게 끝냈다고 기뻐했지만, 오히려 내가 맡기로 한 모든 것을 100%+의 모습으로 만들어내지 못한 것에 대한 좌절감이 들었다. 하지만 2019년 말, 오늘들에 생각하는 것은 공포를 기반으로 하는 성장은 오래가지 못한다는 것이다. 즐거워야, 오래할 수 있다.2018년 파이콘 발표를 준비할 때도 앞선 감정이 비슷하게 들었다. “내가 부족한 면을 보이면 어떡하지? 내가 생각한 이상적 모습은 이게 아니야… 더 노력해야하는데.” -라고 이야기 하지만 정작 내가 가진 지식과 능력으로는 이상적 모습을 만들 수 없다는 것을 아는데, 그런 수준을 만들기 위한 노력은 오히려 덜하는 모습을 보였다. 스트레스가 오히려 더 나쁜 방향으로 나를 이끌었다. 해결책은 결국은 마음을 좀 더 편하게 먹는 것이었다. 내 한계를 그저 인정을 하고, 내가 지금 생각하는 것, 그리고 지금 만들어 낼 수 있는 것을 잡고서 보다 많은 사람들이 기술의 사회적 환원을 할 수 있는 모습을 보여주자, 라는 측면으로 접근했다. 커뮤니티는 내가 잘했다고 칭찬받으려고 하는 활동이 아니라,우리가 경험한 것을 나누고 서로에게 열정을 쏟아주기 위한 곳이니까. 학교로 돌아가야하나? - 2019년 같아, 그리고 진짜 2019년.그러던 중 한가지 큰 이슈가 남아있었다. 교대에서 일반 휴학은 1회에 1년으로 제한이 걸려있던 것. 하지만 내가 휴학한 것은 2017년 2학기로, 2018년 2학기 시작때 복학을 해야 했다. “어떻게 해야하지? 학교로 돌아가야 하나..?” 이때의 고민은 사실 2019년까지도 쭉 이어져 왔다. 엄밀히는 고민이 아닌 ‘후회’라고 볼 수도 있다. ‘저때 그냥 학교를 등록’만’ 하고 회사를 계속 다닐걸, 그랬으면 어땠을까…’ 하는 아쉬움. 내가 나왔을때 선택한 것은 결국 대학원을 가즈아! 였다. 대학원을 가서, 전문연을 하는 방향을 선택하기로 한 것이다. 하지만 당연하게도 아래의 문제는 나를 따라다녔다. “그런데, 대학원 가려면 학점 좋아야 하지 않아?” 실제로 맞는 말이었다. 나는 학점이 좀 많이 안좋았다. 보통 SKP 대학원을 생각하면 4.0+/4.5의 평균 학점을 필요로 한다는 이야기가 있는 것처럼, 연구를 위해서 가는 대학원 입시에는 높은 학점을 요구받았던 것. 하지만 저런 학점에는 많이 못미치는 내 학점, 그리고 당장 교육과정을 외우고 각 과목에서 말하는 교육 모델들의 세부적인 내용 하나하나를 외우는 것에 질린 나로서는 높은 학점을 받지 못한다는 것에 대해 지속적 스트레스를 받으면서도… 여전히 학점을 높게 받지 못했다. 그러면서 나 자신에게는 ‘괜찮아, 나는 외부 활동과 내가 관심있는 분야의 활동을 더 많이 했잖아.’ 하고서 괜찮을거야, 하는 근거없는 위안을 하고 있었다. ???: “모두가 그럴듯한 계획을 가지고 있지. ㅁㅁ하기 전까지는 😋” 논문을 써보자?파이콘에서 발표를 한 것 이후로 관련해 논문을 써보려 고민을 하고 실천을 해보려 애썼던 시기가 있었다. (사실 거의 1년동안 끌었던 것 같다.) 하지만 논문의 Novelty, 그리고 기존의 연구에 대한 미지(선행연구 연구…)등으로 인해 대체 어떤 것을 연구해야 하는지 등 수많은 #망했어요 경험을 했다. 이거 괜찮은거 같은데… 해서 가보면 이미 다 된 연구거나 혹은 Selling이 되지 못할 주제 등, 다른 사람들은 엄청 멋지고 뛰어난 아이디어를 내서 탑티어에 논문을 내는데 나만 무가치한 아이디어를 내는 것 같아서 좌절하기도 했다. 여름방학에는 카이스트 DSLAB에서 여름에 연구인턴을 하면서 댓글 연구를 진행했고, 그러던 중 파이콘에서 네이버 댓글 연구 2편을 발표하기도 했다. (내 여름방학….😂) 다행히도 진행한 연구 하나가 EMNLP 워크숍 W-NUT에 실려서 다행이었다. 처음 쓴 페이퍼가 비록 워크숍이더라도 억셉되는 경험은 진짜로 잊을수 없는 경험일 것이다. (자신감이 급상승하기도 했다.) 대학원을 가자..?! - 멘붕의 길#망했어요 편 😱사실 논문 쓰기 전에 KAIST AI 대학원 입시를 진행했는데, 1차 서류전형에서 떨어졌다… 그냥 아무 생각 없이 될거야, 하는 마음을 가지고 진행했는데 막상 떨어지고 나니 허탈하고 아무것도 하고싶지 않아졌었다. 다른 곳을 준비하지도 않았고, 내 계획이 다 망가진 것 같은 기분이 들었었다. 내 노력이 모두 부정된 기분이었고, 차라리 ~~할걸 이라는 생각이 내 모든 생각을 지배했다. 차라리 개발하지 말걸, 차라리 회사를 계속 다닐걸, 차라리 책을 쓸걸, 차라리 ~~을 할걸….등등. 그리고 서류화되는 스펙의 중요성을 다시한번 체감했다. 커뮤니티에서 했던 모든 활동은 ‘증빙서류’가 없기 때문에 서류에서 나를 드러낼 수 없었고, 증빙되지 않은 것은 공허한 자기소개서의 제한된 글자의 Typo에 불과한 기분이 들었다. 사실, 그래서 파이콘 직전에 멘탈 상태가 극도로 흔들려 아무것도 못하는 것 같은 상태가 되기도 했다. 하지만 내가 하기로 했으니까, 지금까지 잘 준비해 왔으니까. 하는 마음으로 발표를 진행했다. 겉으로는 괜찮은 척, 잘 준비했고 다른 길도 열심히 알아보고 열정을 잃지 않은 척 했지만.. 그때는 진짜로 아무것도 하고싶지 않았다. 동시에 이런 속 상태와 외부 행사에서 보이는 모습의 괴리감에 더 괴로워했던 것 같다. 그때 신경써줬던 사람들 모두 고마워요 💌 #다행이다 편 🤩페친들을 포함한 지인분들은 알고있는 소식이지만, 설대 데사 대학원에 합격했다. 일단 붙고나니 마음의 안정이 찾아오더라. 무언가를 할 수 있을 것 같다는 여유가 다시 찾아오더라. 여유가 생기면 정리를 할 수 있다여유가 생기니 블로그를 갈아엎었다. Jekyll 기반 Trophy테마에서 지금의 Hexo기반으로 싹 갈아엎었다. 그리고 이런 맛과 함께 블로그 쓰는 양도 늘었다. 앞으로도 열심히 더 많이 써봐야지. 역시, 사람은 여유가 있어야 뭔가를 할 수 있다. 무엇을 할 수 있을까?지금 이 문장을 쓰는 시간은 2019년 12월 31일 11시 11분 PM이다. 올해가 벌써 50분도 채 남지 않은 시기다. 내년의 나는 과연 어떤 모습을 보여줄 수 있을까? 어떤 삶을 살아가게 될까? 30살의 나는 어떤 모습을 보이며 살아가고 있을까? 겨우 2년이란 시간 사이에 너무 많은 변화가 일어났다. 30살까지는 앞으로 4년이란 시간이 남아있는데 난 그 4년동안 어떤 일과 어떤 경험을 하고, 어떤 삶의 방향을 선택하게 될까? 현재를 사랑하고 현재를 살아갈래 😋다만, 이런 생각을 갖게 되었다. 최근에는 매일매일 ‘앞으로의 삶이 오늘만 같았으면 좋겠다, 과연 이런 행복한 시간을 또 한번 가질 수 있을까?’ 할 정도로 믿을 수 없을만큼 행복한 순간들을 경험하기도 했다. ex) 디즈니 싱어롱(by 준킴), 장고걸스 OB모임(장걸 사람들 짱죠아), 친구들과의 고기 번개모임(긱헙모이자), 어떤 말을 해도 괜찮아-라는 말(누군지 당사자는 알듯), 친구들과의 여행(이시국여행!), MT(고기꾸워먹자), 당일치기 스키모임(이번에는 신발 사이즈 맞춰가야지), 보드게임 모이기(하자요!), 금요일에 회사 반차내고 놀기(이것도 장걸쓰), 하루종일 스벅에서 모각코(라고 쓰고 수다떨기라 읽기)하기, 겨울왕국2 다시 한번 보러가기(디덕들 모여라), 좋아하는 사람들과 함께 있기(그냥 같이있기만 해도 좋다), 좋아하는 사람들의 이야기 들어주기, 이야기 들어주며 토닥토닥해주기, 무거운 주제더라도 서로의 가치를 이해하는 순간, 친구가 해준 칵테일 마시기, 같이 맛있는 치즈케이크 먹으러 가기, 같이 사진 찍으러 가기, 좋아하는 사람들 사진 예쁘게 찍어주기…. 일상적인데 이러한 일상 하나하나가 너무 소중해서 하나도 놓고싶지 않아진 마음이다. 앞으로도 이런 소중한 사람들과 함께할 수 있다면, 앞으로의 삶도 그만큼 사랑스러울 수 있을거라고 기대한다. 안녕 2018,19년, 안녕 2020년. 잘 부탁해. 그리고 이 글 읽어주신 여러분들도 고마워요. 앞으로도 잘 부탁드려요 :) widgets:Thumbnail Photo by NordWood Themes on Unsplash","link":"/2019/12/31/Postmortem-2019/"},{"title":"Pandas에서 Parquet 사용하기 with Snappy/Gzip","text":"Pandas나 PySpark등을 사용하다보면 *.csv 포맷으로는 만족하지 못하는 경우가 많다. 예를들어.. Data Type이 저장되지 않는다. 너무 많은 데이터는 저장해도 CSV의 이점(엑셀로 열어볼 수 있다)을 살리지 못한다. 특정 Column만 선택하는 것이 불가능하다. (= 전체 파일을 항상 모두 열어야 한다) 용량이 상대적으로 작지만 크다 (압축을 하지 않은 경우) (종종) Escaping이 잘 되지 않은 경우에는 파일 Parsing이 깨진다. 한글이 들어간 csv의 경우 “MS Excel”에서는 BOM이 없으면 UTF-8을 제대로 인식하지 못한다. (한편, euc-kr 인코딩은 잘 읽는다.) 등등.. 여러가지 이슈가 있다. 그렇다면, 어떤 형식을 써야 할까? 그럼 어떤 포맷을 써야 하나?원본 데이터를 곧바로 가져다 사용하는 경우(= Athena, RedShift, BigQuery, DB 등이 데이터소스가 아닌 경우)에는 보통 csv, json, parquet 이 세가지 형식이 가장 범용적이다. 그 중에서도 특히 csv와 json을 자주 사용한다. 작은 데이터의 경우 csv를, API에서는 json을 제공하는 경우가 많다. 위에서 볼 수 있듯 파일 형식에 따라 표현할 수 있는 방식이 다르다. 하지만 결론은 대부분 “Parquet 파일을 써라!” 인 경우가 많다. Snappy 압축으로 Parquet 파일 사용하기Parquet형식은 Pandas에서 기본 옵션으로 Snappy 압축을 사용한다. Snappy 압축은 google에서 개발한 ‘적당히 빠르고 적당히 압축 잘되는’ 라이브러리이고, 대용량의 데이터를 ‘빠르게 읽고 쓰는데 적합한, 하지만 용량 축소는 잘 되는’ 아주 멋진 압축 방식이다. 한편, 기본 옵션인 Snappy 압축을 이용해 parquet파일을 쓰기 위해 새로운 DataFrame을 만들어 저장하려고 해도 보통의 경우 에러에 부딛힌다. 아래와 같은 간단한 코드를 실행한다고 가정해 보자. 1234import pandas as pddf = pd.DataFrame()df.to_parquet('sample.parquet') Docker로 continuumio/anaconda3:2019.10 에서 ipython 쉘에서 실행시 아래와 같은 에러가 발생한다. 아래 에러는 Parquet 파일을 사용하기 위해서는 pyarrow 혹은 fastparquet 패키지가 필요하다는 경고다. 아래 명령어를 통해 fastparquet 패키지를 설치해주자. 1pip install fastparquet 한편, 위 명령어를 입력하면 상황에 따라 다르지만 아래와 같은 에러가 발생한다. 위 에러는 gcc 명령어를 사용할 수 없어서 생긴 문제로, 아래 명령어로 빌드 관련 패키지를 설치하면 문제를 해결할 수 있다. 12apt update &amp;&amp; apt install -y build-essentialpip install fastparquet 이제 다시 df.to_parquet() 를 실행하면 아래와 같이 정상적으로 설치가 되는 것을 볼 수 있다. 혹시 libsnappy 파일 관련해 에러가 발생하고 설치가 실패할 경우 아래 명령어로 libsnappy-dev 를 설치해준 뒤 fastparquet 설치를 다시 시도하면 성공한다. 1apt update &amp;&amp; apt install -y libsnappy-dev Gzip 압축으로 Parquet 파일 사용하기Snappy 압축이 좋기는 하지만 위와 같이 빌드 관련한 의존 패키지도 설치해야하고(꽤 무겁다), 때로는 의존성 라이브러리도 이슈가 종종 있어 사용하기 까다로운 측면이 있다. 따라서 시스템에서 보통 기본적으로 잘 지원하는 gzip 형식을 이용하기도 한다. 압축률은 gzip &gt; snappy이며, 압축 속도는 gzip &lt; snappy로 약간의 차이는 있다. 이때는 별다른 설치 없이 DataFrame 저장시에 compression 옵션만 제공하면 된다. 1234import pandas as pddf = pd.DataFrame()df.to_parquet('sample.parquet', compression='gzip') 아래와 같이 아무런 에러도 없이 한번에 저장이 잘 되는 것을 볼 수 있다. 데이터 읽기한편, 어떤 압축 방식(Gzip/Snappy/Uncompressed)을 사용하든 파일을 읽는 방식은 동일하다. 이때는 압축 방식을 알아서 유추해서 풀기 때문에 별도의 옵션을 주지 않아도 된다. 1df = pd.read_parquet('sample.parquet') 또한, 특정 컬럼만 읽으려고 한다면 아래와 같이 columns 인자를 전달하면 파일 전체 대신 해당 컬럼만 읽어서 DataFrame을 생성한다. 1df = pd.read_parquet('sample.parquet', columns=['a', 'b'])","link":"/2020/01/29/Use-parquet-on-pandas/"},{"title":"AWS SSM로 VPN없이 Private 자원 접근하기","text":"AWS EC2 혹은 RDS를 사용할때 가장 편리한 방법은 EIP를 부여받아 고정 IP를 할당한 뒤 직접 SSH등으로 접속해 제어하는 방법이다. 하지만 이러한 방법은 보안상 취약하기 때문에 DB등 중요한 데이터가 들어있는 자원은 외부 IP를 부여하지 않고 사용하게 된다. 하지만 이렇게 될 경우, “VPC 내부에서만 액세스 가능하다”는 문제가 생긴다. 즉, AWS의 같은 VPC, 같은 내부 IP를 받은 자원에서만 접근이 가능하다는 점이다. 공개 ip가 부여되지 않기 때문에 인터넷에서 직접적으로 요청을 받는 것이 불가능하다. 이를 해결하기 위해서는.. ❌ AWS에 VPN 서버를 세팅하고 VPN에 연결해 작업한다.👉 VPN 서버를 따로 관리해야하는 어려움이 있다. ❌ AWS Client VPN 서비스(AWS Managed)를 사용한다.👉 요금이 비싸다. (최소 월 $70+) 🤩 AWS SSM + Bastion EC2 + SSH 터널링을 이용한다.👉 이번 글에서 다루는 내용! EC2 비용 하나로 서버 세팅 없이 간단히 접속하기 이번 글에서는 3번 방법을 이용해 SSM으로 SSH를 대체하고, SSM 연결을 SSH 세션처럼 이용해 SSH 터널링을 이용해 내부 RDS에 접근하는 과정을 다뤄본다. 0. AWS SSM? AWS SSM은 VPN을 이용하지 않고서 안전하게 AWS 자원에 접근할 수 있도록 만들어주는 명령어다. EC2 등 인스턴스들에 연결된 Security Group에서는 SSH 서비스를 위한 22번 포트를 열지 않아도 되며, 개발자들이 EC2에 접근하기 위해 키 파일을 공유하지 않아도 되며, AWS에 생성한 유저의 접근권한 조절을 통해 어떤 EC2에 로그인 할 수 있는지도 제어할 수 있다. 또한, SSH Session 기능을 통해 일반적인 SSH의 Remote Proxy를 이용할 수도 있다. @2020. 02. 14. SSM은 Bastion 인스턴스 없이 사용할 수 있도록 만들어졌지만, 아직 localhost 에 대해서만 SSH Forwarding을 지원하기 때문에 RDS 등 다른 인스턴스에 접근하려면 Bastion EC2를 생성해야 한다. 이후 업데이트로 추가되면 EC2조차도 필요 없어지면 더 사용성이 높아질 듯 하다. 1. AWS EC2에 SSM 세팅하기Security Groups 설정하기AWS 내 자원끼리 통신하기 위해서는 여러 설정이 필요하지만, 만약 default vpc를 사용한다면 기본적으로 내부 자원끼리 통신하도록 Route Table이 설정되어있다. 이와 함께 해당 자원들이 있는 Security Group에 대해 같은 Security Group 내의 모든 트래픽을 열어줄 수 있다. 모든 트래픽이 아닌 특정 프로토콜과 포트만을 열어서 사용할 수도 있다. 또한, 이후 Bastion으로 사용하는 EC2를 소스로 지정해 해당 EC2에서만 접근 가능하도록 설정할 수도 있다. IAM EC2 역할 설정하기 위 스크린샷처럼 AWS IAM에서 새 역할 생성을 한 뒤, AWS 서비스 - EC2 를 선택한다. 정책에서 AmazonSSMManagedInstanceCore 를 검색한 뒤 체크박스를 클릭해 권한 정책을 연결한다. 태그 추가는 옵션이니 빈칸으로 두고 넘어가도 된다. 역할 이름은 적당한 이름으로 붙이고 ‘역할 만들기’ 버튼을 누르면 역할이 생성된다. EC2 생성하기이제 위에서 만들어준 Security Group과 IAM Role을 가진 EC2 인스턴스를 생성해보자. 인스턴스 종류는 AmazonLinux2, t3.nano를 이용한다. 사실 아마존에서 제공하는 모든 AMI에는 SSM을 위한 세팅이 이미 되어있다.따라서 개발자에게 편리한 OS를 선택하면 된다. 우리가 사용하는 인스턴스는 이후 RDS등과 연결하기 위한 내부 네트워크를 제공하기 때문에, 네트워크 성능이 적절히 나오며 저렴한 EC2를 선택해주면 된다. 이번 글에서는 저렴한 t3a.nano 인스턴스를 이용해본다. ❗️(중요) 인스턴스를 선택한 뒤, 해당 인스턴스의 IAM 역할을 지정해줘야 한다. 앞서 만들어준 EC2-SSM-Bastion 역할을 선택해 준 뒤, 다음을 눌러준다. 스토리지 설정과 태그는 기본값 그대로 넘기면 된다. 이후 Security Group 설정에서 ‘기존 보안그룹 선택’을 고른 뒤, 앞서 만든 ssmProxyAllow 보안그룹을 선택한 뒤 인스턴스를 시작해준다. RDS 보안그룹 바꾸기기존에 사용하던 Private Subnet의 RDS 혹은 Aurora Serverless DB가 있다면 해당하는 자원들의 Security Groups에 ssmProxyAllow 그룹을 추가한다. ‘퍼블릭 액세스 가능성’이 ‘아니오’인 경우에는 엔드포인트 역시 내부 IP를 반환합니다. 사용자 IAM에 권한 추가하기유저가 만약 Admin 권한이 아니라면 아래 권한을 추가해줘야 SSM을 통해 SSH 세션등을 열고 프록시를 쓸 수 있다. 만약 특정 인스턴스만 지정하고 싶다면 아래 json의 10,11번째에서 * 대신 인스턴스 명을 지정해주면 된다. 123456789101112131415161718192021222324252627{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ssm:StartSession\" ], \"Resource\": [ \"arn:aws:ec2:*:*:instance/*\", \"arn:aws:ssm:*:*:managed-instance/*\", \"arn:aws:ssm:*:*:document/AWS-RunShellScript\", \"arn:aws:ssm:*:*:document/AWS-StartPortForwardingSession\", \"arn:aws:ssm:*:*:document/AWS-StartSSHSession\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"ssm:TerminateSession\" ], \"Resource\": [ \"arn:aws:ssm:*:*:session/${aws:username}-*\" ] } ]} 2. 로컬 PC에 SSM Client 설치하기 - macOS 기준AWSCLI 설치하기1brew install awscli aws 명령어를 사용하기 위해 awscli 를 설치해준다. 첫 설치인 경우 아래 명령어로 AWS AccessKey와 SecretKey를 등록해줍니다. (해당 유저는 당연히 앞서 설정한 SSM 권한을 가진 유저여야 합니다.) 1aws configure AWS SSM Extension 설치하기12brew tap dkanejs/aws-session-manager-pluginbrew install aws-session-manager-plugin 위 과정을 진행하면 aws ssm 명령어를 사용할 수 있다. 3. 로컬 SSH Config 수정하기 ssh 인스턴스-id 명령어를 통해 SSH에 접근하기 위해서는 아래 부분을 ~/.ssh/config 파일 가장 아래에 붙여넣으면 된다. User와 IdentityFile은 적절한 유저명과 파일 위치 절대경로로 설정해주면 된다. 1234Host i-* mi-* ProxyCommand sh -c \"aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters 'portNumber=%p'\" User ec2-user # 혹은 ubuntu 등, 실제 유저 이름 IdentityFile ~/.keys/키파일이름.pem 이후 위 스크린샷처럼 ssh 인스턴스명 으로 해당 인스턴스에 SSH로 접근할 수 있다. 4. SSH 터널링으로 Private RDS 연결하기3번과 같이 ssh 인스턴스명 으로 Bastion EC2에 접근할 수 있게 되면 SSH 터널링을 이용해 내부 IP를 가진 RDS에 접속할 수 있다. 아래 명령어를 입력하면 127.0.0.1:3306 에 database-1.crp78xs4s1n9.ap-northeast-2.rds.amazonaws.com:3306 가 연결된다. 즉, DB연결 툴에서 127.0.0.1:3306 으로 접속하면 위 RDS에 접근할 수 있다. 1ssh -L 127.0.0.1:3306:database-1.crp78xs4s1n9.ap-northeast-2.rds.amazonaws.com:3306 i-0f067ac2deb58ee88 References Blogs AWS SSM으로 EC2 인스턴스에 접근하기 (SSH 대체) Configuring SSM Agent on an Amazon Lightsail Instance Official Docs AWS CLI용 Session Manager Plugin 설치 Session Manager에 대한 IAM 정책의 추가 샘플","link":"/2020/02/13/AWS-SSM-with-Bastion/"},{"title":"데이터 분석 파이프라인 풀세트, HopsWorks 설치기","text":"성윤님의 블로그에서 Machine Learning의 Feature Store란? 글을 보고서 hopsworks 라는 제품에 관심을 갖게 되었다. 과연 어떤 기능을 갖고 있는 것일까? 내가 생각하는 것 처럼 데이터 버저닝부터 모델링 버전까지 모두 관리해주는 좋은 ‘실험실’을 구축해줄 수 있는 것일까? 라는 호기심에 설치를 시도해보었다. 로컬 기기(Ubuntu/Debian)에 설치하기 마침 집에서 돌리고 있는 딥러닝용 데스크탑이 있어 거기에 올려보면 어떨까? 하는 마음에 해당 기기에 설치를 진행했다. (공식 문서에서는 Ubuntu 18.04 LTS에서 테스트를 진행했다고 한다.) 설치는 무척 쉽다. 링크에 있는 simplesetup.sh 파일을 받은 뒤 실행하면 된다. 123wget https://hopsworks.readthedocs.io/en/latest/_downloads/e150a261128e5d4a0c804611e116503c/simplesetup.shchmod +x simplesetup.sh./simplesetup.sh --install-deps 한가지 유의할 점은 공식 docs에서는 sh파일만 실행하면 된다고 하지만 실제로는 --install-deps 옵션을 꼭 붙혀줘야 apt-get 을 이용한 의존성 패키지를 제대로 설치해준다. 에러 “Vagrant: * Unknown configuration section ‘disksize’”를 만나다 신나게 VirtualBox와 Vagrant등을 설치하고 Github에서 여러 스토리보드 레포를 다운받으며 무언가 만드는 것을 진행하지만, 그러던 중 문제가 발생하는 것을 볼 수 있다. 바로 Vagrant의 Unknown configuration section 'disksize' 에러. 해당 에러를 어디서 해결해야하나, 하고 구글링을 한 결과, StackOverflow의 글(링크)에서 해답을 찾았다. 아래 명령어를 로컬 터미널에 입력해 vagrant-disksize 패키지를 설치해주면 문제가 해결된다. (Host os 터미널에서 실행하면 된다.) 1vagrant plugin install vagrant-disksize 하지만…놀랍게도 1시간 10분이나 걸린 작업이 Fail로 끝이 났다. 한편, 해당 이슈는 TimeOut으로 보여 다시 실행을 하니 정상적으로 실행되는 것을 볼 수 있었다. 매번 Chef에서 새로운 업데이트가 있는지 github에서 fetch를 진행한 뒤 추가 작업을 진행하다 보니 더 느려지는 경향이 있는 듯 했다. 또한, 위 공식 문서 “Single Machine Installation”에서는 VirtualBox를 이용해 VM을 띄워 각각을 Driver / Worker처럼 제공하다 보니 시스템 자원을 오히려 많이 잡아먹어서 ‘유용하지는 않은데…’ 하는 생각이 들었다. 실제 사용을 해보다 on AWS 한편, 해당 설치과정에서 “1시간 넘게 걸리니 기다리세요~” 하는 말이 있어서 위 에러가 났음에도 에러가 원인이라고 생각하지 않고서 (멍청하게) 마냥 기다리던 중, 링크에서 Amazon AWS에는 사전 설정된 AMI(디스크 이미지)를 지원해준다는 것을 보고서 해당 이미지로 먼저 시도해보기로 했다. 공식 문서에 설명대로 Hopsworks 최신 버전의 AMI를 선택해 EC2 인스턴스를 띄워보았다. 다만, 현재 해당 이미지는 London리전과 Ohio리전에만 지원을 해, 테스트 할 때는 Ohio 리전에서 테스트를 해 보았다. 보다 저렴한 가격에 테스트를 해보기 위해 아래와 같이 스팟 요청으로 t3.xlarge 인스턴스를 띄웠다. HopsWorks에서는 원활한 서비스 자원 이용을 위해서는 최소 16GB가 넘는 메모리를 가진 인스턴스를 사용하도록 권장한다. 또한 해당 인스턴스의 보안그룹에서 22번 포트(ssh)와 443 포트(https)는 기본적으로 열려있어야 이용이 가능하다. 띄워보기인스턴스를 잘 띄우고 SSH로 해당 인스턴스에 접속을 한 뒤, 아래 스크립트 실행을 통해 서버를 띄워야 한다. 1./start-services.sh 위 파일은 ubuntu 유저의 홈 디렉토리에 곧바로 저장되어있고, 해당 파일을 실행하면 다음과 같이 email주소를 요청한 뒤 입력을 하면 아래 스크린샷처럼 IP부터 시작해 여러 세부 서비스들을 한번에 모두 띄워준다. 모두 끝나면 https://&lt;public_ip&gt;/hopswork 주소에 접속하면 된다. 아래의 username와 pw를 이용해 로그인할 수 있다. NOTE: http 가 아닌 https 로만 동작한다. username : admin@hopsworks.ai password : admin 살펴보기실제로 서버를 띄운 뒤 로그인을 진행하고 샘플 프로젝트에 들어가보니 다음과 같이 Jupyter Lab을 이용할 수 있었다. 아래 스크린샷 우측의 ‘Open Jupyter in a new Tab’ 기능으로 JupyterLab에 곧장 접근이 가능했다. 최근 자주 사용하는 라이브러리인 PyTorch를 비롯해 Pandas, sklearn등 데이터 분석을 위해 자주 쓰이는 패키지들이 이미 설치되어있었다. 또한 딥러닝 패키지들과 함께 PySpark 세션을 통해 Worker Node에 딥러닝 학습 및 실험을 버저닝과 함께 맡길 수도 있었다. 한편, 테스트를 진행하다 아래와 같은 질문이 들었다. 위 서버를 띄운 것은 t3.xlarge로 CPU서버이고, CPU 서버에서는 당연히 CUDA가 지원되지는 않는다.(애초에 그래픽카드가 없으니까!) “그렇다면 GPU 인스턴스에서는 곧바로 GPU를 사용할 수 있을까?” GPU 인스턴스에서 CUDA가속이 바로 되는지 확인하기결과부터 말하면, NO. GPU인스턴스인 g3s.xlarge 와 p2.xlarge 를 이용해보았지만 둘 다 torch.cuda.is_available() 명령어에 False 를 반환했다. 그 외에도 당장 AMI에서 nvidia-smi 를 입력했을 때, 설치조차 되어있지 않았다는 것을 알 수 있었다. GPU 사용 공식 Docs 링크 에서는 GPU Layer Interface를 설치해야 한다고 한다. 한편, apt로 maven을 설치하고 아래 명령어들을 따라 실행했지만 실제로 GPU를 전달하고 있는지는 확인할 수 없었다. (추가적 연구가 필요할 듯 하다.) 123456789sudo apt install -y mavencd ~git clone https://github.com/hopshadoop/hops-gpu-management.gitcd hops-gpu-management &amp;&amp; mvn install -DskipTestscd ~git clone https://github.com/hopshadoop/hops-gpu-management-impl-nvidia.gitcd hops-gpu-management-impl-nvidiamvn install -DskipTests 다시 HopsWorks 세션에 들어가 PyTorch에서 Cuda 가속이 되는지 확인해보았지만, 되지 않았다. 추가적으로 확인해 보기 위해 Nvidia Driver를 설치해 보았다. (nvidia-smi 명령어를 사용해보기 위해) 123wget http://kr.download.nvidia.com/tesla/418.116.00/NVIDIA-Linux-x86_64-418.116.00.runchmod +x NVIDIA-Linux-x86_64-418.116.00.runsudo ./NVIDIA-Linux-x86_64-418.116.00.run nvidia-smi 명령어도 잘 먹히고 cuda도 잘 잡혔지만, 여전히 HopsWorks 설정에서는 잘 나타나지 않았다. ㅠㅠ 총평뭔가 기능이 많고 ‘잘 만든’ 느낌이 강하긴 하지만, 설치에 대한 문서화가 아쉬운 상태다. “Data Scientist들, 환경에 대해 걱정하지 말고 개발해!” 라는 철학이 드러나기는 하지만 세팅을 누군가가 빡세게 해줘야 한다는 점은 분명해보였다. 사실은 GPU 가속만 잘 되면 좋을 것 같은데, 해당 부분이 안되어서 어떻게 해야할지 방법을 찾아보는 중이다. 공식 소개 영상에서는 Cluster에 있는 GPU 자원도 워크스페이스별로 N개를 할당해 사용할 수 있도록 제공하고 있어 이게 가능하다면 보다 편리한 병렬 실험이 가능해보이기는 하다.","link":"/2020/02/03/Lookup-HopsWorks/"},{"title":"Colab에서 TPU로 BERT 처음부터 학습시키기 - Tensorflow/Google ver.","text":"2018년말부터 현재까지 NLP 연구에서 BERT는 여전히 압도적인 위치를 차지하고 있다. 한편, BERT모델을 사용하는 이유 중 가장 큰 것 하나가 바로 한국어로 Pretrained된 모델이 있다는 점이다. Google에서 논문을 처음 공개했을 때 Multilingual pretrained model을 공개해 Fine-tuning만으로도 우리가 필요한 데이터셋에 맞춰 분류기를 만드는 등의 여러 응용이 가능하고, 동시에 높은 성능을 보여주었기 때문에 BERT 자체를 학습시키는 것에 대해서는 크게 신경쓰지 않은 것이 사실이다. 한편 작년 ETRI의 한국어 BERT 언어모델, 그리고 SKTBrain의 KoBERT 등 한국어 데이터셋으로 학습시킨 모델들이 등장했고, 이런 모델들을 Fine-tuning할 경우 기존 구글의 다국어 모델을 사용한 것보다 성능이 조금이라도 더 잘 나오기도 한다. (특히 정제되지 않은 글에 대해 좀 더 나은 성능을 보여줬다. OOV문제가 덜한 편이었다.) 다만 이런 모델들 역시 굉장히 ‘보편적’ 글로 학습된 것이라 도메인 특화된 분야에 있어서는 성능이 잘 나오지 않을 수도 있다. 따라서 특수한 경우의 특수한 도메인에 최적화된 Pretrained model을 만든다면 우리의 NLP 모델도 좀 더 성능이 좋아질 수 있다! 이번 글에서는 BERT 모델을 TPU와 Tensorflow를 이용해 처음부터 학습시켜보는 과정을 다뤄본다. 이번 글은 Colab Notebook: Pre-training BERT from scratch with cloud TPU를 기반으로 작성되었습니다. 어떤 환경에서 개발하나?2020년 2월 26일자 기준 Google Colab에서 TPU를 활성화 시킨 상태에서 정상적으로 학습이 가능하다. 단, GCP 서비스 중 Cloud Bucket을 사용하기 때문에 활성화된 GCP 계정이 필요하다. (가입하면 1년 쓸 수 있는 $300을 준다!) 준비물 Google 계정 &amp; 구글 Colab GCP Storage Bucket 필요한 라이브러리 설치하기BERT를 학습시키기 위해서는 Tokenized된 데이터를 넣어줘야 한다. 이때 우리는 토크나이저로 sentencepiece 를 주로 사용한다. 12!pip install sentencepiece!git clone https://github.com/google-research/bert sentencepiece 대신 konlpy 등을 사용할 수 있습니다. 또한, 구글 리서치에서 공식적으로 제공하는 BERT Repo를 받아서 쓰면 모델과 최적화 등을 곧바로 가져와 사용할 수 있다. 필요한 패키지 가져오기tensorflow, sentencepiece등을 가져오고 bert 레포에서 모델 등을 가져온다. 12345678910111213141516171819import osimport sysimport jsonimport nltkimport randomimport loggingimport tensorflow as tfimport sentencepiece as spmfrom glob import globfrom google.colab import auth, drivefrom tensorflow.keras.utils import Progbarsys.path.append(\"bert\")from bert import modeling, optimization, tokenizationfrom bert.run_pretraining import input_fn_builder, model_fn_builderauth.authenticate_user() 마지막 줄(19번째 줄)에서는 GCP 계정으로 구글에 로그인 한 뒤 나온 토큰 값을 입력하면 구글드라이브에 대한 접근과 GCS 버킷에 대한 접근을 허용해줄 수 있다. 이 부분은 이후 Tensorflow에서 TPU를 접근할 때 GCS에 있는 자원에만 접근이 가능하기 때문에 모델 파일과 데이터셋을 GCS 버킷에 업로드할 때 필요하다. 만약 이부분이 진행되지 않으면 TPU에서는 [local] 파일 시스템에 접근할 수 없다는 NotImplementedError가 발생한다. 한편, PyTorch/XLA에서는 TPU 디바이스를 로컬 GPU처럼 간편하게 연결해서 Tensor 객체를 자유롭게 주고받던데, 어떤 방식으로 구현했는지 의문이다. TPU 위치 찾기Colab에서 TPU를 활성화시키면 os.environ['COLAB_TPU_ADDR'] 이라는 시스템 환경변수에 GRPC로 통신가능한 로컬 IP를 얻을 수 있다. 아래 코드를 통해 학습 과정을 로깅하는 것과 함께 TPU에 연결하는 것을 설정해줄 수 있다. 1234567891011121314151617181920212223242526# configure logginglog = logging.getLogger('tensorflow')log.setLevel(logging.INFO)# create formatter and add it to the handlersformatter = logging.Formatter('%(asctime)s : %(message)s')sh = logging.StreamHandler()sh.setLevel(logging.INFO)sh.setFormatter(formatter)log.handlers = [sh]if 'COLAB_TPU_ADDR' in os.environ: log.info(\"Using TPU runtime\") USE_TPU = True TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR'] with tf.Session(TPU_ADDRESS) as session: log.info('TPU address is ' + TPU_ADDRESS) # Upload credentials to TPU. with open('/content/adc.json', 'r') as f: auth_info = json.load(f) tf.contrib.cloud.configure_gcs(session, credentials=auth_info) else: log.warning('Not connected to TPU runtime') USE_TPU = False 위에서 설정한 과정은 /content/adc.json 파일에 저장되고, 이 파일 설정으로 TPU를 사용하게 된다. 데이터 다운받기학습하는 데이터를 모으는 것은 사실 학습과 별개로 굉장히 중요한 부분이다. 이번에는 간단하게 OpenSubtitles 데이터셋을 이용해 한글 데이터를 다운받아보자. 1234567891011121314151617AVAILABLE = {'af','ar','bg','bn','br','bs','ca','cs', 'da','de','el','en','eo','es','et','eu', 'fa','fi','fr','gl','he','hi','hr','hu', 'hy','id','is','it','ja','ka','kk','ko', 'lt','lv','mk','ml','ms','nl','no','pl', 'pt','pt_br','ro','ru','si','sk','sl','sq', 'sr','sv','ta','te','th','tl','tr','uk', 'ur','vi','ze_en','ze_zh','zh','zh_cn', 'zh_en','zh_tw','zh_zh'}LANG_CODE = \"ko\" #@param {type:\"string\"}assert LANG_CODE in AVAILABLE, \"Invalid language code selected\"!wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.'$LANG_CODE'.gz -O dataset.txt.gz!gzip -d dataset.txt.gz!tail dataset.txt 11번째줄의 “LANG_CODE”를 변경해주면 원하는 언어의 데이터셋을 받을 수 있다. 한글 데이터셋의 경우 압축된 상태 기준으로 약 8MB의 데이터셋이다. (옵션) 일부만 사용해 학습하기데이터셋 전체를 사용해 학습하면 학습시간이 굉장히 오래 걸린다. 만약 실제 모델을 얻고싶은 것이 아니라 단순히 학습 가능한지만 알고 싶다면 아래 첫 줄에서 DEMO_MODE=True 로 설정해주면 수량을 100만개 데이터로만 학습한다. 123456789DEMO_MODE = True #@param {type:\"boolean\"}if DEMO_MODE: CORPUS_SIZE = 1000000else: CORPUS_SIZE = 100000000 #@param {type: \"integer\"} !(head -n $CORPUS_SIZE dataset.txt) &gt; subdataset.txt!mv subdataset.txt dataset.txt 텍스트 데이터 전처리하기텍스트 데이터에서 많이 쓰는 문장부호나 기타 이모티콘🤩등을 학습에서 제거할지, 제거하지 않을지는 우리가 학습시키는 모델이 어떤 목적이냐에 따라 달라진다. 위와 같은 이모티콘은 사용 빈도가 낮은 편이기 때문에 위 이모티콘을 임베딩에 포함시킬 경우 Vocab의 용량이 굉장히 커지게 되어 보편적인 Language Model을 만들기 위해서는 보통 특수문자나 이모티콘 등을 제거해준다. 1234567891011121314151617regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")def normalize_text(text): # lowercase text text = str(text).lower() # remove non-UTF text = text.encode(\"utf-8\", \"ignore\").decode() # remove punktuation symbols text = \" \".join(regex_tokenizer.tokenize(text)) return textdef count_lines(filename): count = 0 with open(filename) as fi: for line in fi: count += 1 return count 1234567891011121314RAW_DATA_FPATH = \"dataset.txt\" #@param {type: \"string\"}PRC_DATA_FPATH = \"proc_dataset.txt\" #@param {type: \"string\"}# apply normalization to the dataset# this will take a minute or twototal_lines = count_lines(RAW_DATA_FPATH)bar = Progbar(total_lines)with open(RAW_DATA_FPATH,encoding=\"utf-8\") as fi: with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo: for l in fi: fo.write(normalize_text(l)+\"\\n\") bar.add(1) 데이터셋 텍스트 토크나이징하기BERT등 NLP 모델을 학습시킬때는 토크나이징한 Vocab의 크기를 적절히 제한하는 것이 모델의 성능을 높이는데 도움이 된다. (용량도 역시) 큰 모델일수록 Vocab의 크기도 커지지만, 보통의 경우는 3만개 내외의 Vocab을 만드는 것으로 보인다. 12345678910111213MODEL_PREFIX = \"tokenizer\" #@param {type: \"string\"}VOC_SIZE = 32000 #@param {type:\"integer\"}SUBSAMPLE_SIZE = 12800000 #@param {type:\"integer\"}NUM_PLACEHOLDERS = 256 #@param {type:\"integer\"}SPM_COMMAND = ('--input={} --model_prefix={} ' '--vocab_size={} --input_sentence_size={} ' '--shuffle_input_sentence=true ' '--bos_id=-1 --eos_id=-1').format( PRC_DATA_FPATH, MODEL_PREFIX, VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)spm.SentencePieceTrainer.Train(SPM_COMMAND) 위 코드를 실행하면 SentencePiece에서 해당 모델을 열심히 잘라가며 Vocab을 생성하고, 이후 텍스트를 자르기 위한 Tokenizer를 학습한다. 학습된 Sentencepiece Vocab을 로딩해주자. 123456789101112def read_sentencepiece_vocab(filepath): voc = [] with open(filepath, encoding='utf-8') as fi: for line in fi: voc.append(line.split(\"\\t\")[0]) # skip the first &lt;unk&gt; token voc = voc[1:] return vocsnt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))print(\"Learnt vocab size: {}\".format(len(snt_vocab)))print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10))) 위 SentencePiece를 통해 학습한 Vocab을 BERT가 이해하는 형태로 바꿔주기 위해서는 _로 시작한 토큰들을 ## 으로 시작하도록 바꿔주면 되고, [&quot;[PAD]&quot;,&quot;[UNK]&quot;,&quot;[CLS]&quot;,&quot;[SEP]&quot;,&quot;[MASK]&quot;]의 경우는 BERT에서 사용하는 특수 토큰이기 때문에 해당 토큰에 대한 정보들을 추가해 최종적인 bert_vocab을 만들어준다. 123456789def parse_sentencepiece_token(token): if token.startswith(\"▁\"): return token[1:] else: return \"##\" + tokenbert_vocab = list(map(parse_sentencepiece_token, snt_vocab))ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]bert_vocab = ctrl_symbols + bert_vocab 마지막으로 앞서서 사용하지 않은 vocab range에 있는 것들을 넣어 bert_vocab의 크기를 앞서 지정한 VOC_SIZE에 맞춰준다. 12bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]print(len(bert_vocab)) 이 과정이 마무리되면 vocab.txt 텍스트 파일에 위 토큰들을 모두 한줄 한줄 저장해주면 이후에 사용할 것들이 끝나게 된다. 12345VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}with open(VOC_FNAME, \"w\") as fo: for token in bert_vocab: fo.write(token+\"\\n\") 학습 데이터 쪼개기학습 데이터의 크기가 굉장히 클 수 있기 때문에 학습 원천 데이터를 적당한 사이즈로 잘라준다. 123!mkdir ./shards!split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_!ls ./shards/ BERT Pretraining을 위한 데이터 변수 설정하기BERT를 위한 데이터를 준비하는 과정에 있어서 몇가지 설정해줘야 하는 것들이 있다. MAX_SEQ_LENGTH: BERT의 모델 입력의 최장 토큰 길이 이 이상으로는 BERT모델이 이해하지 못한다. MASKED_LM_PROB: BERT의 학습 중 Masked LM의 비율을 조정한다. MAX_PREDICTIONS: Sequence별 예측할 최대 길이 DO_LOWER_CASE: 영문자를 lower(소문자화) 할 지. 한글에는 의미없다. PROCESSES: 전처리할때 CPU 몇개 쓸지 PRETRAINING_DIR: 프리트레인 데이터 폴더 이름 123456MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}MASKED_LM_PROB = 0.15 #@paramMAX_PREDICTIONS = 20 #@param {type:\"integer\"}DO_LOWER_CASE = True #@param {type:\"boolean\"}PROCESSES = 4 #@param {type:\"integer\"}PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"} 위ㅏ 같이 설정을 진행한 뒤, 아래 코드를 실행하면 Pretraining Data가 만들어진다. 12345678910111213141516171819XARGS_CMD = (\"ls ./shards/ | \" \"xargs -n 1 -P {} -I{} \" \"python3 bert/create_pretraining_data.py \" \"--input_file=./shards/{} \" \"--output_file={}/{}.tfrecord \" \"--vocab_file={} \" \"--do_lower_case={} \" \"--max_predictions_per_seq={} \" \"--max_seq_length={} \" \"--masked_lm_prob={} \" \"--random_seed=34 \" \"--dupe_factor=5\")XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', VOC_FNAME, DO_LOWER_CASE, MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)tf.gfile.MkDir(PRETRAINING_DIR)!$XARGS_CMD GCP 버킷에 모델 &amp; 학습 데이터 올리기Tensorflow를 통해 TPU로 학습을 진행하려면 앞서 언급한 것과 같이 GCS에 데이터와 모델을 업로드해야 한다. 첫째 줄의 BUCKET_NAME을 개개인의 GCS 버킷이름으로 수정하면 된다. 123BUCKET_NAME = \"이부분을_수정해_주세요\" #@param {type:\"string\"}MODEL_DIR = \"bert_model\" #@param {type:\"string\"}tf.gfile.MkDir(MODEL_DIR) BERT Model Hyper Parameters 설정하기BERT 모델을 어떤 구조의 모델을 사용할지에 대한 Hyper Parameters를 설정해야 한다. 얼마나 Dropout을 해줄지, Bidirectional하게 할지, Activation Function을 뭘로 해줄지, Hidden Size를 얼마나 해줄지, Attention Head를 몇개로 해줄지, 레이어를 몇 층으로 쌓을 지 등등… 아래 설정은 BERT Base 모델의 기본값이다. 아래 값을 수정하면 좀 더 다르게 학습된 BERT 모델을 만들 수 있게 된다. 1234567891011121314151617181920212223242526bert_base_config = { \"attention_probs_dropout_prob\": 0.1, \"directionality\": \"bidi\", \"hidden_act\": \"gelu\", \"hidden_dropout_prob\": 0.1, \"hidden_size\": 768, \"initializer_range\": 0.02, \"intermediate_size\": 3072, \"max_position_embeddings\": 512, \"num_attention_heads\": 12, \"num_hidden_layers\": 12, \"pooler_fc_size\": 768, \"pooler_num_attention_heads\": 12, \"pooler_num_fc_layers\": 3, \"pooler_size_per_head\": 128, \"pooler_type\": \"first_token_transform\", \"type_vocab_size\": 2, \"vocab_size\": VOC_SIZE}with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo: json.dump(bert_base_config, fo, indent=2) with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo: for token in bert_vocab: fo.write(token+\"\\n\") 그리고 앞서 만들어준 모델, 프리트레이닝 데이터를 GCS 버킷에 업로드한다. 12if BUCKET_NAME: !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME 모델 학습 Hyper Parameters 설정하기GCS 버킷에 데이터와 모델을 모두 업로드해준 뒤, 실제 TPU에서 학습을 진행하도록 명령을 넘겨줘야 한다. 첫번째 줄의 BUCKET_NAME만 위와 동일하게 설정해주면 된다. 중간의 BATCH_SIZE, LEARNING_RATE, TRAIN_STEPS, NUM_TPU_CORES 등의 변수를 조절해 모델의 학습 속도를 결정할 수 있다. Colab의 TPU는 v3-8이므로 NUM_TPU_CORES는 8Core가 최대다. 123456789101112131415161718192021222324252627282930313233343536BUCKET_NAME = \"beomi-blog-sample\" #@param {type:\"string\"}MODEL_DIR = \"bert_model\" #@param {type:\"string\"}PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}# Input data pipeline configTRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}MAX_PREDICTIONS = 20 #@param {type:\"integer\"}MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}MASKED_LM_PROB = 0.15 #@param# Training procedure configEVAL_BATCH_SIZE = 64LEARNING_RATE = 2e-5TRAIN_STEPS = 1000000 #@param {type:\"integer\"}SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}NUM_TPU_CORES = 8if BUCKET_NAME: BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)else: BUCKET_PATH = \".\"BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))log.info(\"Using {} data shards\".format(len(input_files))) 모델을 TPU로 올리고 학습하기model_fn 이라는 이름의 딥러닝 모델 설정 객체를 만들어주고 TPU에 연결해준 뒤, 어떻게 학습을 하고 어디에 CheckPoint를 정리할지 등을 지정해줘야 한다. 이후 TPUEstimator를 통해 모델 객체, 설정 객체를 전달해주고, 해당 estimator를 estimator.train() 하면 TPU 위에서 BERT 모델의 학습이 진행된다. 1234567891011121314151617181920212223242526272829303132333435model_fn = model_fn_builder( bert_config=bert_config, init_checkpoint=INIT_CHECKPOINT, learning_rate=LEARNING_RATE, num_train_steps=TRAIN_STEPS, num_warmup_steps=10, use_tpu=USE_TPU, use_one_hot_embeddings=True)tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)run_config = tf.contrib.tpu.RunConfig( cluster=tpu_cluster_resolver, model_dir=BERT_GCS_DIR, save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS, tpu_config=tf.contrib.tpu.TPUConfig( iterations_per_loop=SAVE_CHECKPOINTS_STEPS, num_shards=NUM_TPU_CORES, per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))estimator = tf.contrib.tpu.TPUEstimator( use_tpu=USE_TPU, model_fn=model_fn, config=run_config, train_batch_size=TRAIN_BATCH_SIZE, eval_batch_size=EVAL_BATCH_SIZE) train_input_fn = input_fn_builder( input_files=input_files, max_seq_length=MAX_SEQ_LENGTH, max_predictions_per_seq=MAX_PREDICTIONS, is_training=True)# 학습하자!!estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS) 어디에 모델이 저장되나?estimator.train 코드를 실행하면 TPU위에서 학습이 이뤄지고, 동시에 우리가 지정한 체크포인트(SAVE_CHECKPOINTS_STEPS)별로 GCS 버킷의 폴더에 모델의 가중치값이 저장된다. Google Colab은 Pro를 쓰더라도 최대 24시간이 한계이고, 거대한 데이터로 학습시킬때는 세션이 종료되기 때문에 체크포인트를 가져와 해당 부분부터 학습을 재게하는 것이 필요하다. 이와 같이 GCS 버킷에 저장을 하는 것을 통해서 BERT 모델을 Colab TPU로 처음부터 끝까지 우리 데이터를 통해 학습시킬수 있다. References BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Pre-training BERT from scratch with cloud TPU","link":"/2020/02/26/Train-BERT-from-scratch-on-colab-TPU-Tensorflow-ver/"},{"title":"Colab에서 PyTorch 모델 TPU로 학습하기","text":"딥러닝 모델을 학습시키다 보면 항상 vram의 압박에 시달리게 된다. 특히 최근 막대한 크기의 모델들이 등장해 이런 압박은 더 심해지기도 한다. 한편, 일반 사용자용 그래픽 카드 중 최상위인 Nvidia 2080ti조차도 vram이 겨우 11GB밖에 되지 않아 거대한 모델을 Fine-tuning 하는 것조차 굉장히 작은 배치사이즈로 학습시켜야 한다. Google Colab에서 제공하는 TPU는 tpu v3-8 모델로 총 128GB의 메모리를 가지고 있어, 상대적으로 큰 모델과 배치사이즈를 이용해 학습할 수 있다. (tpu v3 하나는 16GB의 HBM 메모리를 가지고 있고, tpu v3-8은 8개의 코어로 총 128GB의 메모리를 가진다.) PyTorch에서는 Pytorch/XLA 프로젝트를 통해 PyTorch에서도 TPU를 통한 학습을 할 수 있도록 컴파일러를 제공하고 있고, colab에 해당 패키지를 설치하면 TPU를 곧바로 사용할 수 있다. NOTE: 이번 글은 아래 공식 튜토리얼의 내용을 따라갑니다. 공식 Tutorial: PyTorch on Cloud TPUs: Single Core Training AlexNet on Fashion MNIST (단 내용의 100%를 담는 대신, 기존 PyTorch와 동일한 부분은 제외함) PyTorch/XLA 설치하기123VERSION = \"20200220\"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py!python pytorch-xla-env-setup.py --version $VERSION PyTorch/XLA 패키지는 Github에서 설치 스크립트를 받아 설치할 수 있다. 글 쓰는 날짜 2020/02/24에는 20200220이 최신버전이다. colab에서 설치를 진행하면 torch-1.4.0 을 torch-1.5.0a0 버전으로, torchvision-0.5.0 을 0.6.0a0 로 업데이트한다. 이와 함께 설치하는 torch-xla 패키지가 메인 패키지가 된다. 데이터셋 준비하기데이터셋을 다루는 방법은 기존 PyTorch에서 사용하던 방법과 동일하다. 12345678910import osimport torchimport torchvisionimport torchvision.datasets as datasetsraw_dataset = datasets.FashionMNIST( os.path.join(\"/tmp/fashionmnist\"), train=True, download=True) torchvision.datasets 를 통해 FashionMNIST를 받을 수 있다. 1234567891011import torchvision.transforms as transforms# See https://pytorch.org/docs/stable/torchvision/models.html for normalization# Pre-trained TorchVision models expect RGB (3 x H x W) images# H and W should be &gt;= 224# Loaded into [0, 1] and normalized as follows:normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])to_rgb = transforms.Lambda(lambda image: image.convert('RGB'))resize = transforms.Resize((224, 224))my_transform = transforms.Compose([resize, to_rgb, transforms.ToTensor(), normalize]) torchvision.transforms를 통해 데이터셋을 이미지넷 기반으로 Normalize하고 &amp; RGB &amp; 244x244 사이즈로 리사이징 하는 처리를 해줄 수 있다. 1234567891011train_dataset = datasets.FashionMNIST( os.path.join(\"/tmp/fashionmnist\"), train=True, download=True, transform=my_transform)test_dataset = datasets.FashionMNIST( os.path.join(\"/tmp/fashionmnist\"), train=False, download=True, transform=my_transform) Dataset을 load할 때 transform 인자로 전달해주면 위 처리가 모두 함께 진행된다. 12train_sampler = torch.utils.data.RandomSampler(train_dataset)test_sampler = torch.utils.data.RandomSampler(test_dataset) 이후 Sampler를 통해 순서를 적절히 섞어준다. 1234567891011batch_size = 8train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=batch_size, sampler=train_sampler)test_loader = torch.utils.data.DataLoader( test_dataset, batch_size=batch_size, sampler=test_sampler) 위와 같이 DataLoader를 통해 데이터셋을 batch_size로 잘라 Iterable한 객체로 바꿔준다. 보다 자세한 Dataset, Sampler, DataLoader에 대한 정보는 아래 링크를 참고해보자. [링크] pytorch dataset 정리 | Hulk의 개인 공부용 블로그 [중요!] torch-xla 패키지로 Device 지정하기PyTorch에서는 .to(device) 문법을 통해 텐서 변수들과 모델들을 GPU와 같은 device에 올릴 수 있다. TPU에 올리기 위해서는 torch_xla 에서 제공하는 xm.xla_device() 를 통해 PyTorch에 호환되는 device 를 지정할 수 있다. 123456789import torch_xlaimport torch_xla.core.xla_model as xm# Creates AlexNet for 10 classesnet = torchvision.models.alexnet(num_classes=10)# Acquires the default Cloud TPU core and moves the model to itdevice = xm.xla_device()net = net.to(device) TPU로 PyTorch 딥러닝 모델 학습시키기DataLoader와 Transforms를 이용해 데이터를 증폭시키고 적절한 수준의 이미지로 변환시키는 과정은 기존의 PyTorch 코드와 완전히 동일하다. 또한, loss function와 optimizer도 torch.nn와 torch.optim에서 사용하는 것 그대로 사용할 수 있다. 그리고 나머지 Training 과정의 코드도 거의 99% 동일하지만 .to(device) 의 device가 TPU라는 점만이 차이가 있다. 12345678910111213141516171819202122232425262728# Note: this will take 4-5 minutes to run.num_epochs = 1loss_fn = torch.nn.CrossEntropyLoss()optimizer = torch.optim.Adam(net.parameters())# Ensures network is in train modenet.train()start_time = time.time()for epoch in range(num_epochs): for data, targets in iter(train_loader): # Sends data and targets to device data = data.to(device) targets = targets.to(device) # Acquires the network's best guesses at each class results = net(data) # Computes loss loss = loss_fn(results, targets) # Updates model optimizer.zero_grad() loss.backward() xm.optimizer_step(optimizer, barrier=True) # 이부분이 TPU 쓸때 필요한 코드!!elapsed_time = time.time() - start_timeprint (\"Spent \", elapsed_time, \" seconds training for \", num_epochs, \" epoch(s) on a single core.\") 위 코드 중 25번째 줄의 xm.optimizer_step(optimizer, barrier=True) 부분이 TPU를 사용하기 위한 코드이다. (GPU 코드에 겨우 한줄 추가!) TPU 코어 FULL 활용하기앞서 다룬 과정에서는 TPU 8core 중 1core만을 사용한다. 한편, TPU 1개에 들어있는 8개의 코어 전체를 사용하면 보다 빠른 학습이 가능하다. 아래 코드는 PyTorch/XLA의 공식 튜토리얼을 참고합니다. 공식 Tutorial: PyTorch on Cloud TPUs: MultiCore Training AlexNet on Fashion MNIST 123456789101112131415161718192021222324import torchimport torch_xlaimport torch_xla.core.xla_model as xmimport torch_xla.distributed.xla_multiprocessing as xmp# \"Map function\": acquires a corresponding Cloud TPU core, creates a tensor on it,# and prints its coredef simple_map_fn(index, flags): # Sets a common random seed - both for initialization and ensuring graph is the same torch.manual_seed(1234) # Acquires the (unique) Cloud TPU core corresponding to this process's index device = xm.xla_device() # Creates a tensor on this process's device t = torch.randn((2, 2), device=device) print(\"Process\", index ,\"is using\", xm.xla_real_devices([str(device)])[0])# Spawns eight of the map functions, one for each of the eight cores on# the Cloud TPUflags = {}# Note: Colab only supports start_method='fork'xmp.spawn(simple_map_fn, args=(flags,), nprocs=8, start_method='fork') TPU 여러 코어를 사용하기 위해서는 torch_xla.distributed.xla_multiprocessing 을 통해 프로세스를 N개 띄우는 방식으로 진행한다. 단, 앞서 진행한 코드는 model과 data 로드 부분이 모두 각자의 코드로 나와 Colab 인스턴스의 CPU/Ram에서 진행되어 코드 내의 변수를 .to(device) 를 사용해 GPU나 TPU로 보낼 수 있지만, TPU의 여러 코어를 사용할때는 하나의 함수 내에 model과 data 모두를 넣고 진행해야 한다. 아래와 같이 map_fn 을 만들어서 PyTorch 학습/평가를 위한 부분을 넣어준다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import torchvisionfrom torchvision import datasetsimport torchvision.transforms as transformsimport torch_xla.distributed.parallel_loader as plimport timedef map_fn(index, flags): torch.manual_seed(flags['seed']) device = xm.xla_device() dataset_path = os.path.join(\"/tmp/fashionmnist\", str(xm.get_ordinal())) normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) to_rgb = transforms.Lambda(lambda image: image.convert('RGB')) resize = transforms.Resize((224, 224)) my_transform = transforms.Compose([resize, to_rgb, transforms.ToTensor(), normalize]) train_dataset = datasets.FashionMNIST( dataset_path, train=True, download=True, transform=my_transform) test_dataset = datasets.FashionMNIST( dataset_path, train=False, download=True, transform=my_transform) train_sampler = torch.utils.data.distributed.DistributedSampler( train_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=True) test_sampler = torch.utils.data.distributed.DistributedSampler( test_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=False) train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=flags['batch_size'], sampler=train_sampler, num_workers=flags['num_workers'], drop_last=True) test_loader = torch.utils.data.DataLoader( test_dataset, batch_size=flags['batch_size'], sampler=test_sampler, shuffle=False, num_workers=flags['num_workers'], drop_last=True) net = torchvision.models.alexnet(num_classes=10).to(device).train() loss_fn = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters()) train_start = time.time() for epoch in range(flags['num_epochs']): para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device) for batch_num, batch in enumerate(para_train_loader): data, targets = batch output = net(data) loss = loss_fn(output, targets) optimizer.zero_grad() loss.backward() xm.optimizer_step(optimizer) # ParallelLoader 쓸때는 barrier=True 필요 없음 elapsed_train_time = time.time() - train_start print(\"Process\", index, \"finished training. Train time was:\", elapsed_train_time) ## Evaluation # Sets net to eval and no grad context net.eval() eval_start = time.time() with torch.no_grad(): num_correct = 0 total_guesses = 0 para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device) for batch_num, batch in enumerate(para_train_loader): data, targets = batch output = net(data) best_guesses = torch.argmax(output, 1) num_correct += torch.eq(targets, best_guesses).sum().item() total_guesses += flags['batch_size'] elapsed_eval_time = time.time() - eval_start print(\"Process\", index, \"finished evaluation. Evaluation time was:\", elapsed_eval_time) print(\"Process\", index, \"guessed\", num_correct, \"of\", total_guesses, \"correctly for\", num_correct/total_guesses * 100, \"% accuracy.\") 이전 코드와 다른 부분은 74번째 줄에서 더이상 barrier=True 가 필요하지 않다는 것이다. 1234567# Configures training (and evaluation) parametersflags['batch_size'] = 32flags['num_workers'] = 8flags['num_epochs'] = 1flags['seed'] = 1234xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork') 위에서 만든 함수를 xmp.spawn 함수를 통해 배치사이즈, 워커(코어수=8개), epochs, seed값을 제공해주면 실제 TPU로 해당 코드가 컴파일 되어 전달된 뒤 학습이 진행된다. References 공식 Tutorial: PyTorch on Cloud TPUs: Single Core Training AlexNet on Fashion MNIST 공식 Tutorial: PyTorch on Cloud TPUs: MultiCore Training AlexNet on Fashion MNIST Hulk의 개인 공부용 블로그: pytorch dataset 정리","link":"/2020/02/24/Pytorch-with-TPU-on-Colab/"},{"title":"딥러닝 프로젝트 100% 재현을 위한 Git-LFS와 Gitlab","text":"부제: GitHub의 1GB 한계를 넘어 개인 Gitlab서버로 LFS 마음껏 쓰기 들어가며딥러닝 프로젝트를 하다보면 종종 마주치는 문제가 있다. “어라? 이거 파일 어디다 뒀더라? (있긴 있는데 어딨는지 모르겠네)” “ipynb파일은 있는데 weight 파일이 없어… (다른 서버에 있나?)” “아 이거 코드 copy 하고 나서 수정할걸 ㅠㅠ” ML/DL 프로젝트가 아니라 파이썬, js등으로 하던 웹 개발 프로젝트에서는 전혀 느껴보지 못했던 이슈였다. (사실 프로젝트의 시작이 git init 이었으니, 무슨 말이 더 필요할까.) 하지만 데이터 분석에서는 ‘일단 결과를 보는게 중요해~’ 라는 악마의 속삭임에 넘어가는 경우가 종종 있다. ‘이거만 수정하면 될 것 같은데…’ 물론, 당연히 수정하고 나면 높은 확률로 망한다. 그러고서는 실수로라도 rm -rf ./checkpoints/ 같은 명령어가 들어있었다면 이전에 기껏 학습시켜둔 모델 파일을 통채로 날려버리는 경우도 허다하다. 사실 이정도만 되어도 양반인데(다시 학습시키면 되니까), 만약 random seed를 고정하지 않고 학습시켰다면 그야말로 끔찍한, 논문은 썼는데 리비전하려니 재현이 안되는..! 상황이 연출된다. 그렇다면 이런 질문이 나온다. “그거 그냥 전부 github에 올리면 되는거 아니에요?” 네, 안됩니다! (단호박) 우선 Github등 여러 git 서비스들은 자체적으로 하나의 Repo에 보통 1GB 용량 제한을 두고 있고, 이로 인해 모델 weight들을 올려서 관리한다는 것은 아주아주아주아주 작은 모델만 가능하다. 특히나 모델같은 경우에는 pickle을 이용해 binary 파일로 관리하는데, 이런 binary 파일은 git에서는 통채로(99%) 파일이 바뀌는 것으로 트래킹이 되기 때문에 버전 관리에서 어마어마하게 큰 용량을 차지하게 된다. 따라서 아래의 Git-LFS가 등장한다. GIT-LFS, 큰 파일의 히스토리를 git으로.Git-LFS는 git의 하나의 애드온이다. LFS: Large File Storage로, 소스코드가 아닌 여러 큰 애셋들(특히 바이너리 파일 등)을 관리하기 위해 등장했다. 다만 git과는 별도로 동작하기 때문에 git lfs 서비스를 먼저 설치해야 한다. https://git-lfs.github.com/ Github에서 제공하는 git lfs. OS에 맞게 받아서 설치해주자. 설치한 뒤 아래 명령어를 한번만 터미널에서 입력하면 시스템 전체에서 git lfs 명령어를 사용할 수 있다. (간혹가다가 이걸 Repo단위로 오해하는 경우가 있는데, 이 명령어는 ‘시스템단위’의 명령어다.) 1git lfs install 그 이후에 cd 등을 통해 원하는 git으로 관리되는 프로젝트의 Root에 들어가 아래와 같이 원하는 확장자/이름을 포함하는 요소를 트래킹하도록 만든다. 1git lfs track \"*.pkl\" 위와같이 입력하면 .pkl 이라는 확장자를 가진 모든 파일이 트래킹 된다. git 프로젝트 최상단에서 입력해주면 하위 폴더를 비롯해 새로 생성되는 파일 역시 저 조건에 충족만 되면 Git lfs로 트래킹이 가능하다. 딥러닝 모델을 위한 Lfs track딥러닝 프로젝트를 할 때 주로 쓰는 Tensorflow, PyTorch 라이브러리에서 쓰는 모델 파일들의 확장자를 정리해 보았다. (제보 환영합니다!) 123456789101112git lfs track \"*.pkl\" # 피클 git lfs track \"*.model\" # TF/Kerasgit lfs track \"*.h5\" # TFgit lfs track \"*.pb\" # TFgit lfs track \"*.ckpt\" # TFgit lfs track \"*.pt\" # Pytorchgit lfs track \"*.pth\" # Pytorchgit lfs track \"*.bin\" # TF/PyTorchgit lfs track \"*.zip\" # 데이터셋git lfs track \"*.tar\" # 데이터셋git lfs track \"*.tar.gz\" # 데이터셋git lfs track \"*.targz\" # 데이터셋 물론 모델 파라미터를 저장할 때 이상한 이름으로 저장하면 위에 해당하지 않겠지만, 일상적으로 저장하는 확장자들은 대부분 커버할 수 있다고 생각한다. 개인 서버의 필요성: LFS는 유료 서비스다.물론 git들도 유료서비스이긴 하지만, 대부분 Public Repo에 대해서는 무료로 제공해주고 있다. 하지만 LFS는 무료 제공량이 있기는 하지만, 굉장히 적다. 아래는 github의 LFS 가격 안내 페이지인데, Storage와 Bandwidth 각각 1GB까지만 무료인 것을 알 수 있다. 사설 Git Server를 만들자: Gitlab docker 이미지!Github은 가장 대중적인 오픈소스 커뮤니티이고 git 서버 서비스이지만, 아쉽게도 커스텀으로 설치하는 옵션은 Github Enterprise에만 지원한다. 따라서 차선책으로 Gitlab을 고려할 수 있다. Gitlab Docker image https://docs.gitlab.com/omnibus/docker/ Gitlab은 친절하게 ‘모든 설정이 완료된’ Docker image로 서비스를 쓸 수 있도록 배포해준다. 따라서 설치를 원하는 디렉토리에 들어가, 아래 명령어만 실행하면 설치가 끝난다. 단, 앞선 부분에서 이부분에_도메인을_넣으면됩니다 대신 서버 IP에 연결된 도메인 이름(ex: beomi.github.io 등)을 넣어주어야 한다. 이부분이 이후 Gitlab에서 글로벌하게 사용하는 서버 주소가 되기 때문. 1234567891011export GITLAB_HOME=$(pwd)sudo docker run --detach \\ --hostname 이부분에_도메인을_넣으면됩니다 \\ --env GITLAB_OMNIBUS_CONFIG=\"gitlab_rails['lfs_enabled'] = true;\" \\ --publish 30443:443 --publish 30080:80 --publish 30022:22 \\ --name gitlab \\ --restart always \\ --volume $GITLAB_HOME/gitlab/config:/etc/gitlab \\ --volume $GITLAB_HOME/gitlab/logs:/var/log/gitlab \\ --volume $GITLAB_HOME/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:latest 약간의 시간(5분)이 지난 뒤 http://도메인이름:30080 에 들어가면 초기 id/pw(관리자용)를 입력해 계정을 생성해 사용할 수 있다. 분명히 공식 문서에서는 GITLAB_OMNIBUS_CONFIG 설정 없이도 git lfs가 기본으로 활성화되어있다고 말 하지만 정작 실행해보면 왜인지 LFS가 안되어있어서 수동으로 활성화 env를 넣어주었다. 나머지는 동일!git lfs track 명령어를 통해 .gitattribute파일이 정상적으로 생성되고 이 파일이 git을 통해 잘 관리되고 있다면 이제 더이상의 문제는 없다! 자연스럽게 git add와 git commit -m 으로 커밋을 완료해보자. 만약 Git LFS만 업로드하고 싶다면git 대신 git lfs 명령어를 쓰면 된다. 1234git lfs push# 땡기고 싶다면git lfs pull 완성!실제로 완성해서 쓰고 있는데 성능이 좋다. 인터넷을 기가비트(1G)로 쓰고 있어서 학교 서버 등에서 사용할 때에도 충분히 빠른 속도가 나오는 편. (아래는 Transformers 라이브러리를 이용한 간단한 예제들을 정리하는 레포다.) :30080 포트가 보기 싫다면.. Reverse Proxy를 사용해보세요!사실, 앞선 설명에서는 모두 30080, 30443 처럼 HTTP/HTTPS/SSH를 위한 ‘이상한 포트’ 대역을 사용했지만, 위 스크린샷을 보면 git.jblee.kr 처럼 기본 포트만 사용하는 것을 볼 수 있다. 시놀로지 NAS에서 Reverse proxy를 쓰는 방법DSM에서는 몇번의 클릭만으로 아래와 같이 Reverse proxy를 만들어 쓸 수 있다. ‘추가’를 눌러 아래와 같이 원하는 도메인을 설정해 주면 된다. 외부에서 http://git.jblee.kr:80으로 들어오는 요청을 http://localhost:30080 으로 Proxy해준다. (프로토콜은 모두 HTTP로.) 같은 방식으로 https://git.jblee.kr:443 을 https://localhost:30443 으로 Proxy 해주면 된다. (당연히 프로토콜은 모두 HTTPS로 한다. 그리고 HTTP/2를 활성화 시키는 것이 성능에 훨씬 더 유리하다.) http 대신 안전한 https 를 사용하고 싶다면시놀로지 NAS에 설치한 경우 인증서 설정법DSM의 ‘제어판’ - ‘보안’ - ‘인증서’ 메뉴에서 아래와 같이 ‘추가’를 눌러 새 주소에 대해 새로운 인증서를 받는다. 위 스크린샷의 ‘구성’을 누르면아래와 같이 시스템 서비스 별, 그리고 Reverse proxy 별로 다른 인증서를 지정할 수 있는데, 이 때 Reverse Proxy로 생성해 준 도메인을 눌러 새 인증서를 아래와 같이 매칭시켜주면 끝난다. 일반 리눅스 버전에 설치한 경우/etc/gitlab/gitlab.rb 파일을 수정해야 한다. 1docker exec -it gitlab vim /etc/gitlab/gitlab.rb 위 명령어로 도커 내부에 들어가 gitlab.rb 파일을 수정해준다. 외부에서 직접 수정해줘도 되지만, 권한이 꼬이는 것을 방지하기 위해 docker exec를 통해 도커 내부에서 수정하는 것을 권장한다. 해당 파일에서 letsencrypt 부분을 찾아 주석(#)을 해제해 주자. 가장 첫 줄은 https:로 시작하는 도메인 풀 주소를 적어주면 된다. (단 이때 비정규포트(ex: 30443)등은 지원하지 않는다. 그래서 Reverse proxy로 받은 주소를 적어줘야 한다.) 1234567# 아래 주석 해제 후 값 변경 [필수]external_url \"https://git.jblee.kr\" # 꼭 https 로 시작하기# 아래 주석 해제 [11이하버전은 필수, 이상은 기본값으로 true]letsencrypt['enable'] = true # 메일로 갱신 알람을 받고 싶다면 적어주면 된다. [옵션]letsencrypt['contact_emails'] = ['foo@email.com'] 만약 매번 자동 갱신되는 것까지 고려한다면.. 사실 매번 리뉴얼 하는 것은 매우 귀찮으니, 이렇게 자동으로 리뉴얼되게 해 두는 것이 훨씬 쉽다. gitlab.rb 파일 중 아래와 같이 주석 해제하고 설정해주면 된다. 1234letsencrypt['auto_renew'] = trueletsencrypt['auto_renew_hour'] = 0letsencrypt['auto_renew_minute'] = 30letsencrypt['auto_renew_day_of_month'] = \"*/20\" 위 세팅으로는 매월 20일 0시 30분에 리뉴얼을 시도한다. Let’s Encrypt는 한번 갱신에 90일의 기간을 주기 때문에, 위와 같은 세팅으로 무한히 연장해서 쓸 수 있다. Gitlab 버전을 업그레이드해야 한다면만약 깃랩을 쓰다 보안/기능의 이슈로 업그레이드 해야한다면 아래 명령어로 쉽게 새 이미지를 받고 새로 띄울 수 있다. 1234docker pull gitlab/gitlab-ce:latestdocker stop gitlabdocker rm gitlab# 그리고 나서 docker run ~~~ References Gitlab 도커이미지 안내 https://docs.gitlab.com/omnibus/docker/","link":"/2020/05/29/Gitlab-for-LFS/"},{"title":"면접에서 나온 통계 질문 리뷰","text":"Bayes Rule / Bayes’ theorem이란?베이즈 룰, 혹은 베이즈 정리라고 부르는 Bayes Rule은 아래와 같다. 결론부터 말하면, 베이즈 정리는 인과를 역전한다 라는 의미가 있다. 수식적으로 이해하려면 아래와 같다. 이때 우리가 알고자 하는 정보는 $Pr(A|B)$이고, 알고있는/알고있다 가정한 정보들은 $Pr(B|A), Pr(A), Pr(B)$ 이다. $$\\Pr(A|B)={\\frac {\\Pr(B|A)\\Pr(A)}{\\Pr(B)}}\\propto {\\mathcal L}(A|B)\\Pr(A)$$ 간단한 예시를 들어보자. 2020년의 가장 큰 이슈인 COVID-19를 검사하는 테스터기의 성능을 알고싶다고 가정할 때, 우리가 최종적으로 알고자 하는 값은 “COVID-19에 양성(+)으로 나타난 사람이 실제로 감염자일 확률, $Pr(COVID|+)$”이 된다. 그렇다면 우리가 알 수 있는 확률은 어떤 것이 있을까? COVID-19에 걸린 사람에게 테스트했을 때 양성(+)으로 나타날 확률 $= Pr(+|COVID)$ 위 식에서 다시 정리를 해 보면 아래와 같이 정리해 볼 수 있다. $$\\Pr(COVID|+)={\\frac {\\Pr(+|COVID)\\Pr(COVID)}{\\Pr(+)}}$$ 하지만 우리는 현재 $Pr(COVID), Pr(+)$ 의 확률을 알지 못하기 때문에 $Pr(COVID|+)$ 를 곧바로 구할 수가 없다. 즉, 우리는 위 정보 두가지인 “COVID-19에 감염될 확률” 과 “검사가 양성(+)으로 나타날 확률(=False Positive)” 두 가지를 알아야 한다. 이때 실제로 COVID-19에 감염될 확률($Pr(COVID)$)을 정확히 알 수는 없기 때문에 이것을 일정 확률로 가정하는 사전확률(Prior) 값으로 둔다. 이를 통해서 우리는 사건/관찰 이라고 부르는, 여기서는 양성(+), $Pr(+)$ 이 관찰 된 이후의 확률값인 사후확률(Posterior, $Pr(COVID|+)$)을 구하게 된다. 다시 앞선 이야기로 돌아가 위 두가지 확률을 구하기 위해서 아래 두 값을 얻었다고 가정해보자. $Pr(COVID) = 0.002$ (0.2%) $Pr(+|not\\ COVID) = 0.05$ (5%) 위 가정을 통해 우리는 $Pr(COVID|+)$ 를 계산하기 위한 모든 정보를 얻을 수 있다. Prior, Posterior, Likelihood란?앞서 정리한 베이즈룰 내용을 정리하면 아래와 같다. Prior: $Pr(COVID)$, COVID-19에 걸릴 확률 Posterior: $Pr(COVID|+)$, 테스트 결과 양성(+)일 때 실제 COVID-19에 감염되었을 확률 Likelihood: $Pr(+|COVID)$, 실제 환자 중 테스트 결과 양성(+)으로 나타나는 비율% Likelihood의 값이 0~1사이의 %일수도 있고, 10,20..처럼 나타나기도 하는 이유는? &amp; Likelihood를 가장 잘 찾는다는 의미는?연속 vs 불연속: Likelihood가 0~1사이로 나타나는 케이스 = PDF(prob density func)가 Normal Distribution처럼 연속적으로 나타나는 케이스 Likelihood가 10, 20처럼 나타나는 케이스 = PDF가 Poisson Distribution처럼 Discrete한 케이스 Likelihood를 설명하는 것은 결국 PDF, CDF를 설명하는 함수를 잘 찾는다는 것과 같은 의미가 된다는 것이고, 원본의 Distribution을 어떻게 ‘잘 설명하는지’를 의미하는 것이다. Linear RegressionLinear Regression, 선형 회귀라고 부르는 모델은 X와 Y간의 Linear/선형적 관계를 설명하려는 모델이다. 보통 Least Square, MSE Loss를 사용해 학습한다. 좀더 이야기를 하면 Biased/unbiased 이야기도 나와야 하긴 하지만… 결과적으로 Linear Regression이 바라는 방향은 알지 못하는 임의의 X가 나왔을때 기대할 Y를 estimate하는 것인 셈이다. Logistic Regression?같은 Regression 명칭을 붙이고는 있지만 Linear Regression와는 완전히 다른 목표를 가지고 있는 모델. 아래 그림과 같이 데이터셋이 있을 때 두 데이터셋 사이의 거리를 최대한 벌리는, 두 그룹(혹은 N개의 카테고리)을 분리하는 것을 가장 잘 설명하는 HyperPlane을 찾는 것이 목표인 모델이다. 물론 logistic regression 자체는 Linear Model 중 하나로 특수한 케이스라고 볼 수 있다. 하지만 바라는 목표가 Categorical Classification이기 때문에, Y의 값이 0~1 사이로 고정된다는 것(이항분류), 그리고 결과가 이항적이기 때문에 $P(y|x)$ 가 Normal Distribution 대신 Binomial Distribution을 따른다. 한편 실제 Linear Model에서 나타나는 특성상 Y 값이 0~1사이를 넘어 더 큰 값을 가질 수 있게 되고, 이는 모델의 성능 하락을 가져오기 때문에 Logistic function을 도입해 이를 해결한다. Why Sigmoid? Odds function?Logistic Regression 모델을 쓸 때, 앞서 언급한 0~1 사이로 모델의 출력값(logits)을 제한하는 것 이외에도 sigmoid를 쓰는 이유가 있다. 여기서 odds ratio 라는 개념이 나오게 된다. Logistic Regression은 0과 1의 Biomial Distribution을 따르게 되는데, 이때 나오는 % 혹은 logits 값은 단순히 해당 사건의 frequentiest적 확률을 이야기 하는 것이 아니라 해당 사건이 일어날 가능도를 의미한다. 이때 주로 사용하는 방법이 odds ratio이고, 아래와 같이 정의된다. $$odds(p) = \\frac{p}{1-p}$$ 이러한 $odds(p)$ 를 통해 0과 1 사이로 확률값을 Mapping해 줄 수 있지만, 이 함수를 실제 그래프로 나타내보면 아래와 같은 형태로 나타난다. 확률 값 즉 $p \\sim 1$이 될 경우 위 그래프처럼 $odds$ 값이 하늘로 치솟게 된다. 따라서 이를 방지하기 위해 $log(odds(p))$ 로 Log Odds 값을 사용한다. 위 값에 단순히 log만 취해주면 아래와 같은, $-\\infin \\sim \\infin$ 사이의 값으로 나타나며, 동시에 $p=0.5$ 일때 Log odds 값은 0이 된다. 한편 이 식은 “Logitics Function”으로 불리기도 한다. 하지만 여전히 문제가 남아있다. 현재의 상태를 수식적으로 나타내면 아래와 같다. $$\\text{log_odds}(P(y=1 \\mid x)) = w_o + w_1x_1 + w_2x_2 + … + w_nx_n$$ 우리가 얻고자 하는 값은 $P(y=1|x)$, 즉 y=1일 확률 이기 때문에, log_odds 함수를 Inverse한 값을 양측에 취해줘야 한다. 간단하게 위 식을 정리하면 아래와 같다. 이 역함수를 차례대로 구해보자. $$y = log(\\frac{x}{1-x})$$ 위 식에서 $x$와 $y$를 치환하면 아래와 같고, $$x = log(\\frac{y}{1-y})$$ 위 식에 양 변에 $\\exp$를 취해주면 아래와 같다. $$e^x = \\frac{y}{1-y}$$ 식을 좀 더 정리해보자. $$y = (1-y)e^x$$ $$y = e^x - y e^x$$ $$y + ye^x = e^x$$ $$y(1 + e^x) = e^x$$ $$y = \\frac{e^x}{1+e^x}$$ $$y = \\frac{1}{\\frac{1}{e^x} + 1}$$ $$y = \\frac{1}{1 + e^{-x}}$$ 결과적으로 마지막 식은 우리가 익히 알고 있는 Sigmoid function이 된다. Vector / Matrix?벡터를 이야기 할 때는 물리적 의미, 대수적 의미, 그리고 ML/DL에서 사용하는 의미로 분류해서 이야기 할 수 있다. 가장 기본적으로 물리적인 의미로 벡터는 “방향과 크기를 가진 표현도구”로 정의할 수 있다. 한편 선형대수에서 이야기 하는 Vector는 Vector Space에서 정의하는 원소가 해당한다. (선형대수와 군 책에서 이야기하는, 그 ‘체에 대한 가군’의 특수한 경우에 해당한다.) 또한 위와 거의 동등한 의미로 딥러닝 등에서 말하는 벡터 역시 위의 선형대수적 의미에서 기반한 일종의 Array로 구현해 사용한다. 그렇다면 행렬, Matrix는 무엇인가? 가장 단순하게 설명한다면 행렬은 “여러 벡터의 집합”이라고 볼 수 있다. 한편, Vector 하나하나가 Vector Space에서의 위치를 의미한다면 이에 Matrix를 곱하는 것은 특정 Vector를 다른 Vector Space로 사영하는, 즉 Linear Tranformation/선형변환을 하는 것이라고 볼 수 있다. 따라서 Matrix가 해주는 역할은 특정한 데이터가 있을 때 해당 데이터를 우리 목적(분류든 회귀든)에 맞도록 잘 쓸 수 있는 Hyper Dimension으로의 Projection을 해주는 것이라고 볼 수 있다. (이에 맞춰서 아래와 같이 SVM등 하면서 원하는 Feature Space를 만들어주는 거라 볼 수 있다.) Reference Wikipedia 베이즈 정리 데이터사이언스스쿨 6.6 베이즈 정리 Wikipedia 로지스틱 회귀 The Sigmoid Function in Logistic Regression Wikipedia 벡터 공간, 선형 변환 Header Photo by Headway on Unsplash SVM Img PERPETUAL ENIGMA","link":"/2020/07/01/Review-the-Interview/"},{"title":"NLP 튜토리얼: 라벨링 없이 트위터 유저들을 자동으로 나누어보기","text":"트위터에는 굉장히 다양한 유저들이 있다. 그리고 트위터 유저들은 “BIO”라고 부르는 자기소개 페이지에 자신에 대한 정보를 적어둔다. 위 스크린샷과 같이, 자신의 계정이 어떤 계정인지를 간단한 160자 내로 드러내는 것이 바로 BIO다. 그렇다면, 이런 계정들이 ‘어떤’ 계정인지 BIO를 이용해 분류해 볼 수 있지 않을까? 하지만 모든 유저를 우리가 손으로 라벨을 붙여 학습시키는 것은 힘들다. 그렇다면 알아서 분류하려면 어떻게 해야할까? 간단한 자연어 처리를 통해 라벨링 없이 유저를 클러스터링해보자. 🌟 바로바로 실행하면서 따라올 수 있는 Google Colab 👨🏻‍💻👩🏻‍💻👇 https://colab.research.google.com/drive/1bgv3CHZDp2smIWXQwAUD3j5z8LAYbLVz 미리보기: 어떤 결과물이 나오나? 이번 튜토리얼이 끝날때, 위와 같이 각 유저의 트위터 프로필들 클러스터링 된 결과물을 2차원 공간에 차원축소를 통해 각각 다른 색의 동그라미로 보여주고, 각 동그라미 위에 마우스를 올리면 어떤 트위터 BIO였는지 보여주는 Plot을 완성한다. 유저를 분류한다? 어떻게?분류, 즉 Classification 문제는 주로 라벨링을 통한 Supervised Learning 방법을 사용한다. 하지만 앞서 언급한 것과 같이 라벨링이 필요하고, 더 높은 정확도를 얻기 위해서는 더 많은 데이터에 더 많은 라벨링(노가다)가 필요하다. 한편 이와 대조되는 방법으로 Unsupervised Learning을 사용하기도 하는데, 데이터만 넣어서 어떻게든 뭔가 만들어내는 방식이기도 하다. 이번 글에서는 NLP 사용의 기본적인 방법인 Word Vectorize인 word2vec 을 이용해본다. Word2VecWord2Vec은 단어를 N차원의 벡터로 만들어준다. (보통 300차원 이내를 쓴다.) Word2Vec 자체가 텍스트만 Tokenize해서 넣어주면 몇 가지 종류의 알고리즘을 통해 단어(토큰) 간의 상관관계를 찾아내고 서로 유사한 공간에 배치하도록 만들어진 Unsupervised Learning이다. 상세한 원리는 Word2Vec으로 문장 분류하기 - Ratsgo’s blog를 참고해보세요. 트위터 프로필로 Word2Vec을 학습시키자? NO!최근 NLP 트렌드는 Transfer Learning을 통한 성능의 개선이었다. 한정된 데이터가 아니라 수많은 데이터가 있다면 임베딩 공간이 좀 더 의미를 잘 부여받은 공간을 갖게된다는 이야기. Word2Vec같은 경우는 연산량이 많지 않아 데스크탑 CPU만으로도 몇만건의 데이터는 금방 학습이 끝나지만, 기존의 거대 데이터(위키 등)를 사용한 학습이 좀 더 높은 성능을 보인다고도 한다. 한편, 학습된 Word2Vec 모델에 없는 ‘새로운 단어’가 등장할 경우에는 Unknown 단어의 벡터로 모두 매핑되기 때문에 한계가 있다. 따라서 이번에는 네이버 댓글 약 100만개를 사용해 미리 학습시켜둔 Word2Vec 모델을 가져와 사용한다. (추가적인 학습은 진행하지 않는다!) 아래 내용은 2020년 1월 3일 Google Colab 환경 기준입니다. 데이터 준비하기우선 어떤 데이터를 쓸지부터가 관건이다. 하지만 걱정하지 마시라! 이미 모아둔 데이터를 받아서 써보자. 어떤 데이터를 사용하나?이번 튜토리얼에서는 트위터에서 네이버 뉴스를 링크한 트윗들을 크롤링한 데이터셋을 사용한다. 아래 명령어를 Google Colab에서 입력하면 곧바로 사용할 수 있다. 만약 일반 PC에서 진행중이라면 curl 앞의 ! 만 생략하고 입력하면 된다. 1234!curl gdrive.sh | bash -s 13fQZtkz4SSzVNcqh73SBqnUDw2mSDgzL!curl gdrive.sh | bash -s 1W8u9ZuEhKCkTbaGLzUF4YcF2F_hYeE1B!curl gdrive.sh | bash -s 1GffvAF3tBtSpjsblIdP1Tcv25XEyBewm!curl gdrive.sh | bash -s 103jom7lxjiqQptIRjrIdSjXR8jRqrlEd 위 다운로드를 모두 진행하면 트위터 유저 데이터 tsv파일 하나, 그리고 네이버 댓글로 학습시킨 word2vec 모델 파일이 다운로드 된다. 데이터 내용은 다음과 같이 유저고유번호, 유저 프로필이름, 유저 로그인이름, 유저의 BIO가 들어있다. 12df = pd.read_csv('twitter_users.tsv', sep='\\t')df.head(1) Word2Vec 모델 불러오기Gensim에서 사용하는 Word2Vec 모델을 이용한다. 아래 두 줄의 명령어로 기존에 학습된 word2vec 모델을 불러올 수 있다. 123from gensim.models import Word2Vecmodel = Word2Vec.load('embedding.save') 추가 라이브러리 설치하기앞서 언급했던 것 중 하나가 Word2Vec은 단어/토큰을 vector로 바꾼다는 것이었다. 이를 위해서는 문장으로 이루어진 트위터 프로필 정보를 단어/토큰단위로 쪼개야 한다는 뜻이다. 이를 위해 KoNLPy 라이브러리 중 Mecab을 사용한다. KoNLPy 설치KoNLPy는 pip를 통해 간단히 설치할 수 있다. 1!pip install -q konlpy 주의: KoNLPy는 JPipe를 사용해 Python 패키지이지만 java를 호출한다. 따라서 Java 1.7 혹은 1.8이 설치되어있고 java 명령어로 실행 가능하도록 시스템이 설정되어있어야 한다. Mecab 설치KoNLPy중 가장 속도가 빠른 mecab은 아래 명령어를 통해 추가 설치를 진행해야 한다. 1!bash &lt;(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh) 트위터 프로필을 토큰화하기 (단어로 쪼개기)KoNLPy의 mecab을 이용해 트위터 프로필 문장을 명사들로 쪼개보자. 12345678910111213from konlpy.tag import Mecabmecab = Mecab()def get_tokens(x): try: return [i for i in mecab.nouns(x) if len(i) &gt; 1] if x else [] except Exception as e: if str(x) == 'nan': return [] print(e) print(str(x)) raise e 위 get_token 함수는 x 문자열을 받으면 문자열 목록을 반환하게 된다. 단, 이때 한 단어는 2글자 이상으로 제한을 걸었다. (한 단어는 무의미한 경우가 많다.) 토큰화 함수 DataFrame에 적용하기앞서 읽은 DataFrame 중 user.description 컬럼에 위 함수를 적용해주자. 12df['user_mecab'] = df['user.description'].map(get_tokens)df['user_mecab_len'] = df['user_mecab'].map(len) 이후 토큰화된 문자의 개수로 필터링 하기 위해 길이 컬럼도 추가로 만들어주자. 명사수가 N개 이상인 프로필만 추출하기빈 프로필, 혹은 엄청 짧은 프로필은 정보가 충분히 있다고 보기 어렵다. 따라서 N개 이상의 단어로 이루어진 프로필만으로 제한을 걸어주자. 앞서 만든 user_mecab_len 컬럼이 5 이상인 것만 남겨두자. 1bio_exists_df = df[df['user_mecab_len'] &gt;= 5] 위 Dataframe을 살펴보면 아래와 같이 단어와 해당 길이가 나타난다. 문장의 벡터 = Mean(각 단어의 벡터)단어의 벡터는 알지만, 문장의 벡터는 어떻게 표현할 수 있을까? 이러한 질문의 답은, “문장의 벡터는 해당 문장의 단어들의 벡터 평균”이라고 볼 수 있다. 다른/혹은 더 높은 성능을 위한 방법으로는, 문맥을 이해하는 BERT, 혹은 Doc2Vec와 같은 문장단위 임베딩, 혹은 Word2Vec의 Mean을 취한 뒤 TF-IDF를 취해주는 방법 등이 있다. 따라서 아래와 같이 sentence vector를 얻는 함수를 만들어줄 수 있다. 1234567891011def get_sentence_mean_vector(morphs): vector = [] for i in morphs: try: vector.append(model.wv[i]) except KeyError as e: pass try: return np.mean(vector, axis=0) except IndexError as e: pass 마찬가지로 위에서 만들어온 DataFrame에 입혀주자. 1bio_exists_df['wv'] = bio_exists_df['user_mecab'].map(get_sentence_mean_vector) 트위터 프로필 문장 벡터로 클러스터링하기클러스터링에도 여러 방법이 있지만, 이번에는 엄청 간단하고 빠르고 기본적인 KMeans를 써보자. 이것 역시 sklearn에 구현된 것을 쓸 수 있다. 12345678910from sklearn.cluster import KMeansimport timeword_vectors = bio_exists_df.wv.to_list() num_clusters = 10# K means 를 정의하고 학습시킨다.kmeans_clustering = KMeans( n_clusters = num_clusters )idx = kmeans_clustering.fit_predict( word_vectors )bio_exists_df['category'] = idx 위 코드를 사용하면 앞서 만든 Dataframe의 wv 컬럼을 이용한다. KMeans는 사용하기 간단하고 다른 것을 건드릴 필요도 없다. 다만, 유일하게 n_clusters 하이퍼파라미터를 지정해주어야 하는데 이 클러스터 갯수를 어떻게 지정하느냐에 따라 클러스터링의 성능이 크게 달라지기 때문에 해당 숫자를 신중하게 결정하는 것이 좋다. 사실 이것도 굉장히 empirical한 접근방법이지만, 데이터셋을 보고 어떤 숫자가 적절할지를 보는 것이 중요하다. 무조건 모델을 돌리기보다 어떤 데이터인지 아는게 더 중요하다. 실제 유저 수는 아래와 같이 나타난다. 1번 클러스터가 제일 크지만, 이게 어떤 의미를 갖지는 않는다. 오히려 각 클러스터에 어떤 유저들이 나타났는지가 더 중요한 것이다. 차원 축소 &amp; 시각화앞서 트위터 프로필 문장을 300차원의 벡터로 만들었다. 하지만 300차원은 컴퓨터 화면으로는 볼 수 없는 어마어마한 고차원이기 때문에, 2차원의 화면에 맞도록 2차원으로 차원축소를 진행해야 한다. TSNE 차원축소t-SNE는 시각화를 위한 차원축소에 자주 사용하는 알고리즘이다. 이것 역시 sklearn에 구현되어있기 때문에, 보다 쉽게 사용할 수 있다. 아래와 같이 Dataframe의 wv 컬럼을 맞춰주도록 하자. 1234567891011121314151617181920212223from sklearn.manifold import TSNEX = bio_exists_df['wv'].to_list()y = bio_exists_df['category'].to_list()import os.pathimport pickletsne_filepath = 'tsne3000.pkl'# File Cacheif not os.path.exists(tsne_filepath): tsne = TSNE(random_state=42) tsne_points = tsne.fit_transform(X) with open(tsne_filepath, 'wb+') as f: pickle.dump(tsne_points, f)else: # Cache Hits! with open(tsne_filepath, 'rb') as f: tsne_points = pickle.load(f)tsne_df = pd.DataFrame(tsne_points, index=range(len(X)), columns=['x_coord', 'y_coord'])tsne_df['user_bio'] = bio_exists_df['user.description'].to_list()tsne_df['cluster_no'] = y 결과물에는 x, y 2차원의 좌표값이 나오게 된다. (tsne_df) 아래 샘플과 같이 300차원이 2차원으로 줄어든 것을 볼 수 있다. Bokeh로 2차원 Plotting단순히 Matplotlib을 이용한 이미지 결과가 아니라 실제 내용물(트위터 프로필)을 확인할 수 있도록 bokeh 를 이용해 Interactive Plot을 그려보자. 우선 bokeh가 Jupyter Notebook에서 동작할 수 있도록 여러 PY/JS들을 로딩해주자. 12345from bokeh.plotting import figure, show, output_notebookfrom bokeh.models import HoverTool, ColumnDataSource, valuefrom bokeh.palettes import breweroutput_notebook() 우리는 10개의 클러스터에 각각 다른 색을 입힐 것이기 때문에 아래와 같이 컬러를 만들어주자. 12345678910# Get the number of colors we'll need for the plot.colors = brewer[\"Spectral\"][len(tsne_df['cluster_no'].unique())]# Create a map between factor and color.colormap = {i: colors[i] for i in tsne_df['cluster_no'].unique()}# Create a list of colors for each value that we will be looking at.colors = [colormap[x] for x in tsne_df['cluster_no']]tsne_df['color'] = colors 이후 Bokeh가 인식하는 DataSource 객체를 만들어준다. 이때, Pandas DataFrame을 .to_dict(orient='list') 를 사용하면 해당 Dataframe의 모든 컬럼정보가 들어갈 수 있다. 1234# Bokeh Datasouce 만들기plot_data = ColumnDataSource( data=tsne_df.to_dict(orient='list')) 그리고 실제 Plot을 그리기 위한 배경으로 650x650 사이즈의 공간을 만들어준다. 12345678# Plot 만들기(배경)tsne_plot = figure( title='TSNE Twitter BIO Embeddings', plot_width = 650, plot_height = 650, active_scroll='wheel_zoom', output_backend=\"webgl\",) output_backend=&quot;webgl&quot;, 옵션을 사용하면 GPU가속을 통해 훨씬 부드러운 차트를 경험할 수 있다. 이후 각각 요소에 마우스를 올릴 때 무엇을 보여줄지 Tooltip으로 user_bio 컬럼을 사용하도록 지정한다. 123456# 해당 Hover 툴팁 만들기tsne_plot.add_tools( HoverTool( tooltips='@user_bio' )) 해당 Plot에 데이터 정보들을 넣어준다. Source를 입력하고 x,y 축을 넣어준 뒤, color정보를 컬럼명(color 컬럼)으로 넣어주면 된다. 12345678910tsne_plot.circle( source=plot_data, x='x_coord', y='y_coord', line_alpha=0.3, fill_alpha=0.2, size=10, fill_color='color', line_color='color',) 그리고 귀찮은 선들을 지운뒤 화면에 보이도록 하면 결과가 나타난다. 123456789# 각 값들 추가해주기 tsne_plot.title.text_font_size = value('16pt')tsne_plot.xaxis.visible = Falsetsne_plot.yaxis.visible = Falsetsne_plot.grid.grid_line_color = Nonetsne_plot.outline_line_color = None# 짠!show(tsne_plot) 결과 보기위 코드를 모두 실행하면 아래와 같이 색깔별로 모인 유저 프로필들을 볼 수 있다. 특히 뭉쳐있는 유저들을 살펴볼 경우, 굉장히 비슷한 말들이 적혀있고 주제도 비슷한 것을 볼 수 있다. 그리고 각 클러스터별로 어떤 유저들이 모여있는지 살펴보면 아래와 같이 연예인 팬, 혹은 정치적 이야기, 혹은 자신의 관심사 등에 따른 집단이 그룹으로 분류되는 것을 볼 수 있다. 맺으며트위터 유저들 모두가 자신의 정보를 빼곡히 프로필에 작성하는 것은 아니다. 하지만 자신의 관심사가 어떤 것인지에 대해, 그리고 이 계정이 어떤 목적을 위한 계정인지 드러나는 것을 간단하게 모아 살펴볼 수 있다. 물론, 성능이 우리가 바라는 것처럼 어마어마하게 멋지게 나오지는 않지만, 무작정 Labeling을 하기 이전게 간단하게 이런 방식으로도 데이터를 재미있게 구경해볼 수 있다는 점에서는 의미가 있다고 생각한다.","link":"/2020/01/05/Clustering_Twitter_Users/"},{"title":"KcBERT Finetune with PyTorch-Lightning v1.3.0","text":"들어가며KcBERT를 공개하며 NSMC를 예제로 PyTorch-Lightning을 이용한 Downstream task Fine-tune을 진행하는 Colab 예제(링크, 새 창)를 만들어 배포해보았다. 한편, Transformers의 버전과 PyTorch-Lightning, 그리고 PyTorch의 버전 자체가 올라가면서 여러가지의 기능이 추가되고, 여러 함수나 내장 세팅 등이 꽤나 많이 Deprecated되었다. 사람은 언제나 귀찮음에 지배되기 때문에 코드를 한번 만들고 최소한의 수정만을 하면서 ‘돌아가기는 하는’ 수준으로 코드를 유지했다. 하지만 실행시마다 뜨는 ‘Deprecate warnings’에 질리는 순간이 오고, 지금이 바로 그 순간이라 기존 코드를 “보다 좋게”, 그리고 기왕 수정하는 김에 모델 체크포인트 저장(성능에 따른) / Logging 서비스 연동 / Inference 코드 추가를 진행해보면 어떨까 싶다. 이 글은 이 Colab 노트북(링크) 에서 직접 실행해 보실 수 있습니다. Args? Hparams?AS-IS우선 가장 이슈였던 부분이자 모델 저장이 되지 않는 만악의 근원은 바로 class Args 였다. 원래는 Tap 이라는 라이브러리를 이용해 CLI에서 args를 쉽게 오버라이딩 할 수 있도록 만드는 것이 목적이었지만… 예제 코드를 Google Colab에서 실행할 수 있도록 제작을 변경하다보니, 단순한 dataset class로 대체해 설정을 단순화하는 방향으로 진행했다. 아래와 같이 값을 넣을 수 있도록 되어있고, 각 값은 args.random_seed 와 같이 액세스 하거나 오버라이딩 할 수 있도록 세팅을 해 두었다. 1234567891011121314151617181920class Arg: random_seed: int = 42 # Random Seed pretrained_model: str = 'beomi/kcbert-large' # Transformers PLM name pretrained_tokenizer: str = '' # Optional, Transformers Tokenizer Name. Overrides `pretrained_model` auto_batch_size: str = 'power' # Let PyTorch Lightening find the best batch size batch_size: int = 0 # Optional, Train/Eval Batch Size. Overrides `auto_batch_size` lr: float = 5e-6 # Starting Learning Rate epochs: int = 20 # Max Epochs max_length: int = 150 # Max Length input size report_cycle: int = 100 # Report (Train Metrics) Cycle train_data_path: str = \"nsmc/ratings_train.txt\" # Train Dataset file val_data_path: str = \"nsmc/ratings_test.txt\" # Validation Dataset file cpu_workers: int = os.cpu_count() # Multi cpu workers test_mode: bool = False # Test Mode enables `fast_dev_run` optimizer: str = 'AdamW' # AdamW vs AdamP lr_scheduler: str = 'exp' # ExponentialLR vs CosineAnnealingWarmRestarts fp16: bool = False # Enable train on FP16 tpu_cores: int = 0 # Enable TPU with 1 core or 8 coresargs = Arg() 이 부분까지는 크게 문제가 없어보인다. 나름 주석도 괜찮게 되었지 않나? 🤣 하지만 이 args 는 아주 심각한 문제가 있다. 바로 PyTorch Lightning에서 지원하는 hparams 속성에서 json으로 serializable하지 않다는 것. 이 점이 save된 ckpt에 hparams가 {} 으로 공백으로 비어있는 상황이 발생한다. TO-BEJSON Serializable하게 바꾸면 된다. 즉, Python dict로 바꿔주자. (그리고 쓸모없는 인자들도 좀 지워주자…) 1234567891011121314151617args = { 'random_seed': 42, # Random Seed 'pretrained_model': 'beomi/kcbert-base', # Transformers PLM name 'pretrained_tokenizer': '', # Optional, Transformers Tokenizer Name. Overrides `pretrained_model` 'batch_size': 32, 'lr': 5e-6, # Starting Learning Rate 'epochs': 20, # Max Epochs 'max_length': 150, # Max Length input size 'train_data_path': \"nsmc/ratings_train.txt\", # Train Dataset file 'val_data_path': \"nsmc/ratings_test.txt\", # Validation Dataset file 'test_mode': False, # Test Mode enables `fast_dev_run` 'optimizer': 'AdamW', # AdamW vs AdamP 'lr_scheduler': 'exp', # ExponentialLR vs CosineAnnealingWarmRestarts 'fp16': True, # Enable train on FP16 'tpu_cores': 0, # Enable TPU with 1 core or 8 cores 'cpu_workers': 4,} 물론 이제는 args.batch_size 로 액세스하는 것은 불가능하다. 하지만 그 대신 hparams로 바꿀 수 있다. PyTorch-Lightning의 hparamsPyTorch-Lightning에서의 HyperParams는 아래와 같은 형식으로 세팅해 줄 경우, 자동으로 model.harpams 내에 저장된다. 123456789101112&gt;&gt;&gt; class ManuallyArgsModel(LightningModule):... def __init__(self, arg1, arg2, arg3):... super().__init__()... # manually assign arguments... self.save_hyperparameters()... def forward(self, *args, **kwargs):... ...&gt;&gt;&gt; model = ManuallyArgsModel(1, 'abc', 3.14)&gt;&gt;&gt; model.hparams\"arg1\": 1\"arg2\": 'abc'\"arg3\": 3.14 Reference: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#save-hyperparameters 하지만 모든 값을 모델 입력에 직접 넣어주는 것은 상당히 귀찮다. 따라서 **kwargs 를 사용해 모델 Initialize에 넣어준 뒤 self.save_hyperparameters() 를 실행해주면, 해당 값들을 모두 self.hparams 에서 액세스 할 수 있다. 123456789101112131415class Model(LightningModule): def __init__(self, **kwargs): super().__init__() self.save_hyperparameters() # 이 부분에서 self.hparams에 위 kwargs가 저장된다. self.bert = BertForSequenceClassification.from_pretrained(self.hparams.pretrained_model) self.tokenizer = BertTokenizer.from_pretrained( self.hparams.pretrained_tokenizer if self.hparams.pretrained_tokenizer else self.hparams.pretrained_model ) [... 중략 ...]model = Model(**args) LoggingAS-IS현재는 validation_epoch_end 부분에서 Tensorboard에 로깅할 값들을 아래와 같이 Dict 중 log 라는 Key의 value로 새로운 dict를 전달해주는 방식으로 로깅이 이루어지고 있다. 1234567891011121314151617181920212223242526272829303132def validation_epoch_end(self, outputs): loss = torch.tensor(0, dtype=torch.float) for i in outputs: loss += i['loss'].cpu().detach() _loss = loss / len(outputs) loss = float(_loss) y_true = [] y_pred = [] for i in outputs: y_true += i['y_true'] y_pred += i['y_pred'] # Acc, Precision, Recall, F1 metrics = [ metric(y_true=y_true, y_pred=y_pred) for metric in (accuracy_score, precision_score, recall_score, f1_score) ] tensorboard_logs = { 'val_loss': loss, 'val_acc': metrics[0], 'val_precision': metrics[1], 'val_recall': metrics[2], 'val_f1': metrics[3], } print() pprint(tensorboard_logs) return {'loss': _loss, 'log': tensorboard_logs} 하지만 현재의 방식은, 아래와 같이 PyTorch-Lightning v0.9.1 에서 Deprecated 되었고 v1.0.0 에 Remove 될 것이라고 한다. (실제로 1.0.0 버전에서 사라지지는 않았고, 여전히 지원하고 있기는 하다.) 12345/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0Please use self.log(...) inside the lightningModule instead.# log on a step or aggregate epoch metric to the logger and/or progress bar (inside LightningModule)self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True) warnings.warn(*args, **kwargs) TO-BE따라서 위에서 제공하는 것과 같이 self.log() 기능을 이용해야 한다. 1self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True) 실제 코드로 변경한 것은 아래 중복코드 제거와 함께 진행해보았다. 중복되는 코드 제거하기AS-IS기존의 코드는 training_step, validation_step, validation_epoch_end 세 가지로 쪼개져있는데, 실제로는 step 부분이 완전히 동일하기 때문에 이렇게 중복으로 작성할 이유가 전혀 없다. 또한, training step에서 train loss등을 출력하면, Train dataset에 대해 전체적인 metric이 나오는 것이 아니기 때문에 제거하는 것이 나았다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def training_step(self, batch, batch_idx): data, labels = batch output = self(input_ids=data, labels=labels) # Transformers 4.0.0+ loss = output.loss logits = output.logits preds = logits.argmax(dim=-1) y_true = labels.cpu().numpy() y_pred = preds.cpu().numpy() # Acc, Precision, Recall, F1 metrics = [ metric(y_true=y_true, y_pred=y_pred) for metric in (accuracy_score, precision_score, recall_score, f1_score) ] tensorboard_logs = { 'train_loss': loss.cpu().detach().numpy().tolist(), 'train_acc': metrics[0], 'train_precision': metrics[1], 'train_recall': metrics[2], 'train_f1': metrics[3], } if (batch_idx % self.args.report_cycle) == 0: print() pprint(tensorboard_logs) return {'loss': loss, 'log': tensorboard_logs}def validation_step(self, batch, batch_idx): data, labels = batch output = self(input_ids=data, labels=labels) # Transformers 4.0.0+ loss = output.loss logits = output.logits preds = logits.argmax(dim=-1) y_true = list(labels.cpu().numpy()) y_pred = list(preds.cpu().numpy()) return { 'loss': loss, 'y_true': y_true, 'y_pred': y_pred, } TO-BE따라서 아래와 같이 공통함수 step 을 만들어서 training_step 과 validation_step 각각에 대해 동일하게 값을 return하도록 만들어주면 된다. 또한 step 내에서 logging을 하는 것을 제거해준다. 그리고 해당 Logging을 각 epoch end에서 처리해주기 위해 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def step(self, batch, batch_idx): data, labels = batch output = self(input_ids=data, labels=labels) # Transformers 4.0.0+ loss = output.loss logits = output.logits preds = logits.argmax(dim=-1) y_true = list(labels.cpu().numpy()) y_pred = list(preds.cpu().numpy()) return { 'loss': loss, 'y_true': y_true, 'y_pred': y_pred, }def training_step(self, batch, batch_idx): return self.step(batch, batch_idx)def validation_step(self, batch, batch_idx): return self.step(batch, batch_idx) def epoch_end(self, outputs, state='train'): loss = torch.tensor(0, dtype=torch.float) for i in outputs: loss += i['loss'].cpu().detach() loss = loss / len(outputs) y_true = [] y_pred = [] for i in outputs: y_true += i['y_true'] y_pred += i['y_pred'] self.log(state+'_loss', float(loss), on_step=True, on_epoch=True, prog_bar=True) self.log(state+'_acc', accuracy_score(y_true, y_pred), on_step=True, on_epoch=True, prog_bar=True) self.log(state+'_precision', precision_score(y_true, y_pred), on_step=True, on_epoch=True, prog_bar=True) self.log(state+'_recall', recall_score(y_true, y_pred), on_step=True, on_epoch=True, prog_bar=True) self.log(state+'_f1', f1_score(y_true, y_pred), on_step=True, on_epoch=True, prog_bar=True) return {'loss': loss}def train_epoch_end(self, outputs): return self.epoch_end(outputs, state='train')def validation_epoch_end(self, outputs): self.epoch_end(outputs, state='val') # validation_epoch_end는 아무것도 반환하지 않아야 함. 위처럼 바꿔주면 학습시 아래와 같이 로그가 남는다. 12620/6251 [21:19&lt;29:32, 2.05it/s, loss=0.273, v_num=0, val_loss=0.266, val_acc=0.887, val_precision=0.914, val_recall=0.856, val_f1=0.884] validation_epoch_end 함수는 아무것도 반환하지 않아야 한다. 만약 어떤 값을 반환한다면 아래와 같은 warning이 뜬다. (Train에서야 Loss를 반환받아서 Gradient + backprop을 수행해야 하지만 Validation은 Forward만 계산하는거라 굳이 Loss를 반환할 필요가 없기 때문.) 1UserWarning: The validation_epoch_end should not return anything as of 9.1. To log, use self.log(...) or self.write(...) directly in the LightningModule DataLoader 코드 중복 제거AS-ISDataLoader를 커스텀으로 정의할 때 path, shuffle 설정을 제외하고서는 모두 동일한 코드다. 1234567891011121314151617181920212223242526272829def train_dataloader(self): df = self.read_data(self.args.train_data_path) df = self.preprocess_dataframe(df) dataset = TensorDataset( torch.tensor(df['document'].to_list(), dtype=torch.long), torch.tensor(df['label'].to_list(), dtype=torch.long), ) return DataLoader( dataset, batch_size=self.args.batch_size or self.batch_size, shuffle=True, num_workers=self.args.cpu_workers, )def val_dataloader(self): df = self.read_data(self.args.val_data_path) df = self.preprocess_dataframe(df) dataset = TensorDataset( torch.tensor(df['document'].to_list(), dtype=torch.long), torch.tensor(df['label'].to_list(), dtype=torch.long), ) return DataLoader( dataset, batch_size=self.args.batch_size or self.batch_size, shuffle=False, num_workers=self.args.cpu_workers, ) TO-BE따라서 아래와 같이 dataloader 공용 함수를 만들어 정리를 해주었다. 1234567891011121314151617181920def dataloader(self, path, shuffle=False): df = self.read_data(path) df = self.preprocess_dataframe(df) dataset = TensorDataset( torch.tensor(df['document'].to_list(), dtype=torch.long), torch.tensor(df['label'].to_list(), dtype=torch.long), ) return DataLoader( dataset, batch_size=self.hparams.batch_size or self.batch_size, shuffle=shuffle, num_workers=self.hparams.cpu_workers, )def train_dataloader(self): return self.dataloader(self.hparams.train_data_path, shuffle=True)def val_dataloader(self): return self.dataloader(self.hparams.val_data_path, shuffle=False) Model CKPT CallbackAS-IS현재는 작업폴더(ipynb 있는 곳) 내의 ./lightning_logs/version_5 형식과 같은 폴더에 Tensorboard형식의 로그, 그리고 마지막 step의 체크포인트가 저장된다. 하지만 마지막 step이 아닌, logging에서 나타나는 여러 값(val_acc 등)을 모니터링하며 가장 성능이 좋은 모델을 찾기 위해서는 추가적인 작업이 필요하다. TO-BE아래와 같이 ModelCheckpoint 콜백을 사용해 아래와 같이 어떤 파일 이름으로, 어떤 Metric을 모니터링할지, 몇 개의 파일을 저장할지 지정해주면 결과값과 함께 데이터를 저장할 수 있다. 아래 코드에서는 epoch, val_acc 기준 소수점 아래 4자리까지 기록해 저장한다. 1234567891011121314from pytorch_lightning.callbacks import ModelCheckpointcheckpoint_callback = ModelCheckpoint( filename='epoch{epoch}-val_acc{val_acc:.4f}', monitor='val_acc', save_top_k=3, mode='max', auto_insert_metric_name=False,)trainer = Trainer( callbacks=[checkpoint_callback], ...) NOTE: 글 작성 시기인 2021.03.16 기준 pytorch-lightning의 Stable버전이 v1.2.3으로, auto_insert_metric_name 옵션을 지원하지 않습니다. 따라서 Colab code에서는 master branch를 받아 설치합니다. Main Function (for DDP) 수정AS-IS12345def main(): print(\"Using PyTorch Ver\", torch.__version__) print(\"Fix Seed:\", args.random_seed) seed_everything(args.random_seed) model = Model(args) TO-BE위에서 사용한 args 인자에 attribute로 액세스 하지 못하기 때문에 아래처럼 전달해줘야 한다. 또한, Model Init 인자에 Dict를 kwargs 형식으로 전달해주기 위해 아래와 같이 dic을 unpacking해서 전달해준다. 12345def main(): print(\"Using PyTorch Ver\", torch.__version__) print(\"Fix Seed:\", args['random_seed']) seed_everything(args['random_seed']) model = Model(**args) 맺으며이전 게시글, BertForSequenceClassification on Transformers v4.0.0을 맺으며 말한 것과 동일하게 마무리한다. “잘 돌아가는 코드는 건드리는게 아니지만, 코드는 가만 내비두면 썩는다.”","link":"/2021/03/16/KcBERT-Downstream-Finetune/"},{"title":"BertForSequenceClassification on Transformers v4.0.0","text":"Huggingface Transformers v4.0.0!Huggingface에서 Transformers 패키지 4버전을 릴리즈했다. 많은 변화가 있지만, 이 변화를 체감하게 된 것은 KcBERT 레포에 한 이슈가 달리게 되어서이다. “KcBERT-Large NSMC Finetune 코드가 동작하지 않아요.” 1주일 전까지 잘 되던 코드가 안된다? 내가 수정한 적도 없는데… 하다가 떠올린 생각. 아. Transformers가 4버전으로 나왔지. 그리고 나는…. Colab에 pip로 항상 최신 버전의 패키지를 설치하도록 해 두었구나. 그래서 직접 실행해보니 위와 같이 도대체 이해가 되지 않는 에러가 보였다. “str object has no attribute ‘argmax’” 아니, str 이라고? 다른 Type이라면 모르겠지만, 도대체 왜 Str 이지? 하는 의문을 갖고 코드를 실행해보니.. Model, 즉 샘플 코드에서는 BertForSequenceClassification Class에서 forward 실행시 반환하는 객체가 달라진 것이다. (ㅠㅠ) 이유는 return_dict 옵션 때문 위와 같이 return_dict=False 로 두어야 이전과 동일한 결과가 나오고, 기본 값은 True 이기 때문에 주의가 필요하다. 하지만 새 코드를 굳이 예전처럼 쓸 이유는 없다고 판단해, 새로운 코드에 적응하기로 했다. AS-IS and TO-BE기존 코드에서는 아래와 같이 모델을 Forward시 loss와 logits 값이 Tuple로 반환되어서 곧장 사용할 수 있었다. 123456789101112131415def validation_step(self, batch, batch_idx): data, labels = batch # Transformers v3.x.x loss, logits = self(input_ids=data, labels=labels) preds = logits.argmax(dim=-1) y_true = list(labels.cpu().numpy()) y_pred = list(preds.cpu().numpy()) return { 'loss': loss, 'y_true': y_true, 'y_pred': y_pred, } 하지만 이제는 아예 여러 attribute를 가진 객체를 반환하기에, 아래와 같이 output.loss, output.logits 라는 attribute로 접근해야 값을 가져올 수 있다. 123456789101112131415161718def validation_step(self, batch, batch_idx): data, labels = batch output = self(input_ids=data, labels=labels) # Transformers 4.0.0+ loss = output.loss logits = output.logits preds = logits.argmax(dim=-1) y_true = list(labels.cpu().numpy()) y_pred = list(preds.cpu().numpy()) return { 'loss': loss, 'y_true': y_true, 'y_pred': y_pred, } 맺으며 “잘 돌아가는 코드는 건드리는게 아니지만, 코드는 가만 내비두면 썩는다.”","link":"/2020/12/04/Transformers4/"},{"title":"KcBERT MLM Finetune으로 Domain adaptation하기","text":"들어가며BERT와 GPT등 여러 Transformer 기반의 Pretrained model을 통해 보다 쉬운 Transfer learning이 가능하다. 게다가 우리에게는 Huggingface🤗 Transformers 라이브러리를 통해 훨씬 쉽게 downstream task에 여러 모델들을 적용하고/테스트 해 볼 수 있다. 한편, 이와 같은 사전학습된 모델을 적용할 때, 기존 학습된 Corpus의 도메인(ex: 댓글)과 Downstream task에 사용하는 도메인(ex: 금융)이 일치하지 않을 경우 전반적으로 성능이 높지 않게 나오기도 한다. 이뿐만 아니라, 특정 도메인에서 사용하는 Vocab이 Sub-word로 쪼개지는 이슈로 인해 전체적으로 Transformer model에 부하가 가는(학습이 잘 안되는) 상황도 생기게 된다. 따라서 이번 튜토리얼에서는 Pretrained BERT모델 중 댓글로 학습한 KcBERT를 새로운 도메인 Corpus로 MLM 학습을 추가로 진행해본다. (용어로는 Domain Adaptive Pretraining이라고 부른다. aka DAPT) 이 튜토리얼은 아래 Github gist와 Colab에서 직접 실행해볼 수 있습니다. Colab: https://colab.research.google.com/gist/Beomi/972c6442a9c15a22dfd1903d0bb0f577/2021-03-15-kcbert-mlm-finetune-with-petition-dataset.ipynb Github Gist: https://gist.github.com/Beomi/972c6442a9c15a22dfd1903d0bb0f577 꼭 DAPT를 해야할까? 처음부터 학습하면 안되나?사실 최근 컴퓨팅 파워 자체가 무척 많이 올라가면서 BERT-base정도 규모의 학습은(데이터셋 규모와 사용 가능한 컴퓨팅 파워에 따라 다르지만) TPUv3-8 기준 짧게는 1주, 길게는 한달 내외로 학습이 가능하다. 원하는 도메인과 원하는 task에 해당하는 데이터셋이 수십GB만큼 있다면, 오히려 Pretrain from scratch를 하는 것이 더 좋을 수 있다. (Colab에서 TPU로 BERT 처음부터 학습시키기 - Tensorflow/Google ver. 글을 참고해보자.) 하지만 실제로는 관련 도메인의 데이터셋을 GB단위로 구하는 것조차 매우 어렵다. 수집 뿐 아니라 데이터셋에 달린 권리의 문제 등 여러가지 이슈가 따라오기 때문. 따라서 연구를 위해 구할 수 있는 최대한의 규모의 corpus를 제작하되, 기존에 학습된 PLM 모델을 통해 성능을 보다 레버리징 하는 것이 낫다. (옵션) Vocab을 바꿔서 성능을 높이자BERT등 Transformer계열의 모델뿐만 아니라 대부분의 embedding이 사용되는 NLP 모델에서 공통적으로 보이는 한계 중 하나가 vocab의 한계, 그리고 주로 [UNK] 로 대표되는 OOV문제다. 이를 해결하기 위해 BPE(CBPE/BBPE) 계열의 Sub-word tokenization이 제안되고 실제로도 높은 성능을 보여주지만, 근본적으로 특정 도메인에서 나타나는 단어들이 수많은 Subword로 쪼개져 Pretrain된 모델에서도 해당 단어의 representation이 제대로 나타나지 않는 현상을 보여준다. 따라서 특정 모델에서는 [unused01] 와 같이 ‘미사용 토큰’을 만들어 downstream task에 finetuning을 진행할 때 해당 부분을 도메인에서 중요한 토큰으로 변환해서 사용할 수도 있다. 이번 과정을 진행할 때 KcBERT 모델에서 사용하는 emoji중 일부를 원하는 도메인의 특정 단어들로 바꾸어 사용해도 성능이 오를 수 있다. KcBERT PLM MLM Finetune하기Pretrained KcBERT(base) 모델을 MLM task로 새 데이터셋에 추가적으로 학습을 시켜보자. 필요한 패키지 설치1pip install -q Korpora emoji soynlp kss transformers \"datasets &gt;= 1.1.3\" \"sentencepiece != 0.1.92\" protobuf Korpora: 예제 데이터셋 다운받기 emoji: Unicode emoji 목록 받아오기 soynlp: KcBERT에서 진행했던 것과 동일하게 데이터 클리닝을 위해 사용 kss: 예제 데이터셋에서 문단을 문장단위로 분리하기 transformers: MLM학습을 위해 사용 datasets, sentencepiece, protobuf는 transformers에서 필요 예시 데이터셋 받고 정제하기이번 글에서는 Korean petitions dataset(국민청원 데이터셋)을 받아, 본문만 뽑아서 학습을 시켜본다. 국민청원 데이터셋의 전체 크기는 약 43만건이 넘지만, 이 중에서 청원 동의 수가 1000건이 넘은 데이터만 필터링해서 사용해보자. (실제로는 전체를 사용해도 괜찮다.) 123from Korpora import KorporaKorpora.fetch('korean_petitions', root_dir='./Korpora') 위 코드로 데이터셋을 현재 폴더 내 Korpora 폴더에 다운받을 수 있다. 123from glob import globdataset = glob('./Korpora/korean_petitions/petitions*') 실제로 받아진 데이터셋은 jsonl 형식으로, Line-By-Line으로 된 json이다. pandas로 데이터셋을 읽어 content 컬럼만 뽑아서 사용해보자. 12345import pandas as pdfrom tqdm.auto import tqdmdf = pd.concat([pd.read_json(i, lines=True) for i in tqdm(dataset)])df.head() 데이터셋을 읽어 pandas dataframe으로 만들면 위와 같이 데이터를 볼 수 있다. 우선 num_agree, 즉 청원 수가 1000 초과인 것만 남겨두자. 1agreed_df = df[df['num_agree'] &gt; 1000] 이 과정을 거치면 데이터셋이 약 3700개로 줄어든다. 그리고 KcBERT에서 사용하는 clean 함수를 그대로 가져오자. 1234567891011121314import emojifrom soynlp.normalizer import repeat_normalizeemojis = ''.join(emoji.UNICODE_EMOJI.keys())pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣{emojis}]+')url_pattern = re.compile( r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&amp;//=]*)')def clean(x): x = pattern.sub(' ', x) x = url_pattern.sub('', x) x = x.strip() x = repeat_normalize(x, num_repeats=2) return x 위 함수는 emoji를 살리고, URL을 제거하며, 한글과 영어, 그리고 일반적으로 사용하는 특수문자를 제외하고 모두 공백으로 치환한다. 위 클리닝을 거친 contents 컬럼을 python list로 뽑아내자. 1contents = agreed_df['content'].map(clean).to_list() KSS로 문장 분리위에서 추출한 각 본문은 문장별로 쪼개져 있지 않다. 따라서 KSS 라이브러리를 이용해 문장단위로 데이터를 분리해준다. 분리해준 데이터는 korean_petitions_safe.txt 파일에 한 문장씩 써지고, 문서별 \\n\\n 으로 문서를 분리해줄 수 있다. 123456789from kss import split_sentencesimport osif not os.path.exists('korean_petitions_safe.txt'): with open('korean_petitions_safe.txt', 'w') as f: for doc in tqdm(contents): for line in split_sentences(doc, safe=True): f.write(line+'\\n') f.write('\\n') 문서를 분리해주면 BERT의 NSP task를 수행할 수 있다. 문서 분리가 이뤄지지 않으면 사실상 MLM만 학습이 이뤄진다. 이렇게 문장을 분리해줘야 KcBERT의 max_length인 300자 이내로 문장들이 줄여진다. Huggingface로 MLM 학습하기Github에서 run_mlm.py 파일을 받아서 학습을 진행해주자. 아래 코드는 beomi/kcbert-base 모델을 받아 vocab 수정 없이 위에서 만든 txt 파일을 기반으로 학습을 진행하는 명령어다. 1234567wget -nc https://raw.githubusercontent.com/huggingface/transformers/4c32f9f26e6a84f0d9843fec8757e6ce640bb44e/examples/language-modeling/run_mlm.pypython run_mlm.py \\ --model_name_or_path beomi/kcbert-base \\ --train_file korean_petitions_safe.txt \\ --do_train \\ --output_dir ./test-mlm 실제로 실행시 아래와 같은 로그가 나타나며 Loss값이 잘 떨어지면서 학습이 정상적으로 이루어지는 것을 볼 수 있다. 12345678910111213141516171819202122[...생략...][INFO|trainer.py:837] 2021-03-15 05:39:33,828 &gt;&gt; ***** Running training *****[INFO|trainer.py:838] 2021-03-15 05:39:33,828 &gt;&gt; Num examples = 7159[INFO|trainer.py:839] 2021-03-15 05:39:33,828 &gt;&gt; Num Epochs = 3[INFO|trainer.py:840] 2021-03-15 05:39:33,828 &gt;&gt; Instantaneous batch size per device = 8[INFO|trainer.py:841] 2021-03-15 05:39:33,828 &gt;&gt; Total train batch size (w. parallel, distributed &amp; accumulation) = 8[INFO|trainer.py:842] 2021-03-15 05:39:33,828 &gt;&gt; Gradient Accumulation steps = 1[INFO|trainer.py:843] 2021-03-15 05:39:33,828 &gt;&gt; Total optimization steps = 2685{'loss': 2.7071, 'learning_rate': 4.068901303538175e-05, 'epoch': 0.56} 19% 500/2685 [01:39&lt;07:14, 5.03it/s][INFO|trainer.py:1408] 2021-03-15 05:41:13,197 &gt;&gt; Saving model checkpoint to ./test-mlm/checkpoint-500[INFO|configuration_utils.py:304] 2021-03-15 05:41:13,199 &gt;&gt; Configuration saved in ./test-mlm/checkpoint-500/config.json[INFO|modeling_utils.py:817] 2021-03-15 05:41:14,457 &gt;&gt; Model weights saved in ./test-mlm/checkpoint-500/pytorch_model.bin{'loss': 2.5856, 'learning_rate': 3.13780260707635e-05, 'epoch': 1.12} 37% 1000/2685 [03:23&lt;05:34, 5.03it/s][INFO|trainer.py:1408] 2021-03-15 05:42:57,398 &gt;&gt; Saving model checkpoint to ./test-mlm/checkpoint-1000[INFO|configuration_utils.py:304] 2021-03-15 05:42:57,399 &gt;&gt; Configuration saved in ./test-mlm/checkpoint-1000/config.json[INFO|modeling_utils.py:817] 2021-03-15 05:42:58,615 &gt;&gt; Model weights saved in ./test-mlm/checkpoint-1000/pytorch_model.bin{'loss': 2.5064, 'learning_rate': 2.206703910614525e-05, 'epoch': 1.68} 56% 1500/2685 [05:07&lt;03:53, 5.07it/s][INFO|trainer.py:1408] 2021-03-15 05:44:41,722 &gt;&gt; Saving model checkpoint to ./test-mlm/checkpoint-1500[INFO|configuration_utils.py:304] 2021-03-15 05:44:41,723 &gt;&gt; Configuration saved in ./test-mlm/checkpoint-1500/config.json[INFO|modeling_utils.py:817] 2021-03-15 05:44:42,948 &gt;&gt; Model weights saved in ./test-mlm/checkpoint-1500/pytorch_model.bin{'loss': 2.4143, 'learning_rate': 1.2756052141527003e-05, 'epoch': 2.23}[...생략...] 참고: MultiGPU환경에서는 자동으로 DDP로 학습이 이뤄지며, GPU 갯수에 비례하는 batch size로 학습이 이루어진다. 참고: 학습이 완료된 뒤 AttributeError: 'Trainer' object has no attribute 'log_metrics' 라는 에러가 발생할 수 있다. 하지만 학습 자체는 정상적으로 이뤄진 것이기에 걱정하지 않아도 된다. 학습 완료된 파일들학습 과정에는 ./test-mlm/checkpoint-숫자/ 폴더 내에 각 step별 정보가 남는다. 학습이 완료된 이후에는 ./test-mlm/ 폴더 내에 weight파일과 vocab, 그리고 여러 Huggingface 관련 config 파일들이 생긴다. Checkpoint 폴더를 제외한 나머지를 Huggingface Hub에 업로드해 새로운 모델로 사용할 수 있다. (참고링크) Huggingface에 모델 업로드하기: https://huggingface.co/transformers/model_sharing.html Reference/Links KcBERT: https://github.com/Beomi/KcBERT Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks","link":"/2021/03/15/KcBERT-MLM-Finetune/"}],"tags":[{"name":"brew","slug":"brew","link":"/tags/brew/"},{"name":"mac","slug":"mac","link":"/tags/mac/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"mysqlclient","slug":"mysqlclient","link":"/tags/mysqlclient/"},{"name":"pip","slug":"pip","link":"/tags/pip/"},{"name":"python3","slug":"python3","link":"/tags/python3/"},{"name":"vsc++","slug":"vsc","link":"/tags/vsc/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"deeplearning","slug":"deeplearning","link":"/tags/deeplearning/"},{"name":"crawling","slug":"crawling","link":"/tags/crawling/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"twitter","slug":"twitter","link":"/tags/twitter/"},{"name":"올해도무사히","slug":"올해도무사히","link":"/tags/%EC%98%AC%ED%95%B4%EB%8F%84%EB%AC%B4%EC%82%AC%ED%9E%88/"},{"name":"내년도즐겁게","slug":"내년도즐겁게","link":"/tags/%EB%82%B4%EB%85%84%EB%8F%84%EC%A6%90%EA%B2%81%EA%B2%8C/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"aws","slug":"aws","link":"/tags/aws/"},{"name":"hopsworks","slug":"hopsworks","link":"/tags/hopsworks/"},{"name":"data-pipeline","slug":"data-pipeline","link":"/tags/data-pipeline/"},{"name":"colab","slug":"colab","link":"/tags/colab/"},{"name":"tensorflow","slug":"tensorflow","link":"/tags/tensorflow/"},{"name":"tpu","slug":"tpu","link":"/tags/tpu/"},{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"bert","slug":"bert","link":"/tags/bert/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"gitlab","slug":"gitlab","link":"/tags/gitlab/"},{"name":"git-lfs","slug":"git-lfs","link":"/tags/git-lfs/"},{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"statistics","slug":"statistics","link":"/tags/statistics/"},{"name":"postmortem","slug":"postmortem","link":"/tags/postmortem/"},{"name":"clustering","slug":"clustering","link":"/tags/clustering/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"kcbert","slug":"kcbert","link":"/tags/kcbert/"},{"name":"finetune","slug":"finetune","link":"/tags/finetune/"},{"name":"huggingface","slug":"huggingface","link":"/tags/huggingface/"},{"name":"transformers","slug":"transformers","link":"/tags/transformers/"},{"name":"mlm","slug":"mlm","link":"/tags/mlm/"}],"categories":[{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"DevEnv","slug":"DevEnv","link":"/categories/DevEnv/"},{"name":"MacOS","slug":"MacOS","link":"/categories/MacOS/"},{"name":"Ubuntu","slug":"DevEnv/Ubuntu","link":"/categories/DevEnv/Ubuntu/"},{"name":"Translation","slug":"Translation","link":"/categories/Translation/"},{"name":"MacOS","slug":"DevEnv/MacOS","link":"/categories/DevEnv/MacOS/"},{"name":"DjangoTDDStudy","slug":"DjangoTDDStudy","link":"/categories/DjangoTDDStudy/"},{"name":"Fabric","slug":"Python/Fabric","link":"/categories/Python/Fabric/"},{"name":"Python","slug":"MacOS/Python","link":"/categories/MacOS/Python/"},{"name":"Tech","slug":"Tech","link":"/categories/Tech/"},{"name":"Django","slug":"Django","link":"/categories/Django/"},{"name":"Celery","slug":"Python/Celery","link":"/categories/Python/Celery/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"MacOS","slug":"Python/MacOS","link":"/categories/Python/MacOS/"},{"name":"react","slug":"react","link":"/categories/react/"},{"name":"Python","slug":"DjangoTDDStudy/Python","link":"/categories/DjangoTDDStudy/Python/"},{"name":"tips","slug":"tips","link":"/categories/tips/"},{"name":"django","slug":"django","link":"/categories/django/"},{"name":"azure","slug":"azure","link":"/categories/azure/"},{"name":"macos","slug":"macos","link":"/categories/macos/"},{"name":"javascript","slug":"javascript","link":"/categories/javascript/"},{"name":"Python","slug":"Django/Python","link":"/categories/Django/Python/"},{"name":"ubuntu","slug":"ubuntu","link":"/categories/ubuntu/"},{"name":"howtomakewebcrawler","slug":"python/howtomakewebcrawler","link":"/categories/python/howtomakewebcrawler/"},{"name":"flask","slug":"python/flask","link":"/categories/python/flask/"},{"name":"flask","slug":"flask","link":"/categories/flask/"},{"name":"django","slug":"python/django","link":"/categories/python/django/"},{"name":"howtomakewebcrawler","slug":"howtomakewebcrawler","link":"/categories/howtomakewebcrawler/"},{"name":"ubuntu","slug":"python/ubuntu","link":"/categories/python/ubuntu/"},{"name":"Ubuntu","slug":"Python/MacOS/Ubuntu","link":"/categories/Python/MacOS/Ubuntu/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"pyspark","slug":"python/pyspark","link":"/categories/python/pyspark/"},{"name":"aws","slug":"python/aws","link":"/categories/python/aws/"},{"name":"life","slug":"life","link":"/categories/life/"},{"name":"celery","slug":"python/celery","link":"/categories/python/celery/"},{"name":"javascript","slug":"react/javascript","link":"/categories/react/javascript/"},{"name":"aws","slug":"aws","link":"/categories/aws/"},{"name":"fabric","slug":"python/fabric","link":"/categories/python/fabric/"},{"name":"dataanalysis","slug":"python/dataanalysis","link":"/categories/python/dataanalysis/"},{"name":"machinelearning","slug":"machinelearning","link":"/categories/machinelearning/"},{"name":"githubpages","slug":"tips/githubpages","link":"/categories/tips/githubpages/"},{"name":"vlang","slug":"vlang","link":"/categories/vlang/"},{"name":"DeepLearning","slug":"DevEnv/DeepLearning","link":"/categories/DevEnv/DeepLearning/"},{"name":"DataCollection","slug":"DataCollection","link":"/categories/DataCollection/"},{"name":"Postmortem","slug":"Postmortem","link":"/categories/Postmortem/"},{"name":"DataScience","slug":"DevEnv/DataScience","link":"/categories/DevEnv/DataScience/"},{"name":"fabric","slug":"django/fabric","link":"/categories/django/fabric/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"Git","slug":"DevEnv/Git","link":"/categories/DevEnv/Git/"},{"name":"postmortem","slug":"postmortem","link":"/categories/postmortem/"},{"name":"nlp","slug":"nlp","link":"/categories/nlp/"},{"name":"Translation","slug":"Django/Python/Translation","link":"/categories/Django/Python/Translation/"},{"name":"fabric","slug":"python/flask/fabric","link":"/categories/python/flask/fabric/"},{"name":"sqlalchemy","slug":"flask/sqlalchemy","link":"/categories/flask/sqlalchemy/"},{"name":"webpack","slug":"javascript/webpack","link":"/categories/javascript/webpack/"},{"name":"Windows","slug":"Python/MacOS/Ubuntu/Windows","link":"/categories/Python/MacOS/Ubuntu/Windows/"},{"name":"tips","slug":"macos/tips","link":"/categories/macos/tips/"},{"name":"github","slug":"blog/github","link":"/categories/blog/github/"},{"name":"tensorflow","slug":"python/aws/tensorflow","link":"/categories/python/aws/tensorflow/"},{"name":"celerydocs","slug":"python/celery/celerydocs","link":"/categories/python/celery/celerydocs/"},{"name":"tips","slug":"ubuntu/tips","link":"/categories/ubuntu/tips/"},{"name":"tips","slug":"django/tips","link":"/categories/django/tips/"},{"name":"es6","slug":"react/javascript/es6","link":"/categories/react/javascript/es6/"},{"name":"tips","slug":"python/django/tips","link":"/categories/python/django/tips/"},{"name":"tips","slug":"python/pyspark/tips","link":"/categories/python/pyspark/tips/"},{"name":"python","slug":"aws/python","link":"/categories/aws/python/"},{"name":"django","slug":"python/fabric/django","link":"/categories/python/fabric/django/"},{"name":"machinelearning","slug":"python/dataanalysis/machinelearning","link":"/categories/python/dataanalysis/machinelearning/"},{"name":"seminar","slug":"machinelearning/seminar","link":"/categories/machinelearning/seminar/"},{"name":"Twitter","slug":"DataCollection/Twitter","link":"/categories/DataCollection/Twitter/"},{"name":"LanguageModel","slug":"NLP/LanguageModel","link":"/categories/NLP/LanguageModel/"},{"name":"Unsupervised","slug":"NLP/Unsupervised","link":"/categories/NLP/Unsupervised/"},{"name":"languagemodel","slug":"nlp/languagemodel","link":"/categories/nlp/languagemodel/"},{"name":"pandas","slug":"flask/sqlalchemy/pandas","link":"/categories/flask/sqlalchemy/pandas/"},{"name":"DevEnv","slug":"Python/MacOS/Ubuntu/Windows/DevEnv","link":"/categories/Python/MacOS/Ubuntu/Windows/DevEnv/"},{"name":"translation","slug":"python/celery/celerydocs/translation","link":"/categories/python/celery/celerydocs/translation/"},{"name":"ldap","slug":"ubuntu/tips/ldap","link":"/categories/ubuntu/tips/ldap/"},{"name":"socialdata","slug":"python/dataanalysis/machinelearning/socialdata","link":"/categories/python/dataanalysis/machinelearning/socialdata/"},{"name":"BERT","slug":"NLP/LanguageModel/BERT","link":"/categories/NLP/LanguageModel/BERT/"},{"name":"tips","slug":"flask/sqlalchemy/pandas/tips","link":"/categories/flask/sqlalchemy/pandas/tips/"},{"name":"comments analysis","slug":"python/dataanalysis/machinelearning/socialdata/comments-analysis","link":"/categories/python/dataanalysis/machinelearning/socialdata/comments-analysis/"}]}